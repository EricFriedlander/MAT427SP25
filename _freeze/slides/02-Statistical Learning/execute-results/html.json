{
  "hash": "e7eee6febfa2c075327f8def2b691daf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Intro to Machine Learning'\nauthor: Eric Friedlander <br> [Much of the content in these slides have been adapted from Abhishek Chakraborty at Lawrence University]{.smaller}\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n\n## Data Generating Process\n\nSuppose we have\n\n-   Features: $\\mathbf{X}$\n-   Target: $Y$\n-   Goal: Predict $Y$ using $\\mathbf{X}$\n\n. . .\n\n-   **Data generating process**: underlying, unseen and unknowable\n    process that generates $Y$ given $\\mathbf{X}$\n\n## Population\n\nMore mathematically, the \"true\"/population model can be represented by\n\n$$Y=f(\\mathbf{X}) + \\epsilon$$\n\nwhere $\\epsilon$ is a **random** error term (includes measurement error,\nother discrepancies) independent of $\\mathbf{X}$ and has mean zero.\n\n. . .\n\n*GOAL*: Estimate $f$\n\n## Why Estimate $f(\\mathbf{X})$? {.smaller}\n\nWe wish to know about $f(\\mathbf{X})$ for two reasons:\n\n1.  Prediction: make an educated guess for what $y$ should be given a\n    new $x_0$:\n    $$\\hat{y}_0=\\hat{f}(x_0) \\ \\ \\ \\text{or} \\ \\ \\ \\hat{y}_0=\\hat{C}(x_0)$$\n2.  Inference: Understand the relationship between $\\mathbf{X}$ and $Y$.\n\n. . .\n\n-   An ML algorithm that is developed mainly for predictive purposes is\n    often termed as a **Black Box** algorithm.\n\n## Prediction {.smaller}\n\nThere are two types of prediction problems:\n\n-   **Regression** (response $Y$ is quantitative): Build a model\n    $\\hat{Y} = \\hat{f}(\\mathbf{X})$\n-   **Classification** (response $Y$ is qualitative/categorical): Build\n    a classifier $\\hat{Y}=\\hat{C}(\\mathbf{X})$\n\n. . .\n\n-   Note: a \"hat\", $\\hat{\\phantom{f}}$, over an object represents an\n    estimate of that object\n    -   E.g. $\\hat{Y}$ is an estimate of $Y$ and $\\hat{f}$ is an\n        estimate of $f$\n\n## Prediction and Inference\n\n**Income dataset**\n\n![Why ML? (from ISLR2)](images/02/2_2-1.png){.r-stretch}\n\n## Prediction and Inference\n\n**Income dataset**\n\n::: {layout-ncol=\"2\"}\n![](images/02/2_3-1.png)\n\n![](images/02/2_4-1.png)\n\nWhy ML? (from ISLR2)\n:::\n\n## [Question!!!]{style=\"color:blue\"} {.smaller}\n\nBased on the previous two slides, which of the following statements are\ncorrect?\n\n::: panel-tabset\n## Questions\n\n1.  As `Years of Education` increases, `Income` increases, keeping\n    `Seniority` fixed.\n2.  As `Years of Education` increases, `Income` decreases, keeping\n    `Seniority` fixed.\n3.  As `Years of Education` increases, `Income` increases.\n4.  As `Seniority` increases, `Income` increases, keeping\n    `Years of Education` fixed.\n5.  As `Seniority` increases, `Income` decreases, keeping\n    `Years of Education` fixed.\n6.  As `Seniority` increases, `Income` increases.\n\n## Answers\n\n1.  As `Years of Education` increases, `Income` increases, keeping\n    `Seniority` fixed. **TRUE**\n2.  As `Years of Education` increases, `Income` decreases, keeping\n    `Seniority` fixed. **FALSE**\n3.  As `Years of Education` increases, `Income` increases. **TRUE**\n4.  As `Seniority` increases, `Income` increases, keeping\n    `Years of Education` fixed. **TRUE**\n5.  As `Seniority` increases, `Income` decreases, keeping\n    `Years of Education` fixed. **FALSE**\n6.  As `Seniority` increases, `Income` increases. **TRUE**\n:::\n\n## Discussion\n\nWhat's the difference between these two statements:\n\n1.  As `Years of Education` increases, `Income` increases, keeping\n    `Seniority` fixed.\n2.  As `Years of Education` increases, `Income` increases.\n\n<!-- ## [Question!!!]{style=\"color:blue\"} {.smaller} -->\n\n<!-- Which of the following statements are correct? -->\n\n<!-- 1. The increase in `Income` resulting from increase in `Years of Education` keeping other variables fixed is **more** than the increase in `Income` resulting from increase in `Seniority` keeping other variables fixed. -->\n\n<!-- 2. The increase in `Income` resulting from increase in `Years of Education` keeping other variables fixed is **less** than the increase in `Income` resulting from increase in `Seniority` keeping other variables fixed. -->\n\n## How Do We Estimate $f(\\mathbf{X})$?\n\nBroadly speaking, we have two approaches.\n\n1.  Parametric methods\n2.  Non-parametrics methods\n\n## Parametric Methods\n\n-   Assume a functional form for $f(\\mathbf{X})$\n    -   Linear Regression:\n        $f(\\mathbf{X})=\\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\ldots + \\beta_p \\mathbf{x}_p$\n    -   Estimate the parameters $\\beta_0, \\beta_1, \\ldots, \\beta_p$\n        using labeled data\n-   Choosing $\\beta$'s that minimize some error metrics is called\n    **fitting** the model\n-   The data we use to fit the model is called our **training data**\n\n## Parametric Methods {.smaller}\n\n![Parametric model fit (from ISLR2)](images/02/2_2-1.png){.r-stretch}\n\n::: incremental\n-   What are some potential parametric models that could result in this\n    picture?\n-   Note: Right line is the true relationship\n:::\n\n## Parametric Methods {.smaller}\n\n**Income dataset**\n\n::: {layout-ncol=\"2\"}\n![True relationship](images/02/2_3-1.png){width=\"60%\"}\n\n![Parametric model](images/02/2_4-1.png){width=\"60%\"}\n\nFrom ISLR2\n:::\n\n::: incremental\n-   What are some functions that could have resulted in the model on the\n    right?\n-   $\\text{Income} \\approx \\beta_0 + \\beta_1\\times\\text{Years of Education} + \\beta_2\\times\\text{Seniority}$\n:::\n\n## Non-parametric Methods {.smaller}\n\n-   Non-parametric approach: no explicit assumptions about the\n    functional form of $f(\\mathbf{X})$\n-   Much more observations (compared to a parametric approach) required\n    to fit non-parametric model\n    -   **Idea:** parametric model restricts space of possible answers\n\n**Income dataset**\n\n::: {layout-ncol=\"2\"}\n![True relationship](images/02/2_3-1.png){width=\"50%\"}\n\n![Non-parametric model fit](images/02/2_5-1.png){width=\"50%\"}\n\nFrom ISLR2\n:::\n\n## Supervised Learning: Flexibility of Models {.smaller}\n\n-   Flexibility: smoothness of functions\n-   More theoretically: how many parameters are there to estimate?\n\n\n\n::: {.cell layout-align=\"center\" r-stretch='true'}\n::: {.cell-output-display}\n![](02-Statistical-Learning_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n[More flexible $\\implies$ More complex $\\implies$ Less Smooth $\\implies$\nLess Restrictive $\\implies$ Less Interpretable\n\n## Supervised Learning: Some Trade-offs {.smaller}\n\n-   Prediction Accuracy versus Interpretability\n-   Good Fit versus Over-fit or Under-fit\n\n![Trade-off between flexibility and interpretability (from\nISLR2)](images/02/2_7-1-01.png)\n\n## Supervised Learning: Selecting a Model {.smaller}\n\n-   Why so many different ML techniques?\n-   **There is no free lunch in statistics**: All methods have different\n    pros and cons\n    -   Must select correct model for each use-case\n-   Relevant questions in model selection:\n    -   How much observations $n$ and variables $p$?\n    -   What is the relative importance is prediction, interpretability, and inference?\n    -   Do we expect relationship to be non-linear?\n    -   Regression or classification?\n\n## Supervised Learning: Assessing Model Performance {.smaller}\n\n-   When we estimate $f(\\mathbf{X})$ using $\\hat{f}(\\mathbf{X})$, then,\n\n$$\\underbrace{E\\left[Y-\\hat{Y}\\right]^2}_{Error}=E\\left[f(\\mathbf{X})+\\epsilon - \\hat{f}(\\mathbf{X})\\right]^2=\\underbrace{\\left[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})\\right]^2}_{Reducible} + \\underbrace{Var(\\epsilon)}_{Irreducible}$$\n\n-   $E\\left[Y-\\hat{Y}\\right]^2$: Expected (average) squared difference\n    between predicted and actual (observed) response, **Mean Squared Error (MSE)**\n-   Goal: find an estimate of $f(\\mathbf{X})$ to minimize the reducible\n    error\n\n## Supervised Learning: Assessing Model Performance {.smaller}\n\n::: {style=\"font-size: 75%;\"}\n-   Labeled training data $(x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)$\n    -   i.e. $n$ training observations\n-   Fit/train a model from training data\n    -   $\\hat{y}=\\hat{f}(x)$, regression\n    -   $\\hat{y}=\\hat{C}(x)$, classification\\\n-   Obtain estimates $\\hat{f}(x_1), \\hat{f}(x_2), \\ldots, \\hat{f}(x_n)$\n    (or, $\\hat{C}(x_1), \\hat{C}(x_2), \\ldots, \\hat{C}(x_n)$) of training\n    data\n-   Compute error:\n    -   **Regression**\n        $$\\text{Training MSE}=\\text{Average}_{Training} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\left(y_i-\\hat{f}(x_i)\\right)^2$$\n    -   **Classification** $$\n        \\begin{aligned}\n        \\text{Training Error Rate}\n        &=\\text{Average}_{Training} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]\\\\\n        &= \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\ I\\left(y_i \\ne \\hat{C}(x_i)\\right)\n        \\end{aligned}\n        $$\n:::\n\n## Supervised Learning: Assessing Model Performance {.smaller}\n\n-   In general, not interested in performance on training data\n-   Want: performance on unseen test data... why?\n-   Fresh test data:\n    $(x_1^{test},y_1^{test}), (x_2^{test},y_2^{test}), \\ldots, (x_m^{test},y_m^{test})$.\n-   Compute test error:\n    -   **Regression**\n        $$\\text{Test MSE}=\\text{Average}_{Test} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\left(y_i^{test}-\\hat{f}(x_i^{test})\\right)^2$$\n    -   **Classification**\n        $$\\text{Test Error Rate}=\\text{Average}_{Test} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]= \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\ I\\left(y_i^{test} \\ne \\hat{C}(x_i^{test})\\right)$$\n\n## Supervised Learning: Bias-Variance Trade-off {.smaller}\n\n-   Model fit on training data $\\hat{f}(x)$\n-   \"True\" relationship: $Y=f(x)+\\epsilon$\n-   $(x_0^{test}, y_0^{test})$: test observation\n-   Bias-Variance Trade-Off (Theorical)\n    $$\\underbrace{E\\left(y_0^{test}-\\hat{f}(x_0^{test})\\right)^2}_{total \\ error}=\\underbrace{Var\\left(\\hat{f}(x_0^{test})\\right)}_{source \\ 1} + \\underbrace{\\left[Bias\\left(\\hat{f}(x_0^{test})\\right)\\right]^2}_{source \\ 2}+\\underbrace{Var(\\epsilon)}_{source \\ 3}$$\n    where\n    $Bias\\left(\\hat{f}(x_0)\\right)=E\\left(\\hat{f}(x_0)\\right)-f(x_0)$\n\n. . .\n\n-   Question: Where is $\\hat{y}_0^{test}$?\n\n## Supervised Learning: Bias-Variance Trade-off\n\n-   Reducible Error:\n    -   Source 1: how $\\hat{f}(x)$ varies among different randomly\n        selected possible training data (**Variance**)\n    -   Source 2: how $\\hat{f}(x)$ (when predicting the test data)\n        differs from its target $f(x)$ (**Bias**)\n-   Irreducible Error:\n    -   Source 3: how $y$ differs from \"true\" $f(x)$\n\n## Supervised Learning: Comparing Bias and Variance\n\nInsert App Stuff Here\n\n## Bias-Variance Trade-off: Example {.smaller}\n\n-   For now: focus on regression problems (ideas extend to\n    classification)\n-   Consider: three different examples of simulated \"toy\" datasets and\n    three types of models ($\\hat{f}_i(.)$)\n    +   Linear Regression [**orange**]{style=\"color:orange\"}\n    +   Smoothing Spline 1 [**blue**]{style=\"color:cornflowerblue\"}\n    +   More flexible Smoothing Spline 2\n        [**green**]{style=\"color:green\"}\n-   \"True\" (simulated) function $f(.)$ [**black**]{style=\"color:black\"}\n-   [**Training Error**]{style=\"color:grey\"}\n-   [**Test Error**]{style=\"color:red\"}\n\n## Bias-Variance Trade-off: Example\n\n![From ISLR2](images/02/2_9-1.png)\n\n## Bias-Variance Trade-off: Example\n\n![From ISLR2](images/02/2_10-1.png)\n\n## Bias-Variance Trade-off: Example\n\n![From ISLR2](images/02/2_11-1.png)\n\n## Bias-Variance Trade-off: Example\n\n![From ISLR2](images/02/2_12-1.png)\n\n## [Question!!!]{style=\"color:blue\"}\n\nAs flexibility increases,\n\n::: panel-tabset\n\n## Questions\n\n1. its variance (increases/decreases)\n2. its bias  (increases/decreases)\n3. its training MSE  (increases/decreases)\n4. its test MSE  (describe)\n\n## Answers\n1. its variance  (**increases**)\n2. its bias  (**decreases**)\n3. its training MSE  (**decreases**)\n4. its test MSE  (**decreases at first, then increases and the model starts to overfit, U-shaped**)\n\n:::\n\n## Recap\n\n-   Regression vs. Classification\n-   Parametric vs. non-parametric models\n-   Training v. test data\n-   Assessing regression models: Mean-Squared Error\n-   Trade-offs:\n    +   Flexibility vs. interpretability\n    +   Bias vs. variance",
    "supporting": [
      "02-Statistical-Learning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}