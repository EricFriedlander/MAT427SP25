{
  "hash": "ecaba3383ac82b31d95ed647df7c3f1e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MATH 427: Classification + Logistic Regression'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n\n## Classification Problems {.smaller}\n\n- Response $Y$ is qualitative (categorical).\n- Objective: build a classifier $\\hat{Y}=\\hat{C}(\\mathbf{X})$\n  + assigns class label to a future unlabeled (unseen) observations\n  + understand the relationship between the predictors and response\n- Two ways to make predictions\n  + Class probabilities\n  + Class labels\n\n## Default Dataset {.smaller}\n\n\nA simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR2)   # load library\nhead(Default) |> kable()  # print first six observations\n```\n\n::: {.cell-output-display}\n\n\n|default |student |   balance|    income|\n|:-------|:-------|---------:|---------:|\n|No      |No      |  729.5265| 44361.625|\n|No      |Yes     |  817.1804| 12106.135|\n|No      |No      | 1073.5492| 31767.139|\n|No      |No      |  529.2506| 35704.494|\n|No      |No      |  785.6559| 38463.496|\n|No      |Yes     |  919.5885|  7491.559|\n\n\n:::\n:::\n\n\n\nWe will consider `default` as the response variable.\n\n\n## Split the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427)\n\ndefault_split <- initial_split(Default, prop = 0.6, strata = default)\ndefault_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<6000/4000/10000>\n```\n\n\n:::\n\n```{.r .cell-code}\ndefault_train <- training(default_split)\ndefault_test <- testing(default_split)\n```\n:::\n\n\n\n## Summarizing our response variable {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(janitor)\nDefault |> tabyl(default) |> kable()  # class frequencies\n```\n\n::: {.cell-output-display}\n\n\n|default |    n| percent|\n|:-------|----:|-------:|\n|No      | 9667|  0.9667|\n|Yes     |  333|  0.0333|\n\n\n:::\n\n```{.r .cell-code}\ndefault_train |> tabyl(default) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|default |    n| percent|\n|:-------|----:|-------:|\n|No      | 5796|   0.966|\n|Yes     |  204|   0.034|\n\n\n:::\n\n```{.r .cell-code}\ndefault_test |> tabyl(default) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|default |    n| percent|\n|:-------|----:|-------:|\n|No      | 3871| 0.96775|\n|Yes     |  129| 0.03225|\n\n\n:::\n:::\n\n\n\n\n## Data Types in R {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDefault |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,000\nColumns: 4\n$ default <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, Noâ€¦\n$ student <fct> No, Yes, No, No, No, Yes, No, Yes, No, No, Yes, Yes, No, No, Nâ€¦\n$ balance <dbl> 729.5265, 817.1804, 1073.5492, 529.2506, 785.6559, 919.5885, 8â€¦\n$ income  <dbl> 44361.625, 12106.135, 31767.139, 35704.494, 38463.496, 7491.55â€¦\n```\n\n\n:::\n:::\n\n\n\n-   `fct` = `factor` which is the data type you want to use for categorical data\n-   `as_factor` will typically transform things (including numbers) into factors for you\n-   `chr` can also be used but `factor`s are better because they store all possible levels for your categorical data\n-   `factor`s are helpful for plotting because you can reorder the levels to help you plot things\n\n## K-Nearest Neighbors Classifier\n\n-   Given a value for $K$ and a test data point $x_0$:\n    $$P(Y=j | X=x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} I(y_i = j)$$\n    where $\\mathcal{N}_0$ is the set of the $K$ \"closest\" neighbors.\n-   For classification: neighbors \"vote\" for class (unlike in regression\n    where predictions are obtained by averaging)\n    $$P(Y=j | X=x_0)=\\text{Proportion of neighbors in class }j$$\n\n\n## K-Nearest Neighbors Classifier: Build Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_default_fit <- nearest_neighbor(neighbors = 10) |>\n  set_engine(\"kknn\") |>\n  set_mode(\"classification\") |>\n  fit(default ~ balance, data = default_train)   # fit 10-nn model\n```\n:::\n\n\n\n-   Why don't I need to worry about centering and scaling?\n\n\n## K-Nearest Neighbors Classifier: Predictions\n\n-   `predict` with a categorical response: [documentation](https://parsnip.tidymodels.org/reference/predict.model_fit.html)\n-   Two different ways of making predictions\n\n## Predicting a class\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_class_preds <- predict(knn_default_fit, new_data = default_test, type = \"class\")   # obtain default class label predictions\n\nknn_class_preds |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|.pred_class |\n|:-----------|\n|No          |\n|No          |\n|No          |\n|No          |\n|No          |\n|No          |\n\n\n:::\n:::\n\n\n\n##  Predicting a probability\n\n-   Can anyone pick-out what's wrong here? Hint: $k = 10$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_prob_preds <- predict(knn_default_fit, new_data = default_test, type = \"prob\")   # obtain predictions as probabilities\nknn_prob_preds |> filter(.pred_No*.pred_Yes >0) |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| .pred_No| .pred_Yes|\n|--------:|---------:|\n|   0.8685|    0.1315|\n|   0.8685|    0.1315|\n|   0.8685|    0.1315|\n|   0.8505|    0.1495|\n|   0.7105|    0.2895|\n|   0.4775|    0.5225|\n\n\n:::\n:::\n\n\n\n## I've been lying to you\n\n-   `kknn` actually takes a *weighted average* of the nearest neighbors\n    +   I.e. closer observations get more weight\n-   To use unweighted KNN need `weight_func = \"rectangular\"`\n\n## Unweighted KNN\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_default_unw_fit <- nearest_neighbor(neighbors = 10, weight_fun = \"rectangular\") |>\n  set_engine(\"kknn\") |>\n  set_mode(\"classification\") |>\n  fit(default ~ balance, data = default_train)   # fit 10-nn model\n\nknn_uw_prob_preds <- predict(knn_default_unw_fit, new_data = default_test, type = \"prob\")   # obtain predictions as probabilities\nknn_uw_prob_preds |> filter(.pred_No*.pred_Yes >0) |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| .pred_No| .pred_Yes|\n|--------:|---------:|\n|      0.9|       0.1|\n|      0.9|       0.1|\n|      0.9|       0.1|\n|      0.9|       0.1|\n|      0.7|       0.3|\n|      0.5|       0.5|\n\n\n:::\n:::\n\n\n\n# Logistic Regression\n\n## Why Not Linear Regression? {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDefault_lr <- default_train |> \n  mutate(default_0_1 = if_else(default == \"Yes\", 1, 0))\n\nlrfit <- linear_reg() |> \n  set_engine(\"lm\") |> \n  fit(default_0_1 ~ balance, data = Default_lr)   # fit SLR\n\nlrfit |> predict(new_data = default_train) |> head() |> kable()\n```\n\n::: {.cell-output-display}\n\n\n|      .pred|\n|----------:|\n|  0.0316011|\n|  0.0655518|\n| -0.0065293|\n|  0.0274263|\n|  0.0451629|\n|  0.0327046|\n\n\n:::\n:::\n\n\n\n## Why Not Linear Regression? {.smaller}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-classification-logistic_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n- Linear regression: does not model probabilities well\n  + might produce probabilities less than zero or bigger than one\n  + treats increase from 0.41 to 0.5 as same as 0.01 to 0.1 (bad)\n\n## Why Not Linear Regression?\n\nSuppose we have a response,\n$$Y=\\begin{cases}\n1 & \\text{if stroke} \\\\\n2 & \\text{if drug overdose} \\\\\n3  & \\text{if epileptic seizure}\n\\end{cases}$$\n\n- Linear regression suggests an ordering, and in fact implies that the differences between classes have meaning\n  + e.g. drug overdose $-$ stroke $= 1$? ðŸ¤”\n\n## Logistic Regression {.smaller}\n\nConsider a one-dimensional binary classification problem:\n\n- Transform the linear model $\\beta_0 + \\beta_1 \\ X$ so that the output is a probability\n- Use **logistic** function: $$g(t)=\\dfrac{e^t}{1+e^t} \\ \\ \\ \\text{for} \\ t \\in \\mathcal{R}$$\n- Then: $$p(X)=P(Y=1|X)=g\\left(\\beta_0 + \\beta_1 \\ X\\right)=\\dfrac{e^{\\beta_0 + \\beta_1 \\ X}}{1+e^{\\beta_0 + \\beta_1 \\ X}}$$\n\n## Other important quantities\n- **Odds**: $\\dfrac{p(x)}{1-p(x)}$\n- **Log-Odds (logit)**: $\\log\\left(\\dfrac{p(x)}{1-p(x)}\\right) = \\beta_0 + \\beta_1 \\ X$\n  + Linear function of predictors\n\n## Logistic Regression\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-classification-logistic_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Fitting the model\n\nFitting a logistic regression model with `default` as the response and `balance` as the predictor:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogregfit <- logistic_reg() |> \n  set_engine(\"glm\") |> \n  fit(default ~ balance, data = default_train)   # fit logistic regression model\n\ntidy(logregfit) |> kable()  # obtain results\n```\n\n::: {.cell-output-display}\n\n\n|term        |    estimate| std.error| statistic| p.value|\n|:-----------|-----------:|---------:|---------:|-------:|\n|(Intercept) | -10.6926385| 0.4659035| -22.95033|       0|\n|balance     |   0.0055327| 0.0002841|  19.47329|       0|\n\n\n:::\n:::\n\n\n\n## Interpreting Coefficients\n\n- As $X$ increases by 1, the log-odds increase by $\\hat{\\beta}_1$\n  + I.e. probability of default increases but NOT linearly\n  + Change in the probability of default due to a one-unit change in balance depends on the current balance value\n\n## Interpreting Coefficients\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](07-classification-logistic_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n## Making predictions: Theory {.smaller}\n\nFor `balance`=\\$700,\n\n:::{.incremental}\n- $$\\hat{p}(X)=\\dfrac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1 X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1 X}}=\\dfrac{e^{-10.69 + (0.005533 \\times 700)}}{1+e^{-10.69 + (0.005533 \\times 700)}}=0.0011$$\n-  $$\\textbf{Odds}(X) = \\dfrac{\\hat{p}(X)}{1-\\hat{p}(X)} = \\dfrac{0.0011}{1-0.0011}\\approx 0.0011$$\n- $$\\textbf{Log-Odds}(X)=\\log\\left(\\dfrac{\\hat{p}(X)}{1-\\hat{p}(X)}\\right) = \\log(0.0011) = -6.80$$\n:::\n\n## Making predictions in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"class\") |> kable()   # obtain class predictions\n```\n\n::: {.cell-output-display}\n\n\n|.pred_class |\n|:-----------|\n|No          |\n\n\n:::\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"raw\") |> kable()   # obtain log-odds predictions\n```\n\n::: {.cell-output-display}\n\n\n|         x|\n|---------:|\n| -6.819727|\n\n\n:::\n\n```{.r .cell-code}\npredict(logregfit, new_data = tibble(balance = 700), type = \"prob\") |> kable()  # obtain probability predictions\n```\n\n::: {.cell-output-display}\n\n\n|  .pred_No| .pred_Yes|\n|---------:|---------:|\n| 0.9989092| 0.0010908|\n\n\n:::\n:::",
    "supporting": [
      "07-classification-logistic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}