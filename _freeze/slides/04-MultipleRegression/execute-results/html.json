{
  "hash": "b0f5e4fe78dc88ed6f47f2f1c83a2529",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MAT-427: Multiple Linear Regression + Data Splitting'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(readODS)\n```\n:::\n\n\n\n## [Question!!!]{style=\"color:blue\"}\n\nAs a model's flexibility increases:\n\n::: panel-tabset\n\n## Questions\n\n1. its variance (increases/decreases)\n2. its bias  (increases/decreases)\n3. its training MSE  (increases/decreases)\n4. its test MSE  (describe)\n\n## Answers\n1. its variance  (**increases**)\n2. its bias  (**decreases**)\n3. its training MSE  (**decreases**)\n4. its test MSE  (**decreases at first, then increases and the model starts to overfit, U-shaped**)\n\n:::\n\n\n## Bias-Variance Trade-Off App {.smaller}\n\nWhen Dr. F tell you, navigate to [this app](https://efriedlander.shinyapps.io/BVTappRegression/)\n\n-   Select your assigned data set and look at the second row of plots\n  +   Which order polynomials have the lowest Bias, Variance, training MSE, and test MSE\n      +   Which order polynomial would you say is best?\n  +   Scroll down to the last row of plots and discuss how these images support your answers\n  +   Repeat this process for different amounts of noise?\n  +   Return to the first row of plots and select three different order polynomials based on your answers above... is the bias variance trade-off visible in this plot?\n\n\n# Modeling in R\n\n## Outlet Data {.smaller} \n\nSuppose the CEO of a restaurant franchise is considering opening new outlets in different cities. They would like to expand their business to cities that give them higher profits with the assumption that highly populated cities will probably yield higher profits.\n\nThey have data on the population (in 100,000) and profit (in $1,000) at 97 cities where they currently have outlets.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutlets <- readRDS(\"../data/outlets.rds\")   # load dataset\nhead(outlets) |> kable() # first six observations of the dataset\n```\n\n::: {.cell-output-display}\n\n\n| population|  profit|\n|----------:|-------:|\n|     6.1101| 17.5920|\n|     5.5277|  9.1302|\n|     8.5186| 13.6620|\n|     7.0032| 11.8540|\n|     5.8598|  6.8233|\n|     8.3829| 11.8860|\n\n\n:::\n:::\n\n\n\n## R as an open-source language\n\n- R is an open source language\n  + Advantages:\n    + Packages for almost anything you want\n    + \"Cutting edge\" methods rolled out quickly and early\n  + Disadvantages\n    + Many packages (especially new ones) may  have bugs\n    + Lots of syntactical diversity\n    + Syntax is frequently dependent on the needs of the person who wrote the package and conventions at the time the package was created\n    \n## Enter `tidyverse`\n\n> The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\n- `tidyverse` is for manipulating and visualizing data\n- the `tidyverse` is a *meta-package* meaning it is a collection of a bunch of other packages\n\n## Enter `tidymodels`\n\n>The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n- `tidymodels` creates a unified framework for building models in R\n- Eric's opinion: similar idea to `scikit-learn` in Python\n\n## Back to linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = outlets) +\n  geom_point(mapping = aes(x = population, y = profit)) +    # create scatterplot\n  geom_smooth(mapping = aes(x = population, y = profit), \n              method = \"lm\", se = FALSE)   # add the regression line\n```\n\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n## Classic Linear Regression in R {.smaller}\n\n::::{.columns}\n:::{.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\noutlets_model <- lm(profit ~ population, data = outlets)\noutlets_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = profit ~ population, data = outlets)\n\nCoefficients:\n(Intercept)   population  \n     -3.896        1.193  \n```\n\n\n:::\n:::\n\n\n:::\n\n:::{.column}\n\nThis corresponds to the model:\n\n$$\n\\begin{aligned}\n\\text{Profit}  &= -3.90 + 1.19\\times\\text{Population}\\\\\n\\hat{Y}_i &= -3.90 + 1.19X_i\n\\end{aligned}\n$$\ni.e. $\\hat{\\beta}_0 = -3.90$ and $\\hat{\\beta}_1 = 1.19$\n:::\n::::\n\n## Modeling with `tidymodels`\n\n1. Specify mathematical structure of model (e.g. linear regression, logistic regression)\n\n2. Specify the *engine* for fitting the model. (e.g. `lm`, `stan`, `glmnet`).\n\n3. When required, declare the mode of the model (i.e. regression or classification). \n\n## Linear Regression with `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2|3|5|6|11,12\"}\n# Usually put these at the top\nlibrary(tidymodels) # load tidymodels package\ntidymodels_prefer() # avoid common conflicts\n\nlm_model <- linear_reg() |> # Step 1\n  set_engine(\"lm\") # Step 2\n\n# Step 3 not required since linear regression can't be used for classification\n\n# Fit the model\nlm_model_fit <- lm_model |> \n  fit(profit ~ population, data = outlets)\n```\n:::\n\n\n\n## Linear Regression with `tidymodels`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_model_fit |> \n  tidy() |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term        |  estimate| std.error| statistic| p.value|\n|:-----------|---------:|---------:|---------:|-------:|\n|(Intercept) | -3.895781| 0.7194828| -5.414696|   5e-07|\n|population  |  1.193034| 0.0797439| 14.960806|   0e+00|\n\n\n:::\n:::\n\n\n\nSame model as before:\n\n$$\n\\begin{aligned}\n\\text{Profit}  &= -3.90 + 1.19\\times\\text{Population}\\\\\n\\hat{Y}_i &= -3.90 + 1.19X_i\n\\end{aligned}\n$$\n\n\n\n## Linear Regression in R: Prediction\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_cities <- tibble(population = rnorm(100, 7, 3))\n\nlm_model_fit |> \npredict(new_data = new_cities) |> \n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|      .pred|\n|----------:|\n|  0.4889652|\n|  3.6625105|\n|  7.6818386|\n|  3.4912164|\n| -0.7417786|\n|  6.3256699|\n|  9.5310272|\n|  1.0828577|\n|  2.4970978|\n|  4.1252529|\n|  1.4799014|\n|  6.5884128|\n| 14.6291040|\n|  5.0633175|\n| 11.7794863|\n|  6.1050430|\n|  4.0859515|\n| 11.3531761|\n|  5.5484202|\n|  7.8737727|\n|  4.3650040|\n|  6.1872627|\n| -1.9048730|\n|  1.8354761|\n|  2.8479956|\n|  2.5517259|\n|  0.6772379|\n|  1.4211071|\n|  6.2520754|\n|  6.9582377|\n| -1.6184066|\n| 10.4123130|\n|  2.8143659|\n|  4.2905259|\n| -0.9925139|\n|  1.4346711|\n| -0.8131297|\n|  7.2087757|\n|  0.2289460|\n|  4.3721440|\n|  1.2394631|\n|  6.0835128|\n| 10.2457562|\n|  9.8387182|\n|  2.6512463|\n|  6.4799195|\n|  4.0905007|\n|  7.3948929|\n|  3.9030657|\n|  6.0808643|\n|  7.0669680|\n|  3.6433094|\n|  3.8995837|\n|  4.8850569|\n|  7.4816315|\n|  9.2464340|\n|  7.2600035|\n|  3.8371986|\n|  5.2976027|\n|  9.5031881|\n|  1.6360366|\n|  2.0667820|\n|  5.6629904|\n|  5.3926843|\n|  0.6904280|\n|  9.2293647|\n|  2.5340768|\n|  5.3347039|\n|  4.7395661|\n|  9.3761433|\n|  0.9122518|\n|  8.5049099|\n|  4.3271392|\n|  9.4414074|\n| -0.3818308|\n|  1.1997303|\n|  0.2092529|\n|  7.3398047|\n| 13.3067511|\n|  3.8234740|\n|  1.3570409|\n|  6.7523653|\n|  6.4900025|\n| -0.7842835|\n|  2.4121447|\n|  7.4250145|\n|  4.0034491|\n|  6.0303675|\n|  5.9445652|\n|  5.6623623|\n|  2.7837142|\n|  4.2680377|\n|  8.1597372|\n|  4.8997639|\n|  9.2750552|\n|  4.6735481|\n|  2.3362903|\n|  5.6321530|\n| -4.3415974|\n|  3.1087472|\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_cities <- tibble(population = rnorm(100, 7, 3))\n\nnew_cities <- new_cities |> \n  bind_cols(predict(lm_model_fit, new_data = new_cities, type = \"pred_int\")) |> \n  kable()\n```\n:::\n\n\n\nNote: New data must be a data frame with the same columns names as the training data\n\n\n# Multiple Linear Regression\n\n## Multiple Linear Regression\n\n- Response: $Y$\n- Predictor Variables: $X_1, X_2, \\ldots, X_p$\n- Assume true relationship:\n\n$$\n\\begin{aligned}\nY&=f(\\mathbf{X}) + \\epsilon\\\\\n&=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\n\\end{aligned}\n$$\nwhere $\\beta_j$ quantifies the association between the $j^{th}$ predictor and the response.\n\n\n\n## [Multiple Linear Regression: Estimating Parameters]{.r-fit-text} {.smaller}\n\n- Suppose $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ are estimates of $\\beta_0, \\beta_1, \\ldots, \\beta_p$\n- Training Data:\n  + Observed response: $y_i$ for $i=1,\\ldots,n$\n  + Observed predictors: $x_{1i}, x_{2i}, \\ldots x_{pi}$ for $i=1,\\ldots, n$\n- Predicted response: \n$$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\ldots + \\hat{\\beta}_px_{pi} \\text{ for } i=1, \\ldots, n$$\n- Residuals: $e_i = \\hat{y}_i - y_i$ for $i=1, \\ldots, n$\n- Mean Squared Error (MSE): $MSE =\\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n}$\n\n## [Multiple Linear Regression: Estimating Parameters]{.r-fit-text}\n\n- **Goal:** Use *training data* to find $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ that minimizes MSE\n  + $\\hat{\\beta}_i$'s called **least-squares estimators**\n  + Since minimizing MSE $\\implies$ MSE is called **cost/loss function**\n- Can use calculus or gradient descent to find $\\hat{\\beta}_i$'s\n\n\n\n## House Prices dataset {.smaller}\n\n- `size` is in square feet\n- `num_bedrooms` is a count\n- `price` is in $1,000's\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhouse_prices <- readRDS(\"../data/house_prices.rds\")   # load dataset\nhead(house_prices, 6) |> kable()  # print first 6 observations\n```\n\n::: {.cell-output-display}\n\n\n| size| num_bedrooms| price|\n|----:|------------:|-----:|\n| 2104|            3| 399.9|\n| 1600|            3| 329.9|\n| 2400|            3| 369.0|\n| 1416|            2| 232.0|\n| 3000|            4| 539.9|\n| 1985|            4| 299.9|\n\n\n:::\n:::\n\n\n\n\n## Multiple Linear Regression\n\nSome Exploratory Data Analysis (EDA)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GGally)\nggpairs(data = house_prices)   # correlation plot\n```\n\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n## Multiple Linear Regression in R {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_model <- linear_reg() |> \n  set_engine(\"lm\")\n\nhouse_price_mlr <- mlr_model |> \n  fit(price ~ size + num_bedrooms, data = house_prices)   # fit the model\n\nhouse_price_mlr |> \n  tidy() |>   # produce result summaries of the model\n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term         |   estimate|  std.error|  statistic|   p.value|\n|:------------|----------:|----------:|----------:|---------:|\n|(Intercept)  | 89.5977660| 41.7674230|  2.1451591| 0.0374991|\n|size         |  0.1392106|  0.0147951|  9.4092391| 0.0000000|\n|num_bedrooms | -8.7379154| 15.4506975| -0.5655353| 0.5745825|\n\n\n:::\n:::\n\n\n\n## [Multiple Linear Regression: Interpreting Parameters]{.r-fit-text} {.smaller}\n\n- $\\hat{\\beta}_0=89.5978$: The intercept $\\implies$ a house with 0 square feet and 0 bedrooms would cost approximately \\$89,598.80. Is this meaningful in context? [Not really]{.fragment .fade-in}\n- $\\hat{\\beta}_1=0.1392$: With `num_bedrooms` remaining fixed, an additional 1 square foot of `size` leads to an increase in `price` by approximately \\$139.20.\n- $\\hat{\\beta}_2=-8.7379$: With `size` remaining fixed, an additional bedroom leads to an decrease in `price` of approximately \\$8,737.90.\n\n. . .\n\n- Hmm.... that's a little weird...\n- **Simpson's Paradox:** when relationship between two variables disappears or reverses when controlling for a third, **confounding variable**\n\n## [Multiple Linear Regression: Interpreting Parameters]{.r-fit-text}\n\n:::{.incremental}\n- Write down our model in mathematical notation\n- $\\text{price} = 89.5978 + 0.1392\\times\\text{size} - 8.7379\\times\\text{num_bedrooms}$\n- $Y = 89.5978 + 0.1392X_1 - 8.7379X_2$\n:::\n\n\n## Multiple Linear Regression: Prediction {.smaller}\n\n- Prediction of `price` when `size` is 2000 square feet for a house with 3 bedrooms\n- $\\text{sales} = 89.5978 + 0.1392\\times2000 - 8.7379\\times3 = 341.7841$\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(house_price_mlr, new_data = tibble(size = 2000, num_bedrooms = 3))   # obtain prediction\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  .pred\n  <dbl>\n1  342.\n```\n\n\n:::\n:::\n\n\n- Why don't these match exactly? [**rounding**]{.fragment .fade-in}\n\n\n## Linear Regression: Comparing Models {.smaller}\n\n:::{.fragment}\n:::{.incremental}\n- Many methods for comparing regression models from your regression course\n- Today: Data splitting\n- First: New Data\n:::\n:::\n:::{.fragment}\n- **ames housing data**\n  + Many variables\n  + Focus on:\n    * `Sale_Price`: in dollars\n    * `Gr_Liv_Area`: size in square feet\n    * `Bedroom_AbvGr`: number of bedrooms above grade\n:::\n\n\n\n## Comparing Models: Data Splitting {.smaller}\n\n- Split `ames` data set into two parts\n  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model\n  + Test set: randomly selected proportion $1-p$ of data used for estimating prediction error\n- If comparing A LOT of models, split into *three* parts to prevent **information leakage**\n  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model\n  + Validation set: randomly selected proportion $q$ (typically 20-30%) of data used to choosing tuning parameters\n  + Test set: randomly selected proportion $1-p-q$ of data used for estimating prediction error\n- Idea: use data your model hasn't seen to get more accurate estimate of error and prevent overfitting\n\n## Comparing Models: Data Splitting with `tidymodels` {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427) # Why?\n\names_split <- initial_split(ames, prop = 0.70, strata = Sale_Price) # initialize 70/30\names_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<2049/881/2930>\n```\n\n\n:::\n\n```{.r .cell-code}\names_train <- training(ames_split) # get training data\names_test <- testing(ames_split) # get test data\n```\n:::\n\n\n\n- `strata` not necessary but good practice\n  + `strata` will use *stratified sampling* on the variable you specify (very little downside) \n\n## Linear Regression: Comparing Models {.smaller}\n\n- Let's create three models with `Sale_Price` as the response:\n  + **fit1**: a linear regression model with `Bedroom_AbvGr`  as the only predictor\n  + **fit2**: a linear regression model with `Gr_Liv_Area` as the only predictor\n  + **fit3** (similar to model in previous slides): a multiple regression model with `Gr_Liv_Area` and `Bedroom_AbvGr` as predictors\n  + **fit4**: super flexible model which fits a 10th degree polynomial to `Gr_Liv_Area` and a 2nd degree polynomial to `Bedroom_AbvGr`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- mlr_model |> fit(Sale_Price ~ Bedroom_AbvGr, data = ames_train) # Use only training set\nfit2 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area, data = ames_train)\nfit3 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area + Bedroom_AbvGr, data = ames_train)\nfit4 <- mlr_model |> fit(Sale_Price ~ poly(Gr_Liv_Area, degree = 10) + poly(Bedroom_AbvGr, degree = 2), data = ames_train)\n```\n:::\n\n\n\n## Computing MSE {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit 1\nfit1_train_mse <- mean((ames_train$Sale_Price - predict(fit1, new_data = ames_train)$.pred)^2)\nfit1_test_mse <- mean((ames_test$Sale_Price - predict(fit1, new_data = ames_test)$.pred)^2)\n\n# Fit 2\nfit2_train_mse <- mean((ames_train$Sale_Price - predict(fit2, new_data = ames_train)$.pred)^2)\nfit2_test_mse <- mean((ames_test$Sale_Price - predict(fit2, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit3_train_mse <- mean((ames_train$Sale_Price - predict(fit3, , new_data = ames_train)$.pred)^2)\nfit3_test_mse <- mean((ames_test$Sale_Price - predict(fit3, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit4_train_mse <- mean((ames_train$Sale_Price - predict(fit4, , new_data = ames_train)$.pred)^2)\nfit4_test_mse <- mean((ames_test$Sale_Price - predict(fit4, new_data = ames_test)$.pred)^2)\n```\n:::\n\n\n\n## [Question]{style=\"color:blue\"}\n\nWithout looking at the numbers\n\n1. Do we know which of the following is the smallest: `fit1_train_mse`, `fit2_train_mse`, `fit3_train_mse`, `fit4_train_mse`? [Yes, `fit4_train_mse`]{.fragment .fade-in}\n2. Do we know which of the following is the smallest: `fit1_test_mse`, `fit2_test_mse`, `fit3_test_mse`, `fit4_test_mse`? [No]{.fragment .fade-in}\n\n## Choosing a Model {.smaller}\n\n::::{.columns}\n:::{.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Training Errors\nc(fit1_train_mse, fit2_train_mse, \n  fit3_train_mse, fit4_train_mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6213135279 3188099910 2781293767 2472424544\n```\n\n\n:::\n\n```{.r .cell-code}\nwhich.min(c(fit1_train_mse, fit2_train_mse, \n            fit3_train_mse, fit4_train_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# test Errors\nc(fit1_test_mse, fit2_test_mse, \n  fit3_test_mse, fit4_test_mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.329031e+09 3.203895e+09 2.732389e+09 2.726084e+12\n```\n\n\n:::\n\n```{.r .cell-code}\nwhich.min(c(fit1_test_mse, fit2_test_mse, \n            fit3_test_mse, fit4_test_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\n:::\n:::{.column}\n- `fit4` has the lowest training MSE (to be expected)\n- `fit3` has the lowest test MSE\n  + We would choose `fit3`\n- Anything else interesting we see?\n:::\n::::\n\n# K-Nearest Neighbors\n\n## Regression: Conditional Averaging {.smaller}\n\n**Restaurant Outlets Profit dataset**\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n\nWhat is a good value of $\\hat{f}(x)$ (expected profit), say at $x=6$?\n\nA possible choice is the **average of the observed responses** at $x=6$. But we may not observe responses for certain $x$ values.\n\n\n## K-Nearest Neighbors (KNN) Regression  {.smaller}\n\n- Non-parametric approach\n- Formally: Given a value for $K$ and a test data point $x_0$,\n$$\\hat{f}(x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i=\\text{Average} \\ \\left(y_i \\ \\text{for all} \\ i:\\ x_i \\in \\mathcal{N}_0\\right) $$\nwhere $\\mathcal{N}_0$ is the set of the $K$ training observations closest to $x_0$.\n- Informally, average together the $K$ \"closest\" observations in your training set\n- \"Closeness\": usually use the **Euclidean metric** to measure distance\n- Euclidean distance between $\\mathbf{X}_i=(x_{i1}, x_{i2}, \\ldots, x_{ip})$ and $\\mathbf{x}_j=(x_{j1}, x_{j2}, \\ldots, x_{jp})$:\n$$||\\mathbf{x}_i-\\mathbf{x}_j||_2 = \\sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \\ldots + (x_{ip}-x_{jp    })^2}$$\n\n## [KNN Regression (single predictor): Fit]{.r-fit-text} {.smaller}\n\n::::{.columns}\n:::{.column}\n**$K=1$**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit1 <- nearest_neighbor(mode = \"regression\", neighbors = 1) |> \n  set_engine(\"kknn\") |> \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit1, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction\n```\n\n::: {.cell-output-display}\n\n\n|   .pred|\n|-------:|\n| 0.92695|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n\n:::\n:::{.column}\n**$K=5$**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit5 <- nearest_neighbor(mode = \"regression\", neighbors = 5) |> \n  set_engine(\"kknn\") |> \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit5, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction\n```\n\n::: {.cell-output-display}\n\n\n|    .pred|\n|--------:|\n| 4.113736|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n:::\n::::\n\n## Regression Methods: Comparison\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n\n## <span style=\"color:blue\">Question!!!</span>\n\nAs $K$ in KNN regression increases:\n\n- the flexibility of the fit $\\underline{\\hspace{5cm}}$ ([increases]{.fragment .highlight-red} /decreases)\n- the bias of the fit $\\underline{\\hspace{5cm}}$ (increases/[decreases]{.fragment .highlight-red} )\n- the variance of the fit $\\underline{\\hspace{5cm}}$ ([increases]{.fragment .highlight-red}/decreases)\n\n\n",
    "supporting": [
      "04-MultipleRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}