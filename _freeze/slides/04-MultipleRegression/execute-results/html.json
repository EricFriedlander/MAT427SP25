{
  "hash": "1db90b015d566f48ef758b619bcc4a42",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MAT-427: Multiple Linear Regression + Data Splitting'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(modeldata)\nlibrary(knitr)\n\ntidymodels_prefer()\n```\n:::\n\n\n\n## Multiple Linear Regression\n\n- Response: $Y$\n- Predictor Variables: $X_1, X_2, \\ldots, X_p$\n- Assume true relationship:\n\n$$\n\\begin{aligned}\nY&=f(\\mathbf{X}) + \\epsilon\\\\\n&=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\n\\end{aligned}\n$$\nwhere $\\beta_j$ quantifies the association between the $j^{th}$ predictor and the response.\n\n\n\n## [Multiple Linear Regression: Estimating Parameters]{.r-fit-text} {.smaller}\n\n- Suppose $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ are estimates of $\\beta_0, \\beta_1, \\ldots, \\beta_p$\n- Training Data:\n  + Observed response: $y_i$ for $i=1,\\ldots,n$\n  + Observed predictors: $x_{1i}, x_{2i}, \\ldots x_{pi}$ for $i=1,\\ldots, n$\n- Predicted response: \n$$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\ldots + \\hat{\\beta}_px_{pi} \\text{ for } i=1, \\ldots, n$$\n- Residuals: $e_i = \\hat{y}_i - y_i$ for $i=1, \\ldots, n$\n- Mean Squared Error (MSE): $MSE =\\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n}$\n\n## [Multiple Linear Regression: Estimating Parameters]{.r-fit-text}\n\n- **Goal:** Use *training data* to find $\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p$ that minimizes MSE\n  + $\\hat{\\beta}_i$'s called **least-squares estimators**\n  + Since minimizing MSE $\\implies$ MSE is called **cost/loss function**\n- Can use calculus or gradient descent to find $\\hat{\\beta}_i$'s\n\n\n\n## House Prices dataset {.smaller}\n\n- `size` is in square feet\n- `num_bedrooms` is a count\n- `price` is in $1,000's\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhouse_prices <- readRDS(\"../data/house_prices.rds\")   # load dataset\nhead(house_prices, 6) |> kable()  # print first 6 observations\n```\n\n::: {.cell-output-display}\n\n\n| size| num_bedrooms| price|\n|----:|------------:|-----:|\n| 2104|            3| 399.9|\n| 1600|            3| 329.9|\n| 2400|            3| 369.0|\n| 1416|            2| 232.0|\n| 3000|            4| 539.9|\n| 1985|            4| 299.9|\n\n\n:::\n:::\n\n\n\n\n## Multiple Linear Regression\n\nSome Exploratory Data Analysis (EDA)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GGally)\nggpairs(data = house_prices)   # correlation plot\n```\n\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n## Multiple Linear Regression in R {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_model <- linear_reg() |> \n  set_engine(\"lm\")\n\nhouse_price_mlr <- mlr_model |> \n  fit(price ~ size + num_bedrooms, data = house_prices)   # fit the model\n\nhouse_price_mlr |> \n  tidy() |>   # produce result summaries of the model\n  kable()\n```\n\n::: {.cell-output-display}\n\n\n|term         |   estimate|  std.error|  statistic|   p.value|\n|:------------|----------:|----------:|----------:|---------:|\n|(Intercept)  | 89.5977660| 41.7674230|  2.1451591| 0.0374991|\n|size         |  0.1392106|  0.0147951|  9.4092391| 0.0000000|\n|num_bedrooms | -8.7379154| 15.4506975| -0.5655353| 0.5745825|\n\n\n:::\n:::\n\n\n\n## [Multiple Linear Regression: Interpreting Parameters]{.r-fit-text} {.smaller}\n\n- $\\hat{\\beta}_0=89.5978$: The intercept $\\implies$ a house with 0 square feet and 0 bedrooms would cost approximately \\$89,598.80. Is this meaningful in context? [Not really]{.fragment .fade-in}\n- $\\hat{\\beta}_1=0.1392$: With `num_bedrooms` remaining fixed, an additional 1 square foot of `size` leads to an increase in `price` by approximately \\$139.20.\n- $\\hat{\\beta}_2=-8.7379$: With `size` remaining fixed, an additional bedroom leads to an decrease in `price` of approximately \\$8,737.90.\n\n. . .\n\n- Hmm.... that's a little weird...\n- **Simpson's Paradox:** when relationship between two variables disappears or reverses when controlling for a third, **confounding variable**\n\n## [Multiple Linear Regression: Interpreting Parameters]{.r-fit-text}\n\n:::{.incremental}\n- Write down our model in mathematical notation\n- $\\text{price} = 89.5978 + 0.1392\\times\\text{size} - 8.7379\\times\\text{num_bedrooms}$\n- $Y = 89.5978 + 0.1392X_1 - 8.7379X_2$\n:::\n\n\n## Multiple Linear Regression: Prediction {.smaller}\n\n- Prediction of `price` when `size` is 2000 square feet for a house with 3 bedrooms\n- $\\text{sales} = 89.5978 + 0.1392\\times2000 - 8.7379\\times3 = 341.7841$\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(house_price_mlr, new_data = tibble(size = 2000, num_bedrooms = 3))   # obtain prediction\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  .pred\n  <dbl>\n1  342.\n```\n\n\n:::\n:::\n\n\n- Why don't these match exactly? [**rounding**]{.fragment .fade-in}\n\n\n## Linear Regression: Comparing Models {.smaller}\n\n:::{.fragment}\n:::{.incremental}\n- Many methods for comparing regression models from your regression course\n- Today: Data splitting\n- First: New Data\n:::\n:::\n:::{.fragment}\n- **ames housing data**\n  + Many variables\n  + Focus on:\n    * `Sale_Price`: in dollars\n    * `Gr_Liv_Area`: size in square feet\n    * `Bedroom_AbvGr`: number of bedrooms above grade\n:::\n\n\n\n## Comparing Models: Data Splitting {.smaller}\n\n- Split `ames` data set into two parts\n  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model\n  + Test set: randomly selected proportion $1-p$ of data used for estimating prediction error\n- If comparing A LOT of models, split into *three* parts to prevent **information leakage**\n  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model\n  + Validation set: randomly selected proportion $q$ (typically 20-30%) of data used to choosing tuning parameters\n  + Test set: randomly selected proportion $1-p-q$ of data used for estimating prediction error\n- Idea: use data your model hasn't seen to get more accurate estimate of error and prevent overfitting\n\n## Comparing Models: Data Splitting with `tidymodels` {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(427) # Why?\n\names_split <- initial_split(ames, prop = 0.70, strata = Sale_Price) # initialize 70/30\names_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<2049/881/2930>\n```\n\n\n:::\n\n```{.r .cell-code}\names_train <- training(ames_split) # get training data\names_test <- testing(ames_split) # get test data\n```\n:::\n\n\n\n- `strata` not necessary but good practice\n  + `strata` will use *stratified sampling* on the variable you specify (very little downside) \n\n## Linear Regression: Comparing Models {.smaller}\n\n- Let's create three models with `Sale_Price` as the response:\n  + **fit1**: a linear regression model with `Bedroom_AbvGr`  as the only predictor\n  + **fit2**: a linear regression model with `Gr_Liv_Area` as the only predictor\n  + **fit3** (similar to model in previous slides): a multiple regression model with `Gr_Liv_Area` and `Bedroom_AbvGr` as predictors\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- mlr_model |> fit(Sale_Price ~ Bedroom_AbvGr, data = ames_train) # Use only training set\nfit2 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area, data = ames_train)\nfit3 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area + Bedroom_AbvGr, data = ames_train)\nfit4 <- mlr_model |> fit(Sale_Price ~ poly(Gr_Liv_Area, degree = 10) + poly(Bedroom_AbvGr, degree = 2), data = ames_train)\n```\n:::\n\n\n\n## Computing MSE {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit 1\nfit1_train_mse <- mean((ames_train$Sale_Price - predict(fit1, new_data = ames_train)$.pred)^2)\nfit1_test_mse <- mean((ames_test$Sale_Price - predict(fit1, new_data = ames_test)$.pred)^2)\n\n# Fit 2\nfit2_train_mse <- mean((ames_train$Sale_Price - predict(fit2, new_data = ames_train)$.pred)^2)\nfit2_test_mse <- mean((ames_test$Sale_Price - predict(fit2, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit3_train_mse <- mean((ames_train$Sale_Price - predict(fit3, , new_data = ames_train)$.pred)^2)\nfit3_test_mse <- mean((ames_test$Sale_Price - predict(fit3, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit4_train_mse <- mean((ames_train$Sale_Price - predict(fit4, , new_data = ames_train)$.pred)^2)\nfit4_test_mse <- mean((ames_test$Sale_Price - predict(fit4, new_data = ames_test)$.pred)^2)\n```\n:::\n\n\n\n## [Question]{style=\"color:blue\"}\n\nWithout looking at the numbers\n\n1. Do we know which of the following is the smallest: `fit1_train_mse`, `fit2_train_mse`, `fit3_train_mse`, `fit4_train_mse`? [Yes, `fit4_train_mse`]{.fragment .fade-in}\n2. Do we know which of the following is the smallest: `fit1_test_mse`, `fit2_test_mse`, `fit3_test_mse`, `fit4_test_mse`? [No]{.fragment .fade-in}\n\n## Choosing a Model {.smaller}\n\n::::{.columns}\n:::{.column}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Training Errors\nc(fit1_train_mse, fit2_train_mse, \n  fit3_train_mse, fit4_train_mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6213135279 3188099910 2781293767 2472424544\n```\n\n\n:::\n\n```{.r .cell-code}\nwhich.min(c(fit1_train_mse, fit2_train_mse, \n            fit3_train_mse, fit4_train_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# test Errors\nc(fit1_test_mse, fit2_test_mse, \n  fit3_test_mse, fit4_test_mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.329031e+09 3.203895e+09 2.732389e+09 2.726084e+12\n```\n\n\n:::\n\n```{.r .cell-code}\nwhich.min(c(fit1_test_mse, fit2_test_mse, \n            fit3_test_mse, fit4_test_mse))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\n:::\n:::{.column}\n- `fit4` has the lowest training MSE (to be expected)\n- `fit3` has the lowest test MSE\n  + We would choose `fit3`\n- Anything else interesting we see?\n:::\n::::\n\n# K-Nearest Neighbors\n\n## Regression: Conditional Averaging {.smaller}\n\n**Restaurant Outlets Profit dataset**\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n\nWhat is a good value of $\\hat{f}(x)$ (expected profit), say at $x=6$?\n\nA possible choice is the **average of the observed responses** at $x=6$. But we may not observe responses for certain $x$ values.\n\n\n## K-Nearest Neighbors (KNN) Regression  {.smaller}\n\n- Non-parametric approach\n- Formally: Given a value for $K$ and a test data point $x_0$,\n$$\\hat{f}(x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i=\\text{Average} \\ \\left(y_i \\ \\text{for all} \\ i:\\ x_i \\in \\mathcal{N}_0\\right) $$\nwhere $\\mathcal{N}_0$ is the set of the $K$ training observations closest to $x_0$.\n- Informally, average together the $K$ \"closest\" observations in your training set\n- \"Closeness\": usually use the **Euclidean metric** to measure distance\n- Euclidean distance between $\\mathbf{X}_i=(x_{i1}, x_{i2}, \\ldots, x_{ip})$ and $\\mathbf{x}_j=(x_{j1}, x_{j2}, \\ldots, x_{jp})$:\n$$||\\mathbf{x}_i-\\mathbf{x}_j||_2 = \\sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \\ldots + (x_{ip}-x_{jp    })^2}$$\n\n## [KNN Regression (single predictor): Fit]{.r-fit-text} {.smaller}\n\n::::{.columns}\n:::{.column}\n**$K=1$**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit1 <- nearest_neighbor(mode = \"regression\", neighbors = 1) |> \n  set_engine(\"kknn\") |> \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit1, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction\n```\n\n::: {.cell-output-display}\n\n\n|   .pred|\n|-------:|\n| 0.92695|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\n\n\n:::\n:::{.column}\n**$K=5$**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit5 <- nearest_neighbor(mode = \"regression\", neighbors = 5) |> \n  set_engine(\"kknn\") |> \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit5, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction\n```\n\n::: {.cell-output-display}\n\n\n|    .pred|\n|--------:|\n| 4.113736|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n:::\n::::\n\n## Regression Methods: Comparison\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](04-MultipleRegression_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n\n\n## <span style=\"color:blue\">Question!!!</span>\n\nAs $K$ in KNN regression increases:\n\n- the flexibility of the fit $\\underline{\\hspace{5cm}}$ ([increases]{.fragment .highlight-red} /decreases)\n- the bias of the fit $\\underline{\\hspace{5cm}}$ (increases/[decreases]{.fragment .highlight-red} )\n- the variance of the fit $\\underline{\\hspace{5cm}}$ ([increases]{.fragment .highlight-red}/decreases)\n\n\n",
    "supporting": [
      "04-MultipleRegression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}