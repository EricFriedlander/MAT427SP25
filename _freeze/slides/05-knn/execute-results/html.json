{
  "hash": "15f80fb03c211e7e8e8dc0871f6b3d6d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'MAT-427: Multiple Linear Regression + Data Splitting'\nauthor: Eric Friedlander\nfooter: \"[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n---\n\n\n\n\n## Computational Setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gridExtra)\nlibrary(modeldata)\nlibrary(knitr)\n\ntidymodels_prefer()\n```\n:::\n\n\n\n# K-Nearest Neighbors\n\n## Regression: Conditional Averaging {.smaller}\n\n**Restaurant Outlets Profit dataset**\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-knn_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\n\n\nWhat is a good value of $\\hat{f}(x)$ (expected profit), say at $x=6$?\n\nA possible choice is the **average of the observed responses** at $x=6$. But we may not observe responses for certain $x$ values.\n\n\n## K-Nearest Neighbors (KNN) Regression  {.smaller}\n\n- Non-parametric approach\n- Formally: Given a value for $K$ and a test data point $x_0$,\n$$\\hat{f}(x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i=\\text{Average} \\ \\left(y_i \\ \\text{for all} \\ i:\\ x_i \\in \\mathcal{N}_0\\right) $$\nwhere $\\mathcal{N}_0$ is the set of the $K$ training observations closest to $x_0$.\n- Informally, average together the $K$ \"closest\" observations in your training set\n- \"Closeness\": usually use the **Euclidean metric** to measure distance\n- Euclidean distance between $\\mathbf{X}_i=(x_{i1}, x_{i2}, \\ldots, x_{ip})$ and $\\mathbf{x}_j=(x_{j1}, x_{j2}, \\ldots, x_{jp})$:\n$$||\\mathbf{x}_i-\\mathbf{x}_j||_2 = \\sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \\ldots + (x_{ip}-x_{jp    })^2}$$\n\n## [KNN Regression (single predictor): Fit]{.r-fit-text} {.smaller}\n\n::::{.columns}\n:::{.column}\n**$K=1$**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit1 <- nearest_neighbor(mode = \"regression\", neighbors = 1) |> \n  set_engine(\"kknn\") |> \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit1, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction\n```\n\n::: {.cell-output-display}\n\n\n|   .pred|\n|-------:|\n| 0.92695|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-knn_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n\n:::\n:::{.column}\n**$K=5$**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit5 <- nearest_neighbor(mode = \"regression\", neighbors = 5) |> \n  set_engine(\"kknn\") |> \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit5, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction\n```\n\n::: {.cell-output-display}\n\n\n|    .pred|\n|--------:|\n| 4.113736|\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-knn_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n:::\n::::\n\n## Regression Methods: Comparison\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05-knn_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n\n\n## <span style=\"color:blue\">Question!!!</span>\n\nAs $K$ in KNN regression increases:\n\n- the flexibility of the fit $\\underline{\\hspace{5cm}}$ ([increases]{.fragment .highlight-red} /decreases)\n- the bias of the fit $\\underline{\\hspace{5cm}}$ (increases/[decreases]{.fragment .highlight-red} )\n- the variance of the fit $\\underline{\\hspace{5cm}}$ ([increases]{.fragment .highlight-red}/decreases)\n\n\n## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}\n\n- Let's look at the `house_prices` data\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 3\n  Sale_Price Gr_Liv_Area Bedroom_AbvGr\n       <int>       <int>         <int>\n1     215000        1656             3\n2     105000         896             2\n3     172000        1329             3\n4     244000        2110             3\n5     189900        1629             3\n6     195500        1604             3\n```\n\n\n:::\n:::\n\n\n:::{.fragment}\n:::{.incremental}\n- Should 1 square foot count the same as 1 bedroom?\n- Need to **center and scale** (freq. just say scale)\n  + subtract mean from each predictor\n  + divide by standard deviation of each predictor\n  + compares apples-to-apples\n:::\n:::\n\n## Scaling in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# scale predictors\names_scaled <- tibble(size_scaled = scale(ames$Gr_Liv_Area),\n                                  num_bedrooms_scaled = scale(ames$Bedroom_AbvGr),\n                                  price = ames$Sale_Price)\n\nhead(ames_scaled)   # first six observations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 3\n  size_scaled[,1] num_bedrooms_scaled[,1]  price\n            <dbl>                   <dbl>  <int>\n1           0.309                   0.176 215000\n2          -1.19                   -1.03  105000\n3          -0.338                   0.176 172000\n4           1.21                    0.176 244000\n5           0.256                   0.176 189900\n6           0.206                   0.176 195500\n```\n\n\n:::\n:::\n\n\n\n\n## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nknnfit10 <- knnreg(price ~ size_scaled + num_bedrooms_scaled, data = ames_scaled, k = 10)   # 10-nn regression\n```\n:::\n\n\n\n- Must also scale test data points **using mean and sd from training set!!!!**\n- Test Point: `size` = 2000 square feet, and `num_bedrooms` = 3, then\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# obtain 10-nn prediction\n\npredict(knnfit10, newdata = tibble(size_scaled = (2000 - mean(ames$Gr_Liv_Area))/sd(ames$Gr_Liv_Area),\n                                     num_bedrooms_scaled = (3 - mean(ames$Bedroom_AbvGr))/sd(ames$Bedroom_AbvGr)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 259490\n```\n\n\n:::\n:::\n\n\n\n\n## [Linear Regression vs K-Nearest Neighbors]{.r-fit-text} {.smaller}\n\n- Linear regression is a parametric approach (with restrictive assumptions), KNN is non-parametric.\n- Linear regression works for regression problems ($Y$ numerical), KNN can be used for both regression and classification - i.e. $Y$ qualitative (next lesson)\n- Linear regression is interpretable, KNN is not.\n- Linear regression can accommodate qualitative predictors and can be extended to include interaction terms as well while KNN does not allow for qualitative predictors\n- Performance: KNN can be pretty good for small $p$, that is, $p \\le 4$ and large $n$. Performance of KNN deteriorates as $p$ increases - *curse of dimensionality*\n\n## Classification Problems {.smaller}\n\n- Response $Y$ is qualitative (categorical).\n- Objective: build a classifier $\\hat{Y}=\\hat{C}(\\mathbf{X})$\n  + assigns class label to a future unlabeled (unseen) observations\n  + understand the relationship between the predictors and response\n- Two ways to make predictions\n  + Class probabilities\n  + Class labels\n\n## Classification Problems: Example\n\n**Default dataset**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR2)   # load library\ndata(\"Default\")   # load dataset\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Default)   # print first six observations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(Default$default)   # class frequencies\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  No  Yes \n9667  333 \n```\n\n\n:::\n:::\n\n\n\n\n**We will consider `default` as the response variable.**\n\n\n## Classification Problems: Example\n\nFor some algorithms, we might need to convert the categorical response to numeric (0/1) values.\n\n**Default dataset**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDefault$default_id <- ifelse(Default$default == \"Yes\", 1, 0)   # create 0/1 variable\n\nhead(Default, 10)   # print first ten observations\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   default student   balance    income default_id\n1       No      No  729.5265 44361.625          0\n2       No     Yes  817.1804 12106.135          0\n3       No      No 1073.5492 31767.139          0\n4       No      No  529.2506 35704.494          0\n5       No      No  785.6559 38463.496          0\n6       No     Yes  919.5885  7491.559          0\n7       No      No  825.5133 24905.227          0\n8       No     Yes  808.6675 17600.451          0\n9       No      No 1161.0579 37468.529          0\n10      No      No    0.0000 29275.268          0\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## K-Nearest Neighbors Classifier\n\nGiven a value for $K$ and a test data point $x_0$,\n$$P(Y=j | X=x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} I(y_i = j)$$\n\nwhere $\\mathcal{N}_0$ is known as the **neighborhood** of $x_0$.\n\n\nFor classification problems, the predictions are obtained in terms of **majority vote** (unlike in regression where predictions are obtained by averaging).\n\n\n## K-Nearest Neighbors Classifier: Build Model\n\n**Default dataset**\n\nresponse ($Y$): `default` and predictor ($X$): `balance`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknnfit <- knn3(default ~ balance, data = Default, k = 10)   # fit 10-nn model\n```\n:::\n\n\n\n\n## K-Nearest Neighbors Classifier: Predictions\n\n**Default dataset**\n\n* One can directly obtain the class label predictions as below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_class_preds_1 <- predict(knnfit, newdata = Default, type = \"class\")   # obtain default class label predictions\n```\n:::\n\n\n\n\n* Otherwise, one can first obtain predictions in terms of probabilities and then convert them into class label predictions based on a threshold.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_prob_preds <- predict(knnfit, newdata = Default, type = \"prob\")   # obtain predictions as probabilities\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nthreshold <- 0.5   # set threshold\n\nknn_class_preds_2 <- factor(ifelse(knn_prob_preds[,2] > threshold, \"Yes\", \"No\"))   # obtain predictions as class labels\n```\n:::\n\n\n\n\n## K-Nearest Neighbors Classifier: Performance  {.smaller}\n\n**Default dataset**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create confusion matrix\n\n# use the following code only when all predictions are from the same class\n# levels(knn_class_preds_1) = c(\"No\", \"Yes\")\n\nconfusionMatrix(data = knn_class_preds_1, reference = Default$default, positive = \"Yes\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   No  Yes\n       No  9619  215\n       Yes   48  118\n                                          \n               Accuracy : 0.9737          \n                 95% CI : (0.9704, 0.9767)\n    No Information Rate : 0.9667          \n    P-Value [Acc > NIR] : 3.067e-05       \n                                          \n                  Kappa : 0.461           \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n                                          \n            Sensitivity : 0.3544          \n            Specificity : 0.9950          \n         Pos Pred Value : 0.7108          \n         Neg Pred Value : 0.9781          \n             Prevalence : 0.0333          \n         Detection Rate : 0.0118          \n   Detection Prevalence : 0.0166          \n      Balanced Accuracy : 0.6747          \n                                          \n       'Positive' Class : Yes             \n                                          \n```\n\n\n:::\n:::\n",
    "supporting": [
      "05-knn_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}