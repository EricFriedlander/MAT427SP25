{
  "hash": "dbd2698fa3a74190f2e19993c25a6e6c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Homework 2: Intro to Regression\"\nauthor: \"Our names\"\neditor: visual\nformat:\n  html:\n    embed-resources: true\n---\n\n\n\n\n\n\n# Introduction\n\nIn this homework, you will practice multiple linear regression by working with \nthe data set `Carseats` from the package `ISLR2`. In addition, you will practice\ncollaborating with a team over GitHub. The process of collaborating over GitHub is similar\nto when you completed your work alone. You will clone the repo to you computer, \ndo your work, and then stage, commit, and push changes to GitHub. Once you push \nchanges to GitHub, your partners should be able to then pull your changes to their computer.\nThis data set contains simulated data of child car seat sales at 400 different stores.\n\n# The Data\n\nYour prediction task in this homework will be to predict `Sales` from some combination of the other variables.\nLook at the help documentation for this data to explore the available variables.\n\n## Exercise 1\n\n::: {.callout-tip title=\"Question\"}\nInstall the `ISLR2` package if you haven't and then load it. You should be able to access the dataset `Carseats`. Based on your knowledge of the world, which features do you think will be most predictive of `Sales`. Hint: `?Carseats` will give you more information on the data set and the variables.\n:::\n\n# Data Splitting\n\n## Exercise 2\n\n::: {.callout-tip title=\"Question\"}\nUse `tidymodels` to create a training and test set from the `Carseats` data using a 70-30 split. Note that this is a random process so you will get different partitions every time you split your data. As a result, it is considered good practice to set your seed so that the results a reproducible. For this homework please use the seed 427. For the training set, what quantitative variable is most highly correlated with `Sales`, our target variable?\n:::\n\n# Fitting Our First Model\n\nIn predictive modeling, we often begin with a simple baseline model, \nto which we compare other models. Any more complicated model must outperform the baseline model to be considered useful.\n\n## Exercise 3\n\n::: {.callout-tip title=\"Question\"}\nFit a linear regression model predicting `Sales` using the variable you identified in Exercise 2. Write down the resulting model in the form: $$Price = \\beta_0 + \\beta_1\\times Variable$$ When you get to this part, if you have no idea how to do this, Dr. Friedlander can show you. Don't forget to use your training set rather than the full data to train your model.\n:::\n\n\n# Evaluating Our Model\n\nLet's now see how our model performs. \n\n## Exercise 4\n\n::: {.callout-tip title=\"Question\"}\nCompute the RMSE for your baseline model on both the training and test set and report them. Try and report your results [inline](https://quarto.org/docs/get-started/computations/rstudio.html#inline-code).\n:::\n\nThe second primary metric that we can use to assess the accuracy of regression models is called the **coefficient of determination**, denoted $R^2$. $R^2$ is the proportion of variance (information) in our target variable that is explained by our model and can be computed by squaring $R$, the correlation coefficient between the target variable $y$ and the predicted target $\\hat{y}$. The `lm` function actually computes the $R^2$ of our training data for us which we can access using the `glance` function from the `broom` package which is included in `tidymodels` so you don't need to load it.\n\n## Exercise 5\n\n::: {.callout-tip title=\"Question\"}\nWhat proportion of the variation in `Sales` is explained by our baseline model for the training and validation sets?\n:::\n\n# Categorical Predictors\n\nLet's start to expand our model a bit by adding the predictor `US`.\n\n## Exercise 6\n\n::: {.callout-tip title=\"Question\"}\nBuild a two-input linear model for `Sales` by adding `US` in addition to the \nvariable you selected above. Save your model as `lmfit1`.\nUse `tidy` and `kable` to output the model. Is the coefficient for `Price` the \nsame or different than it was in our baseline model?\n:::\n\nNotice that the only coefficient added for `US` is called `USYes`. When you build a linear model with a categorical variable, R will introduce *dummy variables* which encode each category as a vector of 0's and 1's. In data science, this is sometimes called *one-hot encoding*. One level is always lumped into the intercept coefficient and is called the *reference level*. In this case, the reference level is `No`. When including a categorical variable in a linear model, you can interpret the resulting line being shifted up or down based on the category of a given observation.\n\nLet's now assess the accuracy of our new model. To make computation of RMSE and $R^2$ easier let's take advantage of the `rmse` and `r2` functions in the `yardstick` package.\n\n## Exercise 7\n\n::: {.callout-tip title=\"Question\"}\nUse the `rmse`` and `rsq` functions from the `yardstick` package to compute the RMSE and $R^2$ values for this new model on both the training and validation sets. How do these compare to the baseline model?\n:::\n\n\n# Overfitting and the Bias-Variance Trade-Off\n\nNow let's see what happens if we add in ALL of the predictors to our model. This is sometimes referred to as the **full model**. To include all of your predictors in a model you can use the syntax `fit(Y ~ ., data)`.\n\n## Exercise 8\n\n::: {.callout-tip title=\"Question\"}\nFit a model using all of the predictors in your training data. Call the model `lmfull`. Assess the models accuracy on the training data and the test data, comparing it to the previous models we've fit, and comment on your results.\n:::\n\nYou should notice that while the accuracy metrics on the training data drastically improve, there is little to no difference in the metrics for the test set. This is because of a phenomenon known as **overfitting**. Overfitting occurs when your model starts matching the training TOO well. A good visualization of an overfit model is Figure 2 in [the Wikipedia article for overfitting](https://en.wikipedia.org/wiki/Overfitting). As you include more variables/information in your model, your performance will ALWAYS increase on your training data. This is one of the reasons we always use holdout sets. Eventually your model will begin to overalign to the noise in your training data and the accuracy on holdout sets will be level off and in most cases begin to degrade.\n\nWhen modeling there are two related trade-offs that you need to consider. The first is the trade-off between prediction accuracy and interpretability. In general, one can typically create models with better prediction accuracy by sacrificing interpretability (e.g. by including more variables in your model, transforming these variables, etc.). Another trade-off is something called the bias-variance trade-off. As we increase the complexity of a model, we allow it to account for more and more intricate patterns in our data. In theory, this allows the model to mimic more complex relationships between our predictors and our target variables, reducing **bias**. On the other hand, more complex models typically have more parameters which need to be estimated which require more data to estimate accurately. When you increase the complexity of a model you usually increase the variance of the estimates of model parameters and the predictions the model makes. In other words, the model will be much more sensitive to the noise in the data that you have. Bias and variance will both decrease the accuracy of your model so you should try to minimize both. However, past a certain point, it will be a trade-off between the two.\n\n## Exercise 9\n\n::: {.callout-tip title=\"Question\"}\nFind and fit a model which outperforms all the models we fit so far on the test set. Briefly summarize your results. The \"best\" model will receive a high-five from Dr. Friedlander. Feel free to use any techniques we've learned in this class, up to this point. I encourage you to try out different data transformations like polynomials.\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}