{
  "hash": "56bae231c27a6d120f4a98d0384f82bc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Homework 3: $K$-Nearest Neighbors\"\nauthor: \"Test name 1\"\neditor: visual\nformat:\n  html:\n    embed-resources: true\n---\n\n\n\n\n\n\n# Introduction\n\nIn this homework, you will practice applying the $K$-Nearest Neighbors (KNN) method \nwhich is capable of performing both classification and regression. You will also\npractice collaborating with a team over GitHub.\nThe basic idea is that predictions are made based on the $K$ observation in\nthe training data which are \"closest\" to the observation that we're making a\nprediction for. While many different **metrics** (i.e. measures of distance) can be used, we will work exclusively with the **Euclidean metric**: $$\\text{dist}(x, y) = \\sqrt{\\sum_{i=1}^p(x_i-y_i)^2}$$\nfor vectors $x = (x_1,\\ldots, x_p)$ and $y = (y_1,\\ldots,y_p)$.\n\n## Learning goals\n\nBy the end of the homework, you will...\n\n-   Be able to work simultaneously with teammates on the same document using GitHub\n-   Fit and interpret linear models\n-   Split data using `tidymodels`\n-   Compare and evaluate different linear models\n\n## KNN for Classification\n\nWe'll start by using KNN for classification.\n\n## Data\n\nWe will be working with the famous `iris` data\nset which consists of four measurements (in centimeters) for 150 plants \nbelonging to three species of iris. \nThis data set was first published in a classic 1936 paper by English statistician, and notable racist/proponent of Eugenics,\nRonald Fisher. In that paper, multivariate linear models were applied to classify \nthese plants. Of course, back then, model fitting was an extremely laborious \nprocess that was done without the aid of calculators or statistical software.\nTo access this data first load the package `datasets` and then load the data\nset `iris`.\n\n## Exercise 1\n\n::: {.callout-tip title=\"Question\"}\nImport the `iris` data set from the `datasets` package and take a look at the columns.\nWe would like to visualize the $K$-nearest neighbor method in two dimensions,\nso make a data set called `iris2` consisting of  the columns \n`Sepal.Width`, `Petal.Width`, and `Species`. \nSplit your data into training and test sets using a 70-30 split. \nIMPORTANT: Make sure that each species is represented proportionally in the \ntraining set by using the `strata` argument in the `initial_split` function! \nOnce again, set your seed to 301.\n:::\n\n## The KNN algorithm\n\n## Exercise 2\n\n::: {.callout-tip title=\"Question\"}\nPlot the points in your training set in the xy-plane colored by the `Species` labels.\n:::\n\n## Exercise 3\n\n::: {.callout-tip title=\"Question\"}\nAs the name suggests, the $K$-nearest neighbors (KNN) method classifies\na point based on the classification of the observations in the training set\nthat are nearest to that point. If $k > 1$, then the neighbors essentially\n\"vote\" on the classification of the point.\nBased on your graph, if $k = 1$, how would KNN classify a flower that had sepal\nwidth 3cm and petal width 1cm?\n:::\n\n## Exercise 4\n\n::: {.callout-tip title=\"Question\"}\nJust to verify that we are correct, find the sepal width, petal width,\nand species of the observation in your training set that is closest to our flower with\nsepal width 3cm and petal width 1cm.\n:::\n\n## Exercise 5\n\n::: {.callout-tip title=\"Question\"}\nCenter and scale your data sets. You should be using the mean\nand standard deviation from your training set to center and scale both data sets. \n:::\n\n## Exercise 6\n\n::: {.callout-tip title=\"Question\"}\nRather than implementing this method by hand, we can use the function\n`knn3` in the `caret` package. \n:::\n\n\nWe would like to understand how the method of $K$-nearest neighbors will\nclassify points in the plane. That is, we would like to view the\n_decision boundaries_ of this model.\nTo do this, we will use our model to classify a large grid of points\nin the plane, and color them by their classification.\nThe code below creates a data frame called `grid` consisting of 6.25\\times 10^{4}\npoints in the plane.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# g1 <- rep((200:450)*(1/100), 250)\n# g2 <- rep((0:250)*(1/100), each = 250)\n# grid <- tibble(  x1 = g1\n#                    , x2 = g2)\n```\n:::\n\n\n\n## Exercise 7\n\n::: {.callout-tip title=\"Question\"}\nUncomment the code above and classify the points in `grid` using your training data and $k = 1$. Then,\nplot the points in `grid` colored by their classification. Don't forget to\ncenter and scale the grid.\n:::\n\n## Exercise 8\n\n::: {.callout-tip title=\"Question\"}\n Notice that the decision boundary between versicolor and virginica\nlooks a little strange. What do you observe? Why do you think this is happening?\nDoes using $k = 2$ make things better or worse? Why do you think that is?\n:::\n\n## Exercise 9\n\n::: {.callout-tip title=\"Question\"}\nDetermine which value of $K$, the number of neighbors selected, gives the highest\naccuracy on the test set. Test all $K$s between 1 and 40. Note that there may be ties because our data set is a little bit too small. To break ties just choose the smallest $K$ among the ones which are tied.\n:::\n\n## Exercise 10\n\n::: {.callout-tip title=\"Question\"}\nUse your value of $K$ to classify the points in `test` based on this combined data set.\nYou must recenter and scale your data based on this combination. \nGenerate a confusion matrix and report the accuracy of your method?\n*Note.* You should \"set the seed\" in R by specifying `set.seed(301)`\nbefore you build your model. Because R randomly breaks ties, if you do not\nset the seed, you may get a different result the next time you knit your\ndocument (and your answer won't match your code).\n\nAwesome!! Your model probably did pretty well, because KNN performs really well\non the `iris` data set. However, this isn't a very challenging data set for most\nclassification methods. More challenging data sets have data on different\nscales and _class imbalance_ where there are very few observations belonging\nto a particular class.\n:::\n\n\n## Exercise 11\n\n::: {.callout-tip title=\"Question\"}\nKNN can also be modified to give probabilistic predictions\nlike we get from logistic regression. How would you modify the method to give a probabilistic\nprediction instead of a classification?\n:::\n\n# KNN for Regression\n\nFor regression, we can predict the response\nvariable for our point to be the average (or sometimes median) of the response variable for the\n$K$-nearest neighbors. \n\n## Data & Dummy Variables\n\nFor this portion of the homework, we'll return to using the `Carseat` data from the `ISLR` package that we worked with in homework 4. Frequently, when working with categorical data, you will be required to transform that data into **dummy variables**. Namely, you'll create a unique variable for each column which gets a 1 if the corresponding observation is from that category and a 0 otherwise. In data science, this format is sometimes referred to as **one-hot encoding**.\n\n## Exercise 12\n\n::: {.callout-tip title=\"Question\"}\nLoad the `Carseat` data from the `ISLR` package. Use the `dummyVars` function from the `caret` package to create dummy variables for the categorical variables in `Carseat`. Then, split the data into a training, validation, and test set using a 60-20-20 split and a seed of 301 (as usual). Explain what the `fullRank` options does and why we can create dummy variables before splitting our data.\n:::\n\nThere is no standard way of handling categorical variables when applying KNN to predictors that are categorical. This is because we need to answer the fundamental question of \"What is the distance between two observations that belong to different categories?\". There are several ways of approaching this. One is to use different metrics and another is to just convert your categorical variables into numerical variables by using this one-hot encoding. Past that, you'll need to decide whether to center and standardize your dummy variables.\n\n## Using KNN for Regression\n\nThe function `knnreg` from the `caret` package will apply KNN to a regression problem by taking the average of the $K$ nearest neighbors to the observation you're looking to make a prediction for. `knnreg` works very similar to `knn3` and previous models that we've fit in R.\n\n## Exercise 13\n\n::: {.callout-tip title=\"Question\"}\nFit a KNN model to predict `Sales` from the data we have. Fit your model on the training data and use the validation set to choose the appropriate variable and the number of neighbors to include. You may find it useful to plot the $R^2$ and RMSE against the number of neighbors you include in your model. Once you select a $K$ and the variables you want to include in your model, see how it performs on the test set and compare your result to the linear model you fit at the end of homework 4. Which model works better? You will find that the RMSE and $R^2$ disagree on what the best model is. You will have to make a judgement call on which model is \"best\". One thing that can be helpful is looking at plots of your target variables (`Sales` in this case) against the model residuals. When you do this, what pattern do you see?\n:::\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}