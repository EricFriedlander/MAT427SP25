[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nMWF 3:00 - 3:50pm\nCruzen-Murray Library (CML) 208\n\n\n(Optional) Homework Lab\nW 4:00 - 4:50pm (Tentative)\nCruzen-Murray Library (CML) 208\n\n\nOffice Hours\nM 9:30 - 10:30am\nBoone 126B\n\n\nOffice Hours\nT 9:00 - 10:00am\nBoone 126B\n\n\nOffice Hours\nW 1:30 - 2:30pm\nBoone 126B\n\n\nOffice Hours\nTH 10:00 - 11:00am\nBoone 126B\n\n\n\nOffice hours are also available by appointment, just email me!\n\n\n\n\nInstructor: Dr. Eric Friedlander\nOffice: Boone 126B\nEmail: efriedlander@collegeofidaho.edu",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nMWF 3:00 - 3:50pm\nCruzen-Murray Library (CML) 208\n\n\n(Optional) Homework Lab\nW 4:00 - 4:50pm (Tentative)\nCruzen-Murray Library (CML) 208\n\n\nOffice Hours\nM 9:30 - 10:30am\nBoone 126B\n\n\nOffice Hours\nT 9:00 - 10:00am\nBoone 126B\n\n\nOffice Hours\nW 1:30 - 2:30pm\nBoone 126B\n\n\nOffice Hours\nTH 10:00 - 11:00am\nBoone 126B\n\n\n\nOffice hours are also available by appointment, just email me!\n\n\n\n\nInstructor: Dr. Eric Friedlander\nOffice: Boone 126B\nEmail: efriedlander@collegeofidaho.edu",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of the semester, you will be able to…\n\ntackle predictive modeling problems arising from real data.\nuse R to fit and evaluate machine learning models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate results results through writing and oral presentations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nCollege of Idaho Honor Code\n\nThe College of Idaho maintains that academic honesty and integrity are essential values in the educational process. Operating under an Honor Code philosophy, the College expects conduct rooted in honesty, integrity, and understanding, allowing members of a diverse student body to live together and interact and learn from one another in ways that protect both personal freedom and community standards. Violations of academic honesty are addressed primarily by the instructor and may be referred to the Student Judicial Board.\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the College of Idaho Honor Code.\n\n\nDisability Accommodation Statement\nThe College of Idaho seeks to provide an educational environment that is accessible to the needs of students with disabilities. The College provides reasonable services to enrolled students who have a documented permanent or temporary physical, psychological, learning, intellectual, or sensory disability that qualifies the student for accommodations under the Americans with Disabilities Act or section 504 of the Rehabilitation Act of 1973. If you have, or think you may have, a disability that impacts your performance as a student in this class, you are encouraged to arrange support services and/or accommodations through the Department of Accessibility and Learning Excellence located in McCain 201B and available via email at accessibility@collegeofidaho.edu. Reasonable academic accommodations may be provided to students who submit appropriate and current documentation of their disability. Accommodations can be arranged only through this process and are not retroactively applied. More information can be found on the DALE webpage (https://www.collegeofidaho.edu/accessibility).\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, mat427fa25.netlify.app.\nPeriodic announcements will be sent via email and will also be available through Canvas and grades will be stored in the Canvas gradebook. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nIn class agreements\nIf we discuss/agree to something in class or office hours which requires action from me (e.g. “you may turn in your homework late due to a sporting event”), you MUST send me a follow-up message. If you don’t, I will almost certainly forget, and our agreement will be considered null and void.\n\n\nGetting help in the course\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nI am here to help you be successful in the course. You are encouraged to attend office hours and the homework lab to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them!\nOutside of class and office hours, any general questions about course content or assignments can be emailed to me.\n\n\n\nEmail\nIf you have questions about assignment extensions or accommodations, please email efriedlander@collegeofidaho.edu. Please see Late work policy for more information. If you email me about an error please include a screenshot of the error and the code causing the error. Barring extenuating circumstances, I will respond to MAT 427 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\nCheck out the Support page for more resources.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook",
    "text": "Textbook\nThe official textbook for this course is:\n\nAn Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hatie, and Robert Tibshirani\n\nColloquiually referred to as “ISLR”, it is considered one of the bibles of machine learning\nIt’s free!",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nYou will be assessed based on five components: homework, job applications, job interviews, a hack-a-thon, and project.\n\nHomework\nIn homework, you will apply what you’ve learned during lecture to complete data analysis tasks. Homework will completed in teams of three, must be typed up using Quarto, and submitted as .qmd and .pdf files in via GitHub.\n\n\nJob Applications & Job Interviews\nDuring this course you will apply to two “jobs”. I will generate the job advertisements including real companies and base the job description on the course content and similar job advertisements that I get from online or professional collaborators. Each job application will have three components:\n\nA cover letter.\nA resume.\nA portfolio.\n\nAll three of these should be tailored to the job description and the company to which you are applying. Your portfolio will consist of self-contained data analyses of your choosing. The most straight forward method of creating th to repurpose your homeworks, converting them from a format in which you are responding to exercises to something where you are telling a narrative and demonstrating that you meet the job criteria. To create your portfolio, you will be required to create a website. More details on this will be given during the semester, however the idea of this project is that you will be able to use the things you general when you are applying for jobs.\nAfter you submit your job applications, you will be invited to schedule a one-hour long job interview. It is your job to schedule your job interview with Dr. Friedlander. Each job interview will have three portions. The first, lasting 10-15 minutes, will include typical questions that apply to almost any job interview (e.g. “What are your biggest strengths and weaknesses”). The second, lasting 20-30 minutes, will include questions about the portfolio you submitted and your understanding of the required skills described in the job advertising. The third section will mimic what is called a “case interview”. Case interviews are extremely common for many jobs, especially those requiring quantitative or computational skills, and can be intimidating. During the case interview portion, you will be presented with a “case study” and asked questions on how you would go about approaching it. The cases themselves will be designed so that they can be solved using the content from class. The goal of this whole exercise is to assess your knowledge of the course content in a way that is authentic while also preparing you to get a job.\n\n\nHack-a-thon\nAt some point in the semester we will participate in a “Hack-a-thon” as a class. Namely, you will be given a short period of time (1-3 days) to build a model and make a set of predictions. After the competition is over, you will be required to present on your model. Part of your score will be determine by how well your model performs and extra credit will be given to the top scoring individuals.\n\n\nProject\nDuring the latter portion of the course, you will complete a final project that involves a deep exploration of a problem. More details for the final project will be provided later in the course.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n10%\n\n\nJob Application 1\n15%\n\n\nJob Application 2\n15%\n\n\nJob Interview 1\n15%\n\n\nJob Interview 2\n15%\n\n\nHack-a-thon & Presentation\n15%\n\n\nFinal Project\n15%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nTL;DR: Don’t cheat!\n\nThe job application assignments must be completed individually but you are welcome to discuss the assignment with classmates (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share (i.e. via copy/paste or copying) any code or prose with anyone other than myself.\nFor the hack-a-thon, everyone will submit their predictions and give their own presentations. However, you are encouraged to work together. You are allowed to share code with one another. However, everyone should be able to explain what they did and everyone’s projects should be unique in some way. Point reductions will be given if two individuals submit the exact same predictions.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g. StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity. In general if the following two things are not true, you are cheating:\n\nYou understand and can explain all of the code you have written down or you don’t and you have cited the source of that code.\nAll of your prose and narrative were written by yourself.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, just ask.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all College of Idaho policies, including academic integrity (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the Honor Code.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. I understand that things come up periodically that could make it difficult to submit an assignment by the deadline.\n\nLate Homework: Homework is completion based and well be accepted without penalty for a week. However, if your homework is turned in after I begin grading it, you will not receive any feedback.\nSchool-Sponsored Events/Illness: If an assignment or meeting must be missed due to a school-sponsored event, you must let me know at least a week ahead of time so that we can schedule a time for you to make up the work before you leave. If an assignment or meeting must be missed due to illness, you must let me know as soon as it is safe for you to do so and before the assignment or meeting if possible. Failure to adhere to this policy will result in a 35% penalty on the corresponding assignment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/08-classifiction-metrics.html#classification-problems",
    "href": "slides/08-classifiction-metrics.html#classification-problems",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "",
    "text": "Response \\(Y\\) is qualitative (categorical).\n\nOnly two classes \\(\\implies\\) Binary Classification Problem\n\nObjectives: build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\) that:\n\nassigns a class label to a future unlabeled (unseen) observation\nhelps understand the relationship between the predictors and response\n\nThere can be two types of predictions based on the research problem.\n\nClass probabilities\nClass labels"
  },
  {
    "objectID": "slides/08-classifiction-metrics.html#default-dataset",
    "href": "slides/08-classifiction-metrics.html#default-dataset",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Default Dataset",
    "text": "Default Dataset\n\n\nA simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\n\nhead(Default) |&gt; kable()  # print first six observations\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n729.5265\n44361.625\n\n\nNo\nYes\n817.1804\n12106.135\n\n\nNo\nNo\n1073.5492\n31767.139\n\n\nNo\nNo\n529.2506\n35704.494\n\n\nNo\nNo\n785.6559\n38463.496\n\n\nNo\nYes\n919.5885\n7491.559\n\n\n\n\n\n\nResponse Variable: default\n\nDefault |&gt; \n  tabyl(default) |&gt;  # class frequencies\n  kable()           # Make it look nice\n\n\n\n\ndefault\nn\npercent\n\n\n\n\nNo\n9667\n0.9667\n\n\nYes\n333\n0.0333"
  },
  {
    "objectID": "slides/08-classifiction-metrics.html#k-nearest-neighbors-classifier",
    "href": "slides/08-classifiction-metrics.html#k-nearest-neighbors-classifier",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier",
    "text": "K-Nearest Neighbors Classifier\n\nGiven a value for \\(K\\) and a test data point \\(x_0\\): \\[P(Y=j | X=x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} I(y_i = j)\\] where \\(\\mathcal{N}_0\\) is the set of the \\(K\\) “closest” neighbors.\nFor classification: neighbors “vote” for class (unlike in regression where predictions are obtained by averaging) \\[P(Y=j | X=x_0)=\\text{Proportion of neighbors in class }j\\]"
  },
  {
    "objectID": "slides/08-classifiction-metrics.html#k-nearest-neighbors-classifier-build-model",
    "href": "slides/08-classifiction-metrics.html#k-nearest-neighbors-classifier-build-model",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier: Build Model",
    "text": "K-Nearest Neighbors Classifier: Build Model\n\nResponse (\\(Y\\)): default\nPredictor (\\(X\\)): balance\n\n\nknnfit &lt;- nearest_neighbor(neighbors = 10) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"classification\") |&gt;  \n  fit(default ~ balance, data = Default)   # fit 10-nn model"
  },
  {
    "objectID": "slides/08-classifiction-metrics.html#k-nearest-neighbors-classifier-predictions",
    "href": "slides/08-classifiction-metrics.html#k-nearest-neighbors-classifier-predictions",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier: Predictions",
    "text": "K-Nearest Neighbors Classifier: Predictions\n\nProbabilitiesClass labelsThreshold\n\n\n\nDefault_wpreds &lt;-  Default |&gt; \n  bind_cols(predict(knnfit, new_data = Default, type = \"prob\"))   # obtain predictions as probabilities\nhead(Default_wpreds) |&gt; # predicted probabilities for first six observations\n  kable()\n\n\n\n\ndefault\nstudent\nbalance\nincome\n.pred_No\n.pred_Yes\n\n\n\n\nNo\nNo\n729.5265\n44361.625\n1\n0\n\n\nNo\nYes\n817.1804\n12106.135\n1\n0\n\n\nNo\nNo\n1073.5492\n31767.139\n1\n0\n\n\nNo\nNo\n529.2506\n35704.494\n1\n0\n\n\nNo\nNo\n785.6559\n38463.496\n1\n0\n\n\nNo\nYes\n919.5885\n7491.559\n1\n0\n\n\n\n\n\n\n\n\nPredicts class w/ maximum probability\n\n\nDefault_wpreds &lt;-  Default_wpreds |&gt; \n  bind_cols(predict(knnfit, new_data = Default, type = \"class\")) |&gt;    # obtain class label predictions directly\n  select(-.pred_No) |&gt; \n  rename(prob_yes = .pred_Yes, pred_50 = .pred_class)\nhead(Default_wpreds) |&gt;   # predicted class labels for first six observations\n  kable()\n\n\n\n\ndefault\nstudent\nbalance\nincome\nprob_yes\npred_50\n\n\n\n\nNo\nNo\n729.5265\n44361.625\n0\nNo\n\n\nNo\nYes\n817.1804\n12106.135\n0\nNo\n\n\nNo\nNo\n1073.5492\n31767.139\n0\nNo\n\n\nNo\nNo\n529.2506\n35704.494\n0\nNo\n\n\nNo\nNo\n785.6559\n38463.496\n0\nNo\n\n\nNo\nYes\n919.5885\n7491.559\n0\nNo\n\n\n\n\n\n\n\n\nMore effective when one class is more important and/or classes are imbalanced\n\n\nthreshold &lt;- 0.25   # set threshold\nDefault_wpreds &lt;- Default_wpreds |&gt; \n  mutate(pred_25 = as_factor(if_else(prob_yes &gt; threshold, \"Yes\", \"No\")))   # obtain predictions as class labels\nhead(Default_wpreds) |&gt;   kable()\n\n\n\n\ndefault\nstudent\nbalance\nincome\nprob_yes\npred_50\npred_25\n\n\n\n\nNo\nNo\n729.5265\n44361.625\n0\nNo\nNo\n\n\nNo\nYes\n817.1804\n12106.135\n0\nNo\nNo\n\n\nNo\nNo\n1073.5492\n31767.139\n0\nNo\nNo\n\n\nNo\nNo\n529.2506\n35704.494\n0\nNo\nNo\n\n\nNo\nNo\n785.6559\n38463.496\n0\nNo\nNo\n\n\nNo\nYes\n919.5885\n7491.559\n0\nNo\nNo"
  },
  {
    "objectID": "slides/08-classifiction-metrics.html#k-nearest-neighbors-classifier-performance",
    "href": "slides/08-classifiction-metrics.html#k-nearest-neighbors-classifier-performance",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier: Performance",
    "text": "K-Nearest Neighbors Classifier: Performance\n\nDefault_wpreds |&gt; \n  conf_mat(truth = default, estimate = pred_50)\n\n          Truth\nPrediction   No  Yes\n       No  9624  209\n       Yes   43  124"
  },
  {
    "objectID": "slides/08-classifiction-metrics.html#k-nearest-neighbors-classifier-performance-1",
    "href": "slides/08-classifiction-metrics.html#k-nearest-neighbors-classifier-performance-1",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier: Performance",
    "text": "K-Nearest Neighbors Classifier: Performance\n\nDefault_wpreds |&gt; \n  conf_mat(truth = default, estimate = pred_50) |&gt; \n  autoplot(type = \"heatmap\")"
  },
  {
    "objectID": "slides/08-classifiction-metrics.html#confusion-matrix-terms",
    "href": "slides/08-classifiction-metrics.html#confusion-matrix-terms",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Confusion Matrix Terms",
    "text": "Confusion Matrix Terms\n\n\n\n\n\n\n\n\n\nActual Positive/Event\nActual Negative/Non-event\n\n\n\n\nPredicted Positive/Event\nTrue Negative\nFalse Positive\n\n\nPredicted Negative/Non-event\nFalse Negative\nTrue Positive"
  },
  {
    "objectID": "slides/08-classifiction-metrics.html#roc-curve-and-auc",
    "href": "slides/08-classifiction-metrics.html#roc-curve-and-auc",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "ROC Curve and AUC",
    "text": "ROC Curve and AUC\n\nROC (Receiver Operating Characteristics) curve: popular graphic for comparing different classifiers across all possible thresholds\n\nPlots the Specificity (1-false positive rate) along the x-axis and the Sensitivity (true positive rate) along the y-axis\n\nAUC: area under the AUC curve\n\nIdeal ROC curve will hug the top left corner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;  –&gt;  –&gt;\n\n\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n\n\n–&gt;\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;"
  },
  {
    "objectID": "slides/06-knn-workflows.html#computational-setup",
    "href": "slides/06-knn-workflows.html#computational-setup",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(modeldata) # contains ames dataset\n\ntidymodels_prefer()"
  },
  {
    "objectID": "slides/06-knn-workflows.html#comparing-models-data-splitting-with-tidymodels",
    "href": "slides/06-knn-workflows.html#comparing-models-data-splitting-with-tidymodels",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Comparing Models: Data Splitting with tidymodels",
    "text": "Comparing Models: Data Splitting with tidymodels\n\nset.seed(427) # Why?\n\names_split &lt;- initial_split(ames, prop = 0.70, strata = Sale_Price) # initialize 70/30\names_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2049/881/2930&gt;\n\names_train &lt;- training(ames_split) # get training data\names_test &lt;- testing(ames_split) # get test data"
  },
  {
    "objectID": "slides/06-knn-workflows.html#k-nearest-neighbors-regression-multiple-predictors",
    "href": "slides/06-knn-workflows.html#k-nearest-neighbors-regression-multiple-predictors",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "K-Nearest Neighbors Regression (multiple predictors)",
    "text": "K-Nearest Neighbors Regression (multiple predictors)\n\names |&gt;\n  select(Sale_Price, Gr_Liv_Area, Bedroom_AbvGr) |&gt;\n  head() |&gt; \n  kable()\n\n\n\n\nSale_Price\nGr_Liv_Area\nBedroom_AbvGr\n\n\n\n\n215000\n1656\n3\n\n\n105000\n896\n2\n\n\n172000\n1329\n3\n\n\n244000\n2110\n3\n\n\n189900\n1629\n3\n\n\n195500\n1604\n3\n\n\n\n\n\n\n\nShould 1 square foot count the same as 1 bedroom?\nNeed to center and scale (freq. just say scale)\n\nsubtract mean from each predictor\ndivide by standard deviation of each predictor\ncompares apples-to-apples"
  },
  {
    "objectID": "slides/06-knn-workflows.html#new-observation",
    "href": "slides/06-knn-workflows.html#new-observation",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "New observation",
    "text": "New observation\n\n\nCode\nnew_house &lt;- tibble(Gr_Liv_Area = 2000, Bedroom_AbvGr = 2)\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr, color = Sale_Price)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\")\n\n\n\n\n\n\n\n\n\n\nWhere do you think the 10 nearest neighbors should be?"
  },
  {
    "objectID": "slides/06-knn-workflows.html#computing-distances",
    "href": "slides/06-knn-workflows.html#computing-distances",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Computing Distances",
    "text": "Computing Distances\n\names_dist &lt;- ames_train |&gt; \n  mutate(dist = sqrt((Gr_Liv_Area - 2000)^2 + (Bedroom_AbvGr-2)^2)) |&gt; \n  arrange(dist)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#nearest-neighbors",
    "href": "slides/06-knn-workflows.html#nearest-neighbors",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "10 “Nearest Neighbors”",
    "text": "10 “Nearest Neighbors”\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr, color = Sale_Price)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\") +\n  geom_point(data = ames_dist |&gt; slice(1:10), color = \"green\")\n\n\n\n\n\n\n\n\n\n\nAre they where you thought they should be?"
  },
  {
    "objectID": "slides/06-knn-workflows.html#looking-at-distances",
    "href": "slides/06-knn-workflows.html#looking-at-distances",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Looking at distances",
    "text": "Looking at distances\n\names_dist |&gt;\n  select(Gr_Liv_Area, Bedroom_AbvGr, dist) |&gt; \n  kable()\n\n\n\n\nGr_Liv_Area\nBedroom_AbvGr\ndist\n\n\n\n\n2000\n3\n1.000000\n\n\n2000\n3\n1.000000\n\n\n2004\n4\n4.472136\n\n\n1995\n4\n5.385165\n\n\n2006\n2\n6.000000\n\n\n2007\n3\n7.071068\n\n\n2007\n3\n7.071068\n\n\n1992\n3\n8.062258\n\n\n2008\n3\n8.062258\n\n\n1991\n3\n9.055385\n\n\n1991\n4\n9.219544\n\n\n2009\n4\n9.219544\n\n\n2009\n4\n9.219544\n\n\n1988\n4\n12.165525\n\n\n1987\n2\n13.000000\n\n\n2014\n2\n14.000000\n\n\n1986\n3\n14.035669\n\n\n2014\n3\n14.035669\n\n\n1986\n4\n14.142136\n\n\n2014\n5\n14.317821\n\n\n2016\n4\n16.124516\n\n\n1984\n5\n16.278821\n\n\n1983\n3\n17.029386\n\n\n2018\n3\n18.027756\n\n\n1982\n3\n18.027756\n\n\n2019\n3\n19.026298\n\n\n1981\n4\n19.104973\n\n\n1981\n4\n19.104973\n\n\n2020\n3\n20.024984\n\n\n2020\n3\n20.024984\n\n\n2020\n3\n20.024984\n\n\n1980\n3\n20.024984\n\n\n2021\n3\n21.023796\n\n\n1978\n2\n22.000000\n\n\n1978\n4\n22.090722\n\n\n2022\n4\n22.090722\n\n\n1977\n3\n23.021729\n\n\n1976\n2\n24.000000\n\n\n1973\n3\n27.018512\n\n\n2028\n2\n28.000000\n\n\n1971\n3\n29.017236\n\n\n1970\n3\n30.016662\n\n\n2031\n3\n31.016125\n\n\n2031\n3\n31.016125\n\n\n1968\n4\n32.062439\n\n\n1968\n4\n32.062439\n\n\n2034\n2\n34.000000\n\n\n2034\n3\n34.014703\n\n\n1966\n1\n34.014703\n\n\n2035\n3\n35.014283\n\n\n2036\n3\n36.013886\n\n\n1964\n3\n36.013886\n\n\n1964\n3\n36.013886\n\n\n1964\n4\n36.055513\n\n\n1964\n4\n36.055513\n\n\n2037\n3\n37.013511\n\n\n1962\n3\n38.013156\n\n\n1962\n4\n38.052595\n\n\n1960\n3\n40.012498\n\n\n1960\n4\n40.049969\n\n\n1959\n4\n41.048752\n\n\n1959\n5\n41.109610\n\n\n2042\n3\n42.011903\n\n\n1958\n3\n42.011903\n\n\n2042\n3\n42.011903\n\n\n1958\n3\n42.011903\n\n\n1958\n4\n42.047592\n\n\n1955\n4\n45.044423\n\n\n2046\n2\n46.000000\n\n\n2046\n3\n46.010868\n\n\n2046\n3\n46.010868\n\n\n1954\n3\n46.010868\n\n\n1953\n3\n47.010637\n\n\n2048\n3\n48.010416\n\n\n1952\n4\n48.041649\n\n\n2048\n5\n48.093659\n\n\n2049\n4\n49.040799\n\n\n2050\n4\n50.039984\n\n\n2052\n3\n52.009614\n\n\n1948\n4\n52.038447\n\n\n1947\n3\n53.009433\n\n\n2054\n3\n54.009259\n\n\n1944\n3\n56.008928\n\n\n1944\n3\n56.008928\n\n\n1944\n4\n56.035703\n\n\n2057\n3\n57.008771\n\n\n2058\n3\n58.008620\n\n\n1940\n3\n60.008333\n\n\n2060\n3\n60.008333\n\n\n2060\n4\n60.033324\n\n\n2061\n3\n61.008196\n\n\n1939\n3\n61.008196\n\n\n1939\n3\n61.008196\n\n\n2061\n4\n61.032778\n\n\n1938\n4\n62.032250\n\n\n2064\n3\n64.007812\n\n\n1936\n3\n64.007812\n\n\n2064\n4\n64.031242\n\n\n1935\n3\n65.007692\n\n\n1935\n3\n65.007692\n\n\n1934\n3\n66.007575\n\n\n2067\n3\n67.007462\n\n\n1933\n4\n67.029844\n\n\n1932\n2\n68.000000\n\n\n2068\n3\n68.007352\n\n\n1932\n3\n68.007352\n\n\n2069\n4\n69.028979\n\n\n1930\n4\n70.028566\n\n\n1929\n3\n71.007042\n\n\n2071\n4\n71.028163\n\n\n1928\n4\n72.027772\n\n\n1928\n4\n72.027772\n\n\n2073\n3\n73.006849\n\n\n1923\n3\n77.006493\n\n\n1922\n2\n78.000000\n\n\n2078\n3\n78.006410\n\n\n1922\n3\n78.006410\n\n\n1922\n4\n78.025637\n\n\n1921\n4\n79.025312\n\n\n1920\n4\n80.024996\n\n\n1920\n4\n80.024996\n\n\n1920\n4\n80.024996\n\n\n1916\n3\n84.005952\n\n\n1915\n3\n85.005882\n\n\n1914\n3\n86.005814\n\n\n1913\n3\n87.005747\n\n\n1912\n3\n88.005682\n\n\n2088\n4\n88.022724\n\n\n1911\n3\n89.005618\n\n\n2090\n3\n90.005555\n\n\n2090\n3\n90.005555\n\n\n1909\n4\n91.021975\n\n\n1908\n4\n92.021737\n\n\n1908\n4\n92.021737\n\n\n2093\n3\n93.005376\n\n\n1906\n3\n94.005319\n\n\n1905\n3\n95.005263\n\n\n1904\n3\n96.005208\n\n\n1904\n3\n96.005208\n\n\n2097\n3\n97.005155\n\n\n2097\n1\n97.005155\n\n\n2097\n3\n97.005155\n\n\n2098\n3\n98.005102\n\n\n1902\n4\n98.020406\n\n\n1902\n4\n98.020406\n\n\n2098\n4\n98.020406\n\n\n2100\n3\n100.005000\n\n\n1898\n3\n102.004902\n\n\n2104\n5\n104.043260\n\n\n1895\n3\n105.004762\n\n\n1894\n4\n106.018866\n\n\n2108\n4\n108.018517\n\n\n1891\n3\n109.004587\n\n\n2110\n3\n110.004545\n\n\n1889\n4\n111.018017\n\n\n1886\n4\n114.017543\n\n\n1884\n2\n116.000000\n\n\n1884\n2\n116.000000\n\n\n1884\n4\n116.017240\n\n\n1882\n4\n118.016948\n\n\n1880\n3\n120.004167\n\n\n1879\n3\n121.004132\n\n\n2121\n3\n121.004132\n\n\n2122\n4\n122.016392\n\n\n1877\n4\n123.016259\n\n\n1876\n4\n124.016128\n\n\n2125\n4\n125.015999\n\n\n2126\n3\n126.003968\n\n\n1874\n3\n126.003968\n\n\n2127\n3\n127.003937\n\n\n1873\n3\n127.003937\n\n\n1873\n4\n127.015747\n\n\n1872\n3\n128.003906\n\n\n2128\n4\n128.015624\n\n\n1872\n4\n128.015624\n\n\n1869\n2\n131.000000\n\n\n1869\n2\n131.000000\n\n\n1868\n3\n132.003788\n\n\n2132\n1\n132.003788\n\n\n1868\n4\n132.015151\n\n\n1867\n2\n133.000000\n\n\n2133\n4\n133.015037\n\n\n1866\n2\n134.000000\n\n\n1866\n4\n134.014925\n\n\n2134\n5\n134.033578\n\n\n1865\n3\n135.003704\n\n\n2136\n4\n136.014705\n\n\n1864\n6\n136.058811\n\n\n1863\n4\n137.014598\n\n\n1861\n3\n139.003597\n\n\n2140\n3\n140.003571\n\n\n2140\n4\n140.014285\n\n\n2142\n4\n142.014084\n\n\n1856\n1\n144.003472\n\n\n2144\n4\n144.013888\n\n\n1852\n3\n148.003378\n\n\n2149\n1\n149.003356\n\n\n1848\n3\n152.003289\n\n\n2153\n3\n153.003268\n\n\n1846\n3\n154.003247\n\n\n1846\n3\n154.003247\n\n\n1845\n3\n155.003226\n\n\n1844\n2\n156.000000\n\n\n1844\n3\n156.003205\n\n\n1844\n3\n156.003205\n\n\n2156\n3\n156.003205\n\n\n1844\n3\n156.003205\n\n\n2157\n4\n157.012738\n\n\n1842\n3\n158.003164\n\n\n2158\n4\n158.012658\n\n\n1840\n3\n160.003125\n\n\n2161\n3\n161.003106\n\n\n1839\n4\n161.012422\n\n\n1839\n4\n161.012422\n\n\n1838\n3\n162.003086\n\n\n1838\n3\n162.003086\n\n\n1836\n3\n164.003049\n\n\n1836\n3\n164.003049\n\n\n1836\n4\n164.012195\n\n\n2167\n3\n167.002994\n\n\n1832\n4\n168.011904\n\n\n2168\n5\n168.026784\n\n\n2169\n3\n169.002959\n\n\n1830\n3\n170.002941\n\n\n1829\n4\n171.011696\n\n\n1828\n3\n172.002907\n\n\n1828\n3\n172.002907\n\n\n1828\n3\n172.002907\n\n\n1828\n3\n172.002907\n\n\n2172\n4\n172.011628\n\n\n1827\n2\n173.000000\n\n\n1826\n3\n174.002873\n\n\n1824\n3\n176.002841\n\n\n1824\n4\n176.011363\n\n\n1824\n5\n176.025566\n\n\n1823\n3\n177.002825\n\n\n2177\n5\n177.025422\n\n\n1822\n3\n178.002809\n\n\n1822\n4\n178.011236\n\n\n1820\n3\n180.002778\n\n\n1820\n3\n180.002778\n\n\n2180\n3\n180.002778\n\n\n1820\n4\n180.011111\n\n\n1820\n4\n180.011111\n\n\n1818\n3\n182.002747\n\n\n1818\n4\n182.010989\n\n\n2183\n4\n183.010929\n\n\n2184\n3\n184.002717\n\n\n1816\n3\n184.002717\n\n\n2184\n4\n184.010869\n\n\n1812\n3\n188.002660\n\n\n1812\n3\n188.002660\n\n\n1812\n4\n188.010638\n\n\n1811\n2\n189.000000\n\n\n1809\n3\n191.002618\n\n\n2192\n4\n192.010416\n\n\n1806\n4\n194.010309\n\n\n1804\n3\n196.002551\n\n\n2196\n3\n196.002551\n\n\n1803\n3\n197.002538\n\n\n1803\n3\n197.002538\n\n\n2197\n4\n197.010152\n\n\n1802\n3\n198.002525\n\n\n1802\n3\n198.002525\n\n\n1802\n4\n198.010101\n\n\n2198\n4\n198.010101\n\n\n1801\n3\n199.002512\n\n\n1801\n3\n199.002512\n\n\n1800\n2\n200.000000\n\n\n2200\n3\n200.002500\n\n\n1800\n3\n200.002500\n\n\n1798\n4\n202.009901\n\n\n2202\n4\n202.009901\n\n\n1797\n3\n203.002463\n\n\n1797\n3\n203.002463\n\n\n1797\n3\n203.002463\n\n\n1796\n3\n204.002451\n\n\n1795\n3\n205.002439\n\n\n1795\n3\n205.002439\n\n\n1795\n4\n205.009756\n\n\n2207\n3\n207.002415\n\n\n1792\n2\n208.000000\n\n\n1792\n3\n208.002404\n\n\n1792\n3\n208.002404\n\n\n1792\n4\n208.009615\n\n\n1790\n4\n210.009524\n\n\n1789\n3\n211.002370\n\n\n1788\n3\n212.002359\n\n\n2212\n3\n212.002359\n\n\n1788\n5\n212.021225\n\n\n1787\n3\n213.002347\n\n\n1786\n3\n214.002336\n\n\n1786\n3\n214.002336\n\n\n1786\n3\n214.002336\n\n\n1786\n3\n214.002336\n\n\n2214\n3\n214.002336\n\n\n1784\n4\n216.009259\n\n\n2217\n4\n217.009216\n\n\n1782\n3\n218.002294\n\n\n1782\n3\n218.002294\n\n\n1780\n3\n220.002273\n\n\n1779\n3\n221.002262\n\n\n1779\n3\n221.002262\n\n\n1778\n2\n222.000000\n\n\n2223\n3\n223.002242\n\n\n1776\n3\n224.002232\n\n\n1776\n3\n224.002232\n\n\n1776\n4\n224.008928\n\n\n2224\n4\n224.008928\n\n\n1775\n3\n225.002222\n\n\n1775\n3\n225.002222\n\n\n2225\n4\n225.008889\n\n\n1774\n4\n226.008849\n\n\n1773\n3\n227.002203\n\n\n1772\n3\n228.002193\n\n\n2228\n6\n228.035085\n\n\n1771\n3\n229.002183\n\n\n1771\n3\n229.002183\n\n\n2229\n5\n229.019650\n\n\n2230\n5\n230.019564\n\n\n1768\n2\n232.000000\n\n\n1768\n3\n232.002155\n\n\n1768\n3\n232.002155\n\n\n1768\n3\n232.002155\n\n\n1768\n4\n232.008621\n\n\n1767\n2\n233.000000\n\n\n2233\n5\n233.019313\n\n\n1766\n3\n234.002137\n\n\n2234\n3\n234.002137\n\n\n1764\n3\n236.002119\n\n\n1764\n4\n236.008474\n\n\n2236\n4\n236.008474\n\n\n1762\n3\n238.002101\n\n\n1762\n4\n238.008403\n\n\n1760\n4\n240.008333\n\n\n2240\n6\n240.033331\n\n\n2243\n4\n243.008230\n\n\n1756\n3\n244.002049\n\n\n1755\n3\n245.002041\n\n\n1755\n3\n245.002041\n\n\n1755\n4\n245.008163\n\n\n1752\n4\n248.008064\n\n\n1750\n3\n250.002000\n\n\n2250\n3\n250.002000\n\n\n1750\n4\n250.008000\n\n\n1749\n3\n251.001992\n\n\n2253\n3\n253.001976\n\n\n1746\n3\n254.001969\n\n\n1746\n3\n254.001969\n\n\n1744\n3\n256.001953\n\n\n2256\n4\n256.007812\n\n\n1743\n3\n257.001945\n\n\n1743\n3\n257.001945\n\n\n1743\n0\n257.007782\n\n\n1742\n2\n258.000000\n\n\n1742\n4\n258.007752\n\n\n2259\n3\n259.001931\n\n\n1740\n2\n260.000000\n\n\n1740\n2\n260.000000\n\n\n2260\n3\n260.001923\n\n\n1740\n3\n260.001923\n\n\n1740\n4\n260.007692\n\n\n1740\n4\n260.007692\n\n\n1738\n4\n262.007633\n\n\n2262\n4\n262.007633\n\n\n1737\n3\n263.001901\n\n\n2263\n3\n263.001901\n\n\n1734\n2\n266.000000\n\n\n1734\n3\n266.001880\n\n\n1734\n3\n266.001880\n\n\n1734\n3\n266.001880\n\n\n1733\n3\n267.001873\n\n\n2267\n3\n267.001873\n\n\n1733\n4\n267.007491\n\n\n2267\n4\n267.007491\n\n\n2268\n3\n268.001866\n\n\n1730\n3\n270.001852\n\n\n1728\n3\n272.001838\n\n\n1728\n3\n272.001838\n\n\n1728\n3\n272.001838\n\n\n1728\n3\n272.001838\n\n\n1728\n3\n272.001838\n\n\n1728\n4\n272.007353\n\n\n1728\n4\n272.007353\n\n\n1728\n6\n272.029410\n\n\n1728\n6\n272.029410\n\n\n1728\n6\n272.029410\n\n\n1728\n6\n272.029410\n\n\n1728\n6\n272.029410\n\n\n1726\n3\n274.001825\n\n\n2274\n5\n274.016423\n\n\n1725\n3\n275.001818\n\n\n1724\n3\n276.001812\n\n\n1724\n3\n276.001812\n\n\n1724\n3\n276.001812\n\n\n2276\n3\n276.001812\n\n\n1721\n3\n279.001792\n\n\n1721\n3\n279.001792\n\n\n1721\n3\n279.001792\n\n\n1721\n4\n279.007168\n\n\n2279\n4\n279.007168\n\n\n1720\n3\n280.001786\n\n\n1720\n3\n280.001786\n\n\n1719\n1\n281.001779\n\n\n1718\n3\n282.001773\n\n\n1718\n3\n282.001773\n\n\n1718\n3\n282.001773\n\n\n1717\n2\n283.000000\n\n\n1717\n3\n283.001767\n\n\n1717\n3\n283.001767\n\n\n2283\n4\n283.007067\n\n\n1716\n2\n284.000000\n\n\n1716\n3\n284.001761\n\n\n1716\n3\n284.001761\n\n\n1716\n4\n284.007042\n\n\n2285\n4\n285.007018\n\n\n1714\n3\n286.001748\n\n\n1714\n3\n286.001748\n\n\n1712\n4\n288.006944\n\n\n2288\n4\n288.006944\n\n\n2290\n2\n290.000000\n\n\n1710\n3\n290.001724\n\n\n2290\n4\n290.006896\n\n\n2290\n4\n290.006896\n\n\n1709\n2\n291.000000\n\n\n1709\n3\n291.001718\n\n\n1709\n3\n291.001718\n\n\n1709\n3\n291.001718\n\n\n1709\n3\n291.001718\n\n\n2291\n4\n291.006873\n\n\n2291\n4\n291.006873\n\n\n1708\n3\n292.001712\n\n\n1707\n2\n293.000000\n\n\n2294\n5\n294.015306\n\n\n2295\n4\n295.006780\n\n\n1704\n3\n296.001689\n\n\n2296\n3\n296.001689\n\n\n1702\n1\n298.001678\n\n\n1701\n3\n299.001672\n\n\n1701\n4\n299.006689\n\n\n1700\n2\n300.000000\n\n\n1700\n4\n300.006667\n\n\n1699\n3\n301.001661\n\n\n1698\n3\n302.001656\n\n\n1696\n3\n304.001645\n\n\n1696\n3\n304.001645\n\n\n1696\n3\n304.001645\n\n\n1694\n2\n306.000000\n\n\n1694\n3\n306.001634\n\n\n1694\n3\n306.001634\n\n\n1694\n3\n306.001634\n\n\n1694\n4\n306.006536\n\n\n1692\n3\n308.001623\n\n\n1691\n2\n309.000000\n\n\n1691\n3\n309.001618\n\n\n1690\n3\n310.001613\n\n\n1690\n3\n310.001613\n\n\n1690\n4\n310.006452\n\n\n1689\n3\n311.001608\n\n\n1689\n3\n311.001608\n\n\n1689\n3\n311.001608\n\n\n1688\n2\n312.000000\n\n\n1688\n4\n312.006410\n\n\n1687\n3\n313.001597\n\n\n1686\n3\n314.001592\n\n\n1686\n1\n314.001592\n\n\n2314\n3\n314.001592\n\n\n1685\n2\n315.000000\n\n\n2315\n4\n315.006349\n\n\n1684\n2\n316.000000\n\n\n1683\n3\n317.001577\n\n\n1682\n3\n318.001572\n\n\n1682\n1\n318.001572\n\n\n2318\n3\n318.001572\n\n\n1680\n2\n320.000000\n\n\n2320\n2\n320.000000\n\n\n1680\n3\n320.001562\n\n\n1680\n3\n320.001562\n\n\n2320\n3\n320.001562\n\n\n1680\n4\n320.006250\n\n\n1680\n4\n320.006250\n\n\n1680\n4\n320.006250\n\n\n2322\n4\n322.006211\n\n\n1678\n6\n322.024844\n\n\n1677\n3\n323.001548\n\n\n2324\n4\n324.006173\n\n\n1675\n3\n325.001538\n\n\n1675\n3\n325.001538\n\n\n1674\n3\n326.001534\n\n\n1673\n3\n327.001529\n\n\n2327\n4\n327.006116\n\n\n1671\n3\n329.001520\n\n\n1671\n3\n329.001520\n\n\n2329\n4\n329.006079\n\n\n1671\n4\n329.006079\n\n\n1670\n2\n330.000000\n\n\n1670\n3\n330.001515\n\n\n1670\n4\n330.006061\n\n\n2331\n3\n331.001511\n\n\n1669\n4\n331.006042\n\n\n1668\n3\n332.001506\n\n\n1668\n3\n332.001506\n\n\n1668\n3\n332.001506\n\n\n2332\n3\n332.001506\n\n\n1668\n3\n332.001506\n\n\n2332\n4\n332.006024\n\n\n1666\n2\n334.000000\n\n\n1666\n3\n334.001497\n\n\n1666\n3\n334.001497\n\n\n2334\n3\n334.001497\n\n\n1665\n3\n335.001492\n\n\n1664\n3\n336.001488\n\n\n1664\n3\n336.001488\n\n\n1664\n4\n336.005952\n\n\n1664\n4\n336.005952\n\n\n1663\n3\n337.001484\n\n\n1662\n3\n338.001479\n\n\n2338\n4\n338.005917\n\n\n1661\n3\n339.001475\n\n\n1660\n3\n340.001471\n\n\n1660\n3\n340.001471\n\n\n2340\n4\n340.005882\n\n\n1659\n2\n341.000000\n\n\n1659\n3\n341.001466\n\n\n1658\n3\n342.001462\n\n\n1657\n3\n343.001458\n\n\n1657\n3\n343.001458\n\n\n1656\n2\n344.000000\n\n\n1656\n3\n344.001454\n\n\n1654\n3\n346.001445\n\n\n1654\n3\n346.001445\n\n\n1654\n4\n346.005780\n\n\n1653\n3\n347.001441\n\n\n1652\n2\n348.000000\n\n\n1652\n2\n348.000000\n\n\n1652\n3\n348.001437\n\n\n1652\n3\n348.001437\n\n\n1652\n3\n348.001437\n\n\n1652\n4\n348.005747\n\n\n1652\n4\n348.005747\n\n\n1651\n4\n349.005731\n\n\n1648\n2\n352.000000\n\n\n1647\n3\n353.001416\n\n\n1647\n3\n353.001416\n\n\n2353\n4\n353.005666\n\n\n1646\n2\n354.000000\n\n\n1646\n2\n354.000000\n\n\n1646\n2\n354.000000\n\n\n1646\n2\n354.000000\n\n\n1646\n3\n354.001412\n\n\n1646\n3\n354.001412\n\n\n1644\n3\n356.001404\n\n\n1642\n3\n358.001397\n\n\n2358\n4\n358.005586\n\n\n1642\n4\n358.005586\n\n\n2358\n4\n358.005586\n\n\n1641\n3\n359.001393\n\n\n1641\n3\n359.001393\n\n\n1640\n3\n360.001389\n\n\n1640\n3\n360.001389\n\n\n1640\n3\n360.001389\n\n\n1640\n3\n360.001389\n\n\n1639\n3\n361.001385\n\n\n1638\n2\n362.000000\n\n\n1638\n3\n362.001381\n\n\n2362\n3\n362.001381\n\n\n2364\n2\n364.000000\n\n\n1636\n3\n364.001374\n\n\n1636\n3\n364.001374\n\n\n1635\n2\n365.000000\n\n\n1635\n3\n365.001370\n\n\n2365\n3\n365.001370\n\n\n1634\n3\n366.001366\n\n\n1633\n3\n367.001362\n\n\n1632\n3\n368.001359\n\n\n1632\n4\n368.005435\n\n\n1632\n4\n368.005435\n\n\n1632\n4\n368.005435\n\n\n1630\n3\n370.001351\n\n\n1630\n3\n370.001351\n\n\n1630\n3\n370.001351\n\n\n1629\n3\n371.001348\n\n\n1629\n3\n371.001348\n\n\n1629\n3\n371.001348\n\n\n1628\n3\n372.001344\n\n\n2372\n4\n372.005376\n\n\n1627\n4\n373.005362\n\n\n1626\n2\n374.000000\n\n\n1626\n2\n374.000000\n\n\n1626\n2\n374.000000\n\n\n1626\n3\n374.001337\n\n\n1626\n3\n374.001337\n\n\n2374\n4\n374.005348\n\n\n1625\n2\n375.000000\n\n\n1625\n2\n375.000000\n\n\n1624\n2\n376.000000\n\n\n1624\n2\n376.000000\n\n\n2376\n4\n376.005319\n\n\n1621\n3\n379.001319\n\n\n1621\n3\n379.001319\n\n\n1620\n2\n380.000000\n\n\n2380\n3\n380.001316\n\n\n1620\n4\n380.005263\n\n\n1620\n4\n380.005263\n\n\n1618\n2\n382.000000\n\n\n1616\n2\n384.000000\n\n\n1616\n3\n384.001302\n\n\n1615\n3\n385.001299\n\n\n2385\n3\n385.001299\n\n\n1614\n3\n386.001295\n\n\n1614\n3\n386.001295\n\n\n1614\n3\n386.001295\n\n\n1612\n2\n388.000000\n\n\n1612\n3\n388.001289\n\n\n1612\n3\n388.001289\n\n\n1611\n3\n389.001285\n\n\n1611\n4\n389.005141\n\n\n1610\n3\n390.001282\n\n\n1610\n4\n390.005128\n\n\n1609\n3\n391.001279\n\n\n1609\n3\n391.001279\n\n\n1609\n5\n391.011509\n\n\n1608\n3\n392.001276\n\n\n1608\n3\n392.001276\n\n\n2392\n3\n392.001276\n\n\n2392\n3\n392.001276\n\n\n1606\n3\n394.001269\n\n\n1605\n3\n395.001266\n\n\n1604\n3\n396.001263\n\n\n1604\n3\n396.001263\n\n\n1604\n3\n396.001263\n\n\n1604\n3\n396.001263\n\n\n1604\n3\n396.001263\n\n\n1603\n4\n397.005038\n\n\n1602\n3\n398.001256\n\n\n2398\n3\n398.001256\n\n\n1601\n3\n399.001253\n\n\n1601\n3\n399.001253\n\n\n1600\n2\n400.000000\n\n\n1600\n3\n400.001250\n\n\n1600\n3\n400.001250\n\n\n2400\n4\n400.005000\n\n\n1598\n3\n402.001244\n\n\n2403\n4\n403.004963\n\n\n1596\n3\n404.001238\n\n\n1595\n2\n405.000000\n\n\n1595\n2\n405.000000\n\n\n1594\n2\n406.000000\n\n\n1594\n3\n406.001232\n\n\n1594\n3\n406.001232\n\n\n1593\n3\n407.001229\n\n\n1593\n3\n407.001229\n\n\n1592\n3\n408.001225\n\n\n1590\n3\n410.001219\n\n\n1589\n2\n411.000000\n\n\n2411\n4\n411.004866\n\n\n1588\n3\n412.001214\n\n\n1588\n3\n412.001214\n\n\n1588\n3\n412.001214\n\n\n1588\n5\n412.010922\n\n\n1587\n3\n413.001211\n\n\n2414\n3\n414.001208\n\n\n1586\n3\n414.001208\n\n\n1585\n3\n415.001205\n\n\n1584\n4\n416.004808\n\n\n2417\n4\n417.004796\n\n\n1582\n3\n418.001196\n\n\n1582\n3\n418.001196\n\n\n1582\n3\n418.001196\n\n\n2418\n3\n418.001196\n\n\n1582\n4\n418.004785\n\n\n1580\n3\n420.001191\n\n\n1580\n3\n420.001191\n\n\n1578\n3\n422.001185\n\n\n1578\n3\n422.001185\n\n\n1578\n3\n422.001185\n\n\n2422\n4\n422.004739\n\n\n1577\n2\n423.000000\n\n\n1577\n3\n423.001182\n\n\n1576\n2\n424.000000\n\n\n1576\n3\n424.001179\n\n\n1576\n3\n424.001179\n\n\n1576\n3\n424.001179\n\n\n1575\n2\n425.000000\n\n\n1575\n2\n425.000000\n\n\n1574\n3\n426.001174\n\n\n1574\n3\n426.001174\n\n\n1573\n3\n427.001171\n\n\n1573\n3\n427.001171\n\n\n1573\n3\n427.001171\n\n\n1573\n3\n427.001171\n\n\n1573\n3\n427.001171\n\n\n1572\n3\n428.001168\n\n\n1572\n3\n428.001168\n\n\n1571\n3\n429.001166\n\n\n1571\n3\n429.001166\n\n\n1569\n1\n431.001160\n\n\n1568\n3\n432.001157\n\n\n1568\n3\n432.001157\n\n\n2432\n4\n432.004630\n\n\n1567\n1\n433.001155\n\n\n1566\n5\n434.010369\n\n\n1565\n2\n435.000000\n\n\n1565\n2\n435.000000\n\n\n1564\n2\n436.000000\n\n\n1564\n3\n436.001147\n\n\n1564\n3\n436.001147\n\n\n1561\n2\n439.000000\n\n\n1561\n3\n439.001139\n\n\n1560\n3\n440.001136\n\n\n1560\n3\n440.001136\n\n\n1560\n3\n440.001136\n\n\n1560\n4\n440.004545\n\n\n1560\n4\n440.004545\n\n\n1559\n2\n441.000000\n\n\n1558\n3\n442.001131\n\n\n1556\n3\n444.001126\n\n\n1556\n3\n444.001126\n\n\n1556\n4\n444.004504\n\n\n1556\n4\n444.004504\n\n\n1555\n2\n445.000000\n\n\n1554\n2\n446.000000\n\n\n1553\n3\n447.001119\n\n\n2447\n4\n447.004474\n\n\n1552\n3\n448.001116\n\n\n1552\n3\n448.001116\n\n\n2448\n3\n448.001116\n\n\n2448\n4\n448.004464\n\n\n1551\n3\n449.001114\n\n\n1550\n3\n450.001111\n\n\n1549\n3\n451.001109\n\n\n1548\n2\n452.000000\n\n\n1548\n3\n452.001106\n\n\n1548\n3\n452.001106\n\n\n2452\n3\n452.001106\n\n\n2454\n3\n454.001101\n\n\n1546\n3\n454.001101\n\n\n1544\n3\n456.001097\n\n\n1541\n3\n459.001089\n\n\n1540\n3\n460.001087\n\n\n1540\n3\n460.001087\n\n\n1540\n4\n460.004348\n\n\n1539\n3\n461.001085\n\n\n1539\n3\n461.001085\n\n\n1538\n3\n462.001082\n\n\n2462\n4\n462.004329\n\n\n1537\n3\n463.001080\n\n\n1536\n3\n464.001078\n\n\n1536\n3\n464.001078\n\n\n1536\n3\n464.001078\n\n\n1536\n4\n464.004310\n\n\n2464\n4\n464.004310\n\n\n1535\n3\n465.001075\n\n\n1535\n4\n465.004301\n\n\n1535\n4\n465.004301\n\n\n1534\n3\n466.001073\n\n\n1533\n2\n467.000000\n\n\n2468\n4\n468.004274\n\n\n1531\n2\n469.000000\n\n\n1531\n3\n469.001066\n\n\n1530\n3\n470.001064\n\n\n2470\n1\n470.001064\n\n\n1530\n3\n470.001064\n\n\n1528\n3\n472.001059\n\n\n2473\n4\n473.004228\n\n\n2473\n4\n473.004228\n\n\n1526\n2\n474.000000\n\n\n1526\n4\n474.004219\n\n\n1525\n3\n475.001053\n\n\n1525\n3\n475.001053\n\n\n1525\n3\n475.001053\n\n\n2475\n4\n475.004210\n\n\n1524\n2\n476.000000\n\n\n1524\n3\n476.001050\n\n\n1523\n3\n477.001048\n\n\n1522\n4\n478.004184\n\n\n1521\n3\n479.001044\n\n\n1521\n4\n479.004175\n\n\n1520\n3\n480.001042\n\n\n1520\n3\n480.001042\n\n\n2480\n5\n480.009375\n\n\n2482\n4\n482.004149\n\n\n1517\n3\n483.001035\n\n\n1516\n3\n484.001033\n\n\n1515\n3\n485.001031\n\n\n2486\n5\n486.009259\n\n\n2486\n5\n486.009259\n\n\n1513\n2\n487.000000\n\n\n1513\n4\n487.004107\n\n\n1513\n4\n487.004107\n\n\n1512\n2\n488.000000\n\n\n1512\n2\n488.000000\n\n\n1512\n2\n488.000000\n\n\n1512\n3\n488.001025\n\n\n1511\n2\n489.000000\n\n\n1511\n3\n489.001022\n\n\n2490\n2\n490.000000\n\n\n1510\n3\n490.001020\n\n\n1510\n3\n490.001020\n\n\n1510\n4\n490.004082\n\n\n1509\n3\n491.001018\n\n\n1508\n1\n492.001016\n\n\n1507\n4\n493.004057\n\n\n1506\n2\n494.000000\n\n\n1506\n3\n494.001012\n\n\n2494\n4\n494.004049\n\n\n1505\n2\n495.000000\n\n\n1505\n3\n495.001010\n\n\n2495\n4\n495.004040\n\n\n2495\n5\n495.009091\n\n\n1504\n2\n496.000000\n\n\n1504\n1\n496.001008\n\n\n1504\n3\n496.001008\n\n\n1504\n3\n496.001008\n\n\n2497\n2\n497.000000\n\n\n1502\n3\n498.001004\n\n\n1502\n3\n498.001004\n\n\n1502\n1\n498.001004\n\n\n1501\n2\n499.000000\n\n\n1501\n2\n499.000000\n\n\n1501\n3\n499.001002\n\n\n1501\n3\n499.001002\n\n\n1501\n3\n499.001002\n\n\n2499\n4\n499.004008\n\n\n1500\n2\n500.000000\n\n\n1500\n3\n500.001000\n\n\n1500\n3\n500.001000\n\n\n2500\n5\n500.009000\n\n\n1499\n3\n501.000998\n\n\n1499\n3\n501.000998\n\n\n2501\n4\n501.003992\n\n\n1498\n2\n502.000000\n\n\n1498\n3\n502.000996\n\n\n1497\n3\n503.000994\n\n\n2503\n3\n503.000994\n\n\n1496\n3\n504.000992\n\n\n1496\n3\n504.000992\n\n\n1496\n3\n504.000992\n\n\n1496\n3\n504.000992\n\n\n1495\n3\n505.000990\n\n\n1495\n3\n505.000990\n\n\n1494\n2\n506.000000\n\n\n1494\n3\n506.000988\n\n\n1494\n3\n506.000988\n\n\n1494\n3\n506.000988\n\n\n1494\n3\n506.000988\n\n\n1494\n3\n506.000988\n\n\n1493\n3\n507.000986\n\n\n1492\n3\n508.000984\n\n\n1492\n3\n508.000984\n\n\n1492\n3\n508.000984\n\n\n1491\n3\n509.000982\n\n\n1491\n3\n509.000982\n\n\n1490\n3\n510.000980\n\n\n1490\n3\n510.000980\n\n\n1489\n3\n511.000978\n\n\n1489\n3\n511.000978\n\n\n1489\n3\n511.000978\n\n\n1489\n3\n511.000978\n\n\n1488\n2\n512.000000\n\n\n1488\n3\n512.000977\n\n\n1488\n3\n512.000977\n\n\n1488\n3\n512.000977\n\n\n1487\n3\n513.000975\n\n\n1486\n3\n514.000973\n\n\n1486\n3\n514.000973\n\n\n2514\n4\n514.003891\n\n\n2515\n4\n515.003884\n\n\n1484\n3\n516.000969\n\n\n1484\n3\n516.000969\n\n\n1484\n3\n516.000969\n\n\n1482\n3\n518.000965\n\n\n1482\n4\n518.003861\n\n\n1481\n3\n519.000963\n\n\n1480\n3\n520.000962\n\n\n1480\n4\n520.003846\n\n\n2520\n4\n520.003846\n\n\n2520\n5\n520.008654\n\n\n1479\n3\n521.000960\n\n\n1479\n3\n521.000960\n\n\n1479\n3\n521.000960\n\n\n1479\n3\n521.000960\n\n\n1479\n3\n521.000960\n\n\n1479\n5\n521.008637\n\n\n1478\n2\n522.000000\n\n\n1478\n3\n522.000958\n\n\n1478\n3\n522.000958\n\n\n1476\n3\n524.000954\n\n\n1476\n3\n524.000954\n\n\n2524\n4\n524.003817\n\n\n1475\n4\n525.003809\n\n\n1474\n3\n526.000951\n\n\n2526\n4\n526.003802\n\n\n1473\n1\n527.000949\n\n\n1472\n2\n528.000000\n\n\n1472\n3\n528.000947\n\n\n1472\n3\n528.000947\n\n\n1471\n3\n529.000945\n\n\n1470\n2\n530.000000\n\n\n1470\n3\n530.000943\n\n\n1470\n3\n530.000943\n\n\n1470\n4\n530.003774\n\n\n1468\n2\n532.000000\n\n\n1468\n3\n532.000940\n\n\n1466\n3\n534.000936\n\n\n1466\n3\n534.000936\n\n\n1465\n3\n535.000935\n\n\n1465\n3\n535.000935\n\n\n1464\n3\n536.000933\n\n\n1464\n3\n536.000933\n\n\n1464\n4\n536.003731\n\n\n1463\n3\n537.000931\n\n\n1458\n2\n542.000000\n\n\n1458\n3\n542.000923\n\n\n1458\n3\n542.000923\n\n\n1458\n3\n542.000923\n\n\n1456\n2\n544.000000\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n3\n544.000919\n\n\n1456\n4\n544.003676\n\n\n1456\n4\n544.003676\n\n\n2544\n6\n544.014706\n\n\n1455\n2\n545.000000\n\n\n1453\n2\n547.000000\n\n\n1453\n3\n547.000914\n\n\n1452\n2\n548.000000\n\n\n1452\n2\n548.000000\n\n\n1450\n2\n550.000000\n\n\n1450\n3\n550.000909\n\n\n2550\n4\n550.003636\n\n\n1446\n3\n554.000903\n\n\n1445\n3\n555.000901\n\n\n1445\n3\n555.000901\n\n\n1444\n2\n556.000000\n\n\n1444\n3\n556.000899\n\n\n1444\n3\n556.000899\n\n\n1442\n2\n558.000000\n\n\n1442\n3\n558.000896\n\n\n1442\n3\n558.000896\n\n\n1442\n4\n558.003584\n\n\n1441\n2\n559.000000\n\n\n1441\n3\n559.000894\n\n\n2559\n4\n559.003578\n\n\n1440\n2\n560.000000\n\n\n1440\n2\n560.000000\n\n\n1440\n3\n560.000893\n\n\n1440\n3\n560.000893\n\n\n1440\n3\n560.000893\n\n\n1440\n4\n560.003571\n\n\n1440\n4\n560.003571\n\n\n1437\n3\n563.000888\n\n\n1436\n3\n564.000886\n\n\n1436\n3\n564.000886\n\n\n1436\n3\n564.000886\n\n\n1436\n3\n564.000886\n\n\n1436\n3\n564.000886\n\n\n1435\n3\n565.000885\n\n\n1434\n3\n566.000883\n\n\n1434\n4\n566.003534\n\n\n1432\n2\n568.000000\n\n\n1432\n3\n568.000880\n\n\n1432\n3\n568.000880\n\n\n1432\n3\n568.000880\n\n\n1432\n3\n568.000880\n\n\n1431\n3\n569.000879\n\n\n1431\n3\n569.000879\n\n\n1430\n3\n570.000877\n\n\n1430\n3\n570.000877\n\n\n1430\n3\n570.000877\n\n\n1430\n3\n570.000877\n\n\n1429\n3\n571.000876\n\n\n1428\n3\n572.000874\n\n\n1428\n4\n572.003496\n\n\n1427\n4\n573.003490\n\n\n1426\n3\n574.000871\n\n\n2574\n3\n574.000871\n\n\n1425\n3\n575.000870\n\n\n1425\n3\n575.000870\n\n\n1424\n2\n576.000000\n\n\n1424\n3\n576.000868\n\n\n2576\n4\n576.003472\n\n\n1422\n3\n578.000865\n\n\n1422\n3\n578.000865\n\n\n1422\n3\n578.000865\n\n\n1419\n2\n581.000000\n\n\n1419\n2\n581.000000\n\n\n1419\n2\n581.000000\n\n\n1419\n3\n581.000861\n\n\n1416\n3\n584.000856\n\n\n1416\n3\n584.000856\n\n\n1414\n2\n586.000000\n\n\n1414\n3\n586.000853\n\n\n1414\n3\n586.000853\n\n\n1414\n3\n586.000853\n\n\n1414\n3\n586.000853\n\n\n1414\n3\n586.000853\n\n\n1412\n3\n588.000850\n\n\n1412\n4\n588.003401\n\n\n1411\n3\n589.000849\n\n\n1409\n3\n591.000846\n\n\n1409\n1\n591.000846\n\n\n2592\n6\n592.013513\n\n\n1406\n3\n594.000842\n\n\n1405\n2\n595.000000\n\n\n1404\n3\n596.000839\n\n\n1404\n3\n596.000839\n\n\n2599\n4\n599.003339\n\n\n1400\n3\n600.000833\n\n\n1400\n3\n600.000833\n\n\n2601\n4\n601.003328\n\n\n1398\n2\n602.000000\n\n\n1396\n3\n604.000828\n\n\n1396\n4\n604.003311\n\n\n1395\n3\n605.000826\n\n\n1394\n3\n606.000825\n\n\n1393\n3\n607.000824\n\n\n1392\n2\n608.000000\n\n\n1392\n2\n608.000000\n\n\n1392\n2\n608.000000\n\n\n1392\n3\n608.000822\n\n\n1392\n3\n608.000822\n\n\n1392\n3\n608.000822\n\n\n1392\n3\n608.000822\n\n\n1391\n2\n609.000000\n\n\n1390\n1\n610.000820\n\n\n2610\n4\n610.003279\n\n\n1389\n2\n611.000000\n\n\n1389\n3\n611.000818\n\n\n1387\n3\n613.000816\n\n\n1386\n3\n614.000814\n\n\n2614\n4\n614.003257\n\n\n1383\n2\n617.000000\n\n\n1383\n3\n617.000810\n\n\n1383\n3\n617.000810\n\n\n1382\n1\n618.000809\n\n\n1382\n3\n618.000809\n\n\n1382\n3\n618.000809\n\n\n1382\n3\n618.000809\n\n\n1381\n3\n619.000808\n\n\n2620\n4\n620.003226\n\n\n1378\n3\n622.000804\n\n\n2622\n3\n622.000804\n\n\n1377\n3\n623.000803\n\n\n1376\n2\n624.000000\n\n\n1376\n3\n624.000801\n\n\n1376\n3\n624.000801\n\n\n2624\n4\n624.003205\n\n\n1375\n3\n625.000800\n\n\n1374\n2\n626.000000\n\n\n1374\n3\n626.000799\n\n\n1374\n3\n626.000799\n\n\n1374\n3\n626.000799\n\n\n1374\n3\n626.000799\n\n\n1374\n3\n626.000799\n\n\n1373\n3\n627.000797\n\n\n1372\n3\n628.000796\n\n\n1372\n3\n628.000796\n\n\n1370\n2\n630.000000\n\n\n1370\n2\n630.000000\n\n\n1370\n3\n630.000794\n\n\n1370\n3\n630.000794\n\n\n2630\n4\n630.003175\n\n\n1369\n3\n631.000792\n\n\n1368\n2\n632.000000\n\n\n1368\n3\n632.000791\n\n\n1367\n2\n633.000000\n\n\n1367\n3\n633.000790\n\n\n2634\n6\n634.012618\n\n\n1365\n2\n635.000000\n\n\n1365\n3\n635.000787\n\n\n1365\n3\n635.000787\n\n\n1364\n2\n636.000000\n\n\n1364\n2\n636.000000\n\n\n1363\n2\n637.000000\n\n\n1363\n2\n637.000000\n\n\n1363\n3\n637.000785\n\n\n1362\n2\n638.000000\n\n\n1362\n2\n638.000000\n\n\n1362\n3\n638.000784\n\n\n1362\n3\n638.000784\n\n\n1362\n4\n638.003135\n\n\n1361\n2\n639.000000\n\n\n1361\n3\n639.000783\n\n\n1360\n2\n640.000000\n\n\n1360\n2\n640.000000\n\n\n1360\n3\n640.000781\n\n\n1360\n1\n640.000781\n\n\n2640\n3\n640.000781\n\n\n2640\n4\n640.003125\n\n\n2640\n5\n640.007031\n\n\n1358\n2\n642.000000\n\n\n1358\n2\n642.000000\n\n\n1358\n2\n642.000000\n\n\n1358\n2\n642.000000\n\n\n1358\n3\n642.000779\n\n\n1358\n1\n642.000779\n\n\n1357\n2\n643.000000\n\n\n2643\n3\n643.000778\n\n\n1356\n3\n644.000776\n\n\n1356\n3\n644.000776\n\n\n1355\n3\n645.000775\n\n\n1355\n4\n645.003101\n\n\n2646\n3\n646.000774\n\n\n2646\n4\n646.003096\n\n\n1353\n2\n647.000000\n\n\n1352\n2\n648.000000\n\n\n1352\n2\n648.000000\n\n\n1352\n3\n648.000772\n\n\n1352\n4\n648.003086\n\n\n1352\n4\n648.003086\n\n\n1351\n3\n649.000770\n\n\n2649\n4\n649.003082\n\n\n1350\n2\n650.000000\n\n\n1350\n3\n650.000769\n\n\n1350\n3\n650.000769\n\n\n2650\n6\n650.012308\n\n\n1348\n3\n652.000767\n\n\n1347\n3\n653.000766\n\n\n1346\n2\n654.000000\n\n\n1346\n3\n654.000764\n\n\n2654\n4\n654.003058\n\n\n2654\n4\n654.003058\n\n\n1344\n2\n656.000000\n\n\n1344\n2\n656.000000\n\n\n1344\n2\n656.000000\n\n\n1344\n3\n656.000762\n\n\n1344\n3\n656.000762\n\n\n1344\n3\n656.000762\n\n\n1344\n4\n656.003049\n\n\n1343\n3\n657.000761\n\n\n1342\n2\n658.000000\n\n\n1342\n4\n658.003039\n\n\n1341\n2\n659.000000\n\n\n1341\n1\n659.000759\n\n\n1340\n3\n660.000758\n\n\n1340\n3\n660.000758\n\n\n1338\n2\n662.000000\n\n\n1338\n3\n662.000755\n\n\n1338\n3\n662.000755\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n2\n663.000000\n\n\n1337\n3\n663.000754\n\n\n1337\n3\n663.000754\n\n\n1337\n3\n663.000754\n\n\n1336\n2\n664.000000\n\n\n1336\n3\n664.000753\n\n\n1336\n3\n664.000753\n\n\n1334\n2\n666.000000\n\n\n1334\n3\n666.000751\n\n\n1332\n2\n668.000000\n\n\n2668\n3\n668.000748\n\n\n1330\n3\n670.000746\n\n\n1330\n3\n670.000746\n\n\n1329\n3\n671.000745\n\n\n1329\n3\n671.000745\n\n\n1328\n2\n672.000000\n\n\n1328\n3\n672.000744\n\n\n1328\n3\n672.000744\n\n\n1327\n3\n673.000743\n\n\n2674\n2\n674.000000\n\n\n1326\n3\n674.000742\n\n\n1324\n2\n676.000000\n\n\n1324\n2\n676.000000\n\n\n1324\n3\n676.000740\n\n\n1324\n3\n676.000740\n\n\n1324\n3\n676.000740\n\n\n1324\n3\n676.000740\n\n\n1323\n3\n677.000739\n\n\n1322\n4\n678.002950\n\n\n1321\n1\n679.000736\n\n\n1320\n3\n680.000735\n\n\n1320\n3\n680.000735\n\n\n1320\n3\n680.000735\n\n\n1320\n3\n680.000735\n\n\n1320\n3\n680.000735\n\n\n1317\n3\n683.000732\n\n\n2683\n4\n683.002928\n\n\n1316\n2\n684.000000\n\n\n1316\n3\n684.000731\n\n\n1316\n3\n684.000731\n\n\n1316\n3\n684.000731\n\n\n1316\n4\n684.002924\n\n\n1315\n3\n685.000730\n\n\n1314\n2\n686.000000\n\n\n1314\n3\n686.000729\n\n\n1314\n3\n686.000729\n\n\n1314\n3\n686.000729\n\n\n1313\n3\n687.000728\n\n\n2687\n4\n687.002911\n\n\n2687\n4\n687.002911\n\n\n1312\n3\n688.000727\n\n\n1312\n3\n688.000727\n\n\n1310\n2\n690.000000\n\n\n1310\n3\n690.000725\n\n\n2690\n3\n690.000725\n\n\n1310\n1\n690.000725\n\n\n2690\n4\n690.002899\n\n\n1309\n3\n691.000724\n\n\n1308\n2\n692.000000\n\n\n1308\n2\n692.000000\n\n\n1308\n3\n692.000723\n\n\n1308\n3\n692.000723\n\n\n1306\n2\n694.000000\n\n\n1306\n3\n694.000721\n\n\n1306\n3\n694.000721\n\n\n1306\n1\n694.000721\n\n\n1306\n1\n694.000721\n\n\n1304\n3\n696.000718\n\n\n1304\n3\n696.000718\n\n\n2696\n3\n696.000718\n\n\n2696\n4\n696.002874\n\n\n1302\n2\n698.000000\n\n\n1302\n3\n698.000716\n\n\n1302\n3\n698.000716\n\n\n1302\n3\n698.000716\n\n\n1302\n3\n698.000716\n\n\n1302\n3\n698.000716\n\n\n1302\n1\n698.000716\n\n\n1302\n3\n698.000716\n\n\n2698\n4\n698.002865\n\n\n1301\n2\n699.000000\n\n\n1299\n2\n701.000000\n\n\n1299\n3\n701.000713\n\n\n1298\n2\n702.000000\n\n\n1298\n3\n702.000712\n\n\n1298\n3\n702.000712\n\n\n1298\n3\n702.000712\n\n\n1297\n3\n703.000711\n\n\n1296\n2\n704.000000\n\n\n1296\n2\n704.000000\n\n\n1296\n3\n704.000710\n\n\n2704\n4\n704.002841\n\n\n1295\n2\n705.000000\n\n\n1295\n3\n705.000709\n\n\n1295\n1\n705.000709\n\n\n1294\n2\n706.000000\n\n\n1293\n2\n707.000000\n\n\n1292\n3\n708.000706\n\n\n1290\n2\n710.000000\n\n\n1290\n3\n710.000704\n\n\n1288\n3\n712.000702\n\n\n1288\n4\n712.002809\n\n\n1287\n2\n713.000000\n\n\n1287\n3\n713.000701\n\n\n1287\n3\n713.000701\n\n\n2713\n3\n713.000701\n\n\n1285\n2\n715.000000\n\n\n1285\n3\n715.000699\n\n\n1285\n3\n715.000699\n\n\n2715\n4\n715.002797\n\n\n1283\n3\n717.000697\n\n\n1282\n2\n718.000000\n\n\n1281\n1\n719.000695\n\n\n1280\n2\n720.000000\n\n\n1280\n2\n720.000000\n\n\n1278\n2\n722.000000\n\n\n1277\n2\n723.000000\n\n\n2726\n2\n726.000000\n\n\n1273\n2\n727.000000\n\n\n1273\n3\n727.000688\n\n\n2727\n3\n727.000688\n\n\n2728\n4\n728.002747\n\n\n1271\n4\n729.002743\n\n\n1270\n2\n730.000000\n\n\n2730\n4\n730.002740\n\n\n1269\n3\n731.000684\n\n\n1269\n3\n731.000684\n\n\n1268\n2\n732.000000\n\n\n1268\n3\n732.000683\n\n\n1268\n3\n732.000683\n\n\n1268\n3\n732.000683\n\n\n1266\n2\n734.000000\n\n\n1266\n2\n734.000000\n\n\n1265\n3\n735.000680\n\n\n1264\n2\n736.000000\n\n\n1264\n3\n736.000679\n\n\n1262\n2\n738.000000\n\n\n1261\n3\n739.000677\n\n\n1260\n3\n740.000676\n\n\n1258\n2\n742.000000\n\n\n1258\n3\n742.000674\n\n\n1258\n3\n742.000674\n\n\n1258\n0\n742.002695\n\n\n1256\n3\n744.000672\n\n\n1256\n1\n744.000672\n\n\n1256\n3\n744.000672\n\n\n1254\n3\n746.000670\n\n\n1253\n2\n747.000000\n\n\n1252\n2\n748.000000\n\n\n1252\n3\n748.000668\n\n\n1252\n3\n748.000668\n\n\n1252\n3\n748.000668\n\n\n1252\n1\n748.000668\n\n\n1251\n3\n749.000668\n\n\n1250\n2\n750.000000\n\n\n1250\n2\n750.000000\n\n\n1248\n2\n752.000000\n\n\n1248\n2\n752.000000\n\n\n1248\n2\n752.000000\n\n\n1248\n3\n752.000665\n\n\n1247\n1\n753.000664\n\n\n1246\n2\n754.000000\n\n\n1246\n3\n754.000663\n\n\n1246\n3\n754.000663\n\n\n1245\n3\n755.000662\n\n\n1245\n1\n755.000662\n\n\n1244\n3\n756.000661\n\n\n1242\n2\n758.000000\n\n\n1242\n3\n758.000660\n\n\n1242\n3\n758.000660\n\n\n2758\n4\n758.002638\n\n\n1241\n1\n759.000659\n\n\n1240\n2\n760.000000\n\n\n1240\n3\n760.000658\n\n\n1239\n1\n761.000657\n\n\n1236\n2\n764.000000\n\n\n1236\n2\n764.000000\n\n\n1236\n2\n764.000000\n\n\n1236\n3\n764.000654\n\n\n1235\n2\n765.000000\n\n\n1235\n1\n765.000654\n\n\n1232\n3\n768.000651\n\n\n1232\n3\n768.000651\n\n\n1232\n3\n768.000651\n\n\n1232\n3\n768.000651\n\n\n1230\n3\n770.000649\n\n\n1229\n2\n771.000000\n\n\n1229\n2\n771.000000\n\n\n1229\n2\n771.000000\n\n\n1229\n3\n771.000649\n\n\n1228\n2\n772.000000\n\n\n1228\n3\n772.000648\n\n\n1228\n3\n772.000648\n\n\n2772\n4\n772.002591\n\n\n1226\n2\n774.000000\n\n\n1226\n3\n774.000646\n\n\n1226\n1\n774.000646\n\n\n1226\n1\n774.000646\n\n\n1225\n3\n775.000645\n\n\n1224\n2\n776.000000\n\n\n1224\n2\n776.000000\n\n\n1224\n2\n776.000000\n\n\n1224\n3\n776.000644\n\n\n1224\n3\n776.000644\n\n\n1224\n3\n776.000644\n\n\n1224\n4\n776.002577\n\n\n1223\n2\n777.000000\n\n\n1222\n2\n778.000000\n\n\n1221\n2\n779.000000\n\n\n1221\n4\n779.002567\n\n\n1220\n2\n780.000000\n\n\n1220\n2\n780.000000\n\n\n1220\n2\n780.000000\n\n\n1220\n2\n780.000000\n\n\n1218\n2\n782.000000\n\n\n1218\n3\n782.000639\n\n\n1218\n3\n782.000639\n\n\n1218\n3\n782.000639\n\n\n1218\n3\n782.000639\n\n\n1218\n3\n782.000639\n\n\n1218\n4\n782.002557\n\n\n1217\n2\n783.000000\n\n\n1217\n3\n783.000639\n\n\n1217\n3\n783.000639\n\n\n1216\n2\n784.000000\n\n\n1216\n2\n784.000000\n\n\n1216\n3\n784.000638\n\n\n1216\n3\n784.000638\n\n\n1216\n3\n784.000638\n\n\n2784\n5\n784.005740\n\n\n1215\n3\n785.000637\n\n\n2786\n4\n786.002544\n\n\n2787\n6\n787.010165\n\n\n2787\n6\n787.010165\n\n\n1212\n3\n788.000635\n\n\n1212\n3\n788.000635\n\n\n1212\n3\n788.000635\n\n\n1211\n2\n789.000000\n\n\n1210\n3\n790.000633\n\n\n1210\n3\n790.000633\n\n\n2790\n4\n790.002532\n\n\n1209\n3\n791.000632\n\n\n1208\n2\n792.000000\n\n\n1208\n2\n792.000000\n\n\n1208\n2\n792.000000\n\n\n1208\n2\n792.000000\n\n\n1208\n3\n792.000631\n\n\n1207\n3\n793.000631\n\n\n2795\n4\n795.002516\n\n\n1204\n2\n796.000000\n\n\n1204\n2\n796.000000\n\n\n1204\n3\n796.000628\n\n\n1203\n3\n797.000627\n\n\n2798\n3\n798.000627\n\n\n1200\n2\n800.000000\n\n\n1200\n2\n800.000000\n\n\n1200\n2\n800.000000\n\n\n1200\n2\n800.000000\n\n\n1200\n2\n800.000000\n\n\n1200\n3\n800.000625\n\n\n1200\n3\n800.000625\n\n\n1200\n1\n800.000625\n\n\n1200\n3\n800.000625\n\n\n1200\n3\n800.000625\n\n\n1200\n4\n800.002500\n\n\n1200\n4\n800.002500\n\n\n1196\n2\n804.000000\n\n\n1196\n3\n804.000622\n\n\n1195\n4\n805.002485\n\n\n1194\n3\n806.000620\n\n\n1194\n3\n806.000620\n\n\n1193\n2\n807.000000\n\n\n1193\n3\n807.000620\n\n\n1192\n3\n808.000619\n\n\n1192\n4\n808.002475\n\n\n1191\n2\n809.000000\n\n\n1190\n3\n810.000617\n\n\n1188\n1\n812.000616\n\n\n1187\n2\n813.000000\n\n\n1187\n3\n813.000615\n\n\n1187\n3\n813.000615\n\n\n2814\n4\n814.002457\n\n\n1183\n2\n817.000000\n\n\n1183\n3\n817.000612\n\n\n1182\n3\n818.000611\n\n\n1181\n3\n819.000610\n\n\n1180\n2\n820.000000\n\n\n1180\n2\n820.000000\n\n\n1178\n2\n822.000000\n\n\n1178\n3\n822.000608\n\n\n1178\n3\n822.000608\n\n\n2822\n4\n822.002433\n\n\n1176\n2\n824.000000\n\n\n1175\n3\n825.000606\n\n\n1175\n3\n825.000606\n\n\n1174\n2\n826.000000\n\n\n1174\n3\n826.000605\n\n\n2826\n3\n826.000605\n\n\n1173\n3\n827.000605\n\n\n1173\n3\n827.000605\n\n\n1172\n3\n828.000604\n\n\n1167\n3\n833.000600\n\n\n1167\n3\n833.000600\n\n\n1164\n3\n836.000598\n\n\n1163\n3\n837.000597\n\n\n2840\n4\n840.002381\n\n\n1159\n3\n841.000595\n\n\n1159\n3\n841.000595\n\n\n1158\n3\n842.000594\n\n\n1158\n3\n842.000594\n\n\n1158\n3\n842.000594\n\n\n1155\n3\n845.000592\n\n\n1154\n3\n846.000591\n\n\n1154\n3\n846.000591\n\n\n1154\n3\n846.000591\n\n\n1152\n2\n848.000000\n\n\n1152\n2\n848.000000\n\n\n1152\n2\n848.000000\n\n\n1152\n2\n848.000000\n\n\n1152\n3\n848.000590\n\n\n1152\n3\n848.000590\n\n\n1152\n3\n848.000590\n\n\n1151\n2\n849.000000\n\n\n1150\n3\n850.000588\n\n\n1150\n3\n850.000588\n\n\n1148\n3\n852.000587\n\n\n1146\n3\n854.000586\n\n\n1146\n3\n854.000586\n\n\n1145\n2\n855.000000\n\n\n2855\n4\n855.002339\n\n\n1144\n3\n856.000584\n\n\n1144\n3\n856.000584\n\n\n1144\n3\n856.000584\n\n\n1144\n3\n856.000584\n\n\n1143\n3\n857.000583\n\n\n1143\n1\n857.000583\n\n\n1142\n2\n858.000000\n\n\n1142\n3\n858.000583\n\n\n1142\n3\n858.000583\n\n\n1141\n3\n859.000582\n\n\n1141\n3\n859.000582\n\n\n1140\n3\n860.000581\n\n\n1138\n3\n862.000580\n\n\n1137\n4\n863.002318\n\n\n1136\n2\n864.000000\n\n\n1134\n2\n866.000000\n\n\n1134\n3\n866.000577\n\n\n1132\n2\n868.000000\n\n\n1132\n2\n868.000000\n\n\n2868\n4\n868.002304\n\n\n1131\n3\n869.000575\n\n\n1131\n3\n869.000575\n\n\n1130\n2\n870.000000\n\n\n1128\n2\n872.000000\n\n\n1128\n2\n872.000000\n\n\n1128\n3\n872.000573\n\n\n1128\n3\n872.000573\n\n\n2872\n4\n872.002294\n\n\n2872\n4\n872.002294\n\n\n1127\n3\n873.000573\n\n\n1126\n3\n874.000572\n\n\n1126\n3\n874.000572\n\n\n1125\n3\n875.000571\n\n\n1124\n3\n876.000571\n\n\n1121\n3\n879.000569\n\n\n1120\n3\n880.000568\n\n\n1120\n3\n880.000568\n\n\n1117\n3\n883.000566\n\n\n1117\n3\n883.000566\n\n\n1116\n3\n884.000566\n\n\n1116\n3\n884.000566\n\n\n1114\n3\n886.000564\n\n\n1114\n3\n886.000564\n\n\n1114\n3\n886.000564\n\n\n1114\n3\n886.000564\n\n\n1114\n3\n886.000564\n\n\n1113\n3\n887.000564\n\n\n1112\n2\n888.000000\n\n\n1112\n2\n888.000000\n\n\n1112\n3\n888.000563\n\n\n1112\n4\n888.002252\n\n\n1111\n2\n889.000000\n\n\n1111\n3\n889.000562\n\n\n1110\n1\n890.000562\n\n\n1109\n3\n891.000561\n\n\n1108\n3\n892.000561\n\n\n1107\n3\n893.000560\n\n\n1103\n3\n897.000557\n\n\n1102\n2\n898.000000\n\n\n2898\n2\n898.000000\n\n\n1100\n3\n900.000556\n\n\n1100\n3\n900.000556\n\n\n1100\n3\n900.000556\n\n\n1099\n3\n901.000555\n\n\n1098\n2\n902.000000\n\n\n1098\n3\n902.000554\n\n\n1097\n3\n903.000554\n\n\n1096\n2\n904.000000\n\n\n1096\n3\n904.000553\n\n\n1096\n3\n904.000553\n\n\n1095\n2\n905.000000\n\n\n1094\n3\n906.000552\n\n\n1094\n3\n906.000552\n\n\n1093\n2\n907.000000\n\n\n1093\n3\n907.000551\n\n\n1092\n2\n908.000000\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1092\n3\n908.000551\n\n\n1091\n2\n909.000000\n\n\n1090\n2\n910.000000\n\n\n1089\n3\n911.000549\n\n\n1088\n2\n912.000000\n\n\n1088\n2\n912.000000\n\n\n1086\n3\n914.000547\n\n\n1086\n3\n914.000547\n\n\n1082\n3\n918.000545\n\n\n1081\n3\n919.000544\n\n\n1080\n2\n920.000000\n\n\n1080\n2\n920.000000\n\n\n1080\n3\n920.000544\n\n\n1080\n3\n920.000544\n\n\n1078\n2\n922.000000\n\n\n1078\n3\n922.000542\n\n\n1077\n2\n923.000000\n\n\n1077\n2\n923.000000\n\n\n1077\n3\n923.000542\n\n\n1075\n3\n925.000541\n\n\n1074\n3\n926.000540\n\n\n1073\n2\n927.000000\n\n\n1073\n3\n927.000539\n\n\n1073\n3\n927.000539\n\n\n1073\n3\n927.000539\n\n\n1072\n2\n928.000000\n\n\n1072\n2\n928.000000\n\n\n1072\n2\n928.000000\n\n\n1072\n2\n928.000000\n\n\n1072\n2\n928.000000\n\n\n1072\n2\n928.000000\n\n\n1069\n2\n931.000000\n\n\n1069\n2\n931.000000\n\n\n1069\n2\n931.000000\n\n\n1068\n2\n932.000000\n\n\n1068\n3\n932.000536\n\n\n1067\n2\n933.000000\n\n\n1064\n2\n936.000000\n\n\n1064\n3\n936.000534\n\n\n1063\n3\n937.000534\n\n\n1062\n3\n938.000533\n\n\n1061\n1\n939.000532\n\n\n1060\n3\n940.000532\n\n\n1060\n1\n940.000532\n\n\n1060\n3\n940.000532\n\n\n1059\n3\n941.000531\n\n\n1057\n3\n943.000530\n\n\n1057\n3\n943.000530\n\n\n1057\n3\n943.000530\n\n\n1056\n2\n944.000000\n\n\n1056\n3\n944.000530\n\n\n2944\n3\n944.000530\n\n\n1056\n3\n944.000530\n\n\n1056\n3\n944.000530\n\n\n1056\n0\n944.002119\n\n\n1055\n2\n945.000000\n\n\n1055\n2\n945.000000\n\n\n1055\n2\n945.000000\n\n\n1055\n2\n945.000000\n\n\n1055\n2\n945.000000\n\n\n2945\n3\n945.000529\n\n\n1054\n3\n946.000528\n\n\n1054\n3\n946.000528\n\n\n1053\n2\n947.000000\n\n\n1053\n3\n947.000528\n\n\n1053\n3\n947.000528\n\n\n1052\n3\n948.000527\n\n\n1052\n3\n948.000527\n\n\n1051\n3\n949.000527\n\n\n1050\n2\n950.000000\n\n\n1050\n3\n950.000526\n\n\n1050\n3\n950.000526\n\n\n1049\n2\n951.000000\n\n\n1049\n2\n951.000000\n\n\n1048\n2\n952.000000\n\n\n1048\n3\n952.000525\n\n\n1045\n2\n955.000000\n\n\n1045\n3\n955.000524\n\n\n1045\n3\n955.000524\n\n\n1044\n3\n956.000523\n\n\n1043\n2\n957.000000\n\n\n1041\n3\n959.000521\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n2\n960.000000\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1040\n3\n960.000521\n\n\n1039\n3\n961.000520\n\n\n1038\n2\n962.000000\n\n\n1036\n3\n964.000519\n\n\n1036\n1\n964.000519\n\n\n1034\n2\n966.000000\n\n\n1034\n3\n966.000518\n\n\n1034\n3\n966.000518\n\n\n1034\n1\n966.000518\n\n\n1033\n3\n967.000517\n\n\n1032\n2\n968.000000\n\n\n1032\n2\n968.000000\n\n\n1032\n2\n968.000000\n\n\n1032\n3\n968.000517\n\n\n1031\n2\n969.000000\n\n\n1030\n3\n970.000516\n\n\n1029\n3\n971.000515\n\n\n1027\n2\n973.000000\n\n\n1027\n3\n973.000514\n\n\n1024\n2\n976.000000\n\n\n1024\n3\n976.000512\n\n\n1022\n2\n978.000000\n\n\n2978\n5\n978.004601\n\n\n1020\n3\n980.000510\n\n\n1020\n1\n980.000510\n\n\n1015\n3\n985.000508\n\n\n1013\n3\n987.000507\n\n\n1012\n4\n988.002024\n\n\n1009\n3\n991.000505\n\n\n1008\n2\n992.000000\n\n\n1008\n3\n992.000504\n\n\n1008\n3\n992.000504\n\n\n1008\n1\n992.000504\n\n\n1006\n3\n994.000503\n\n\n1005\n2\n995.000000\n\n\n1005\n2\n995.000000\n\n\n1005\n3\n995.000503\n\n\n1004\n2\n996.000000\n\n\n1003\n3\n997.000502\n\n\n1002\n3\n998.000501\n\n\n1001\n2\n999.000000\n\n\n999\n3\n1001.000500\n\n\n999\n3\n1001.000500\n\n\n998\n3\n1002.000499\n\n\n996\n2\n1004.000000\n\n\n996\n3\n1004.000498\n\n\n3005\n3\n1005.000498\n\n\n992\n2\n1008.000000\n\n\n990\n3\n1010.000495\n\n\n990\n3\n1010.000495\n\n\n990\n3\n1010.000495\n\n\n990\n3\n1010.000495\n\n\n990\n3\n1010.000495\n\n\n990\n3\n1010.000495\n\n\n988\n2\n1012.000000\n\n\n988\n2\n1012.000000\n\n\n988\n3\n1012.000494\n\n\n988\n3\n1012.000494\n\n\n988\n3\n1012.000494\n\n\n988\n1\n1012.000494\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n987\n2\n1013.000000\n\n\n985\n2\n1015.000000\n\n\n985\n3\n1015.000493\n\n\n984\n3\n1016.000492\n\n\n984\n3\n1016.000492\n\n\n984\n3\n1016.000492\n\n\n981\n3\n1019.000491\n\n\n980\n3\n1020.000490\n\n\n980\n3\n1020.000490\n\n\n980\n3\n1020.000490\n\n\n980\n4\n1020.001961\n\n\n976\n2\n1024.000000\n\n\n976\n2\n1024.000000\n\n\n974\n3\n1026.000487\n\n\n972\n2\n1028.000000\n\n\n971\n3\n1029.000486\n\n\n970\n3\n1030.000485\n\n\n968\n2\n1032.000000\n\n\n968\n2\n1032.000000\n\n\n968\n2\n1032.000000\n\n\n968\n4\n1032.001938\n\n\n965\n2\n1035.000000\n\n\n964\n2\n1036.000000\n\n\n964\n3\n1036.000483\n\n\n960\n2\n1040.000000\n\n\n960\n2\n1040.000000\n\n\n960\n2\n1040.000000\n\n\n960\n3\n1040.000481\n\n\n960\n3\n1040.000481\n\n\n960\n3\n1040.000481\n\n\n960\n3\n1040.000481\n\n\n960\n3\n1040.000481\n\n\n960\n0\n1040.001923\n\n\n958\n2\n1042.000000\n\n\n958\n2\n1042.000000\n\n\n958\n2\n1042.000000\n\n\n954\n3\n1046.000478\n\n\n952\n2\n1048.000000\n\n\n952\n2\n1048.000000\n\n\n952\n3\n1048.000477\n\n\n951\n2\n1049.000000\n\n\n950\n2\n1050.000000\n\n\n950\n3\n1050.000476\n\n\n949\n2\n1051.000000\n\n\n948\n2\n1052.000000\n\n\n948\n3\n1052.000475\n\n\n948\n3\n1052.000475\n\n\n948\n3\n1052.000475\n\n\n945\n2\n1055.000000\n\n\n943\n2\n1057.000000\n\n\n943\n2\n1057.000000\n\n\n943\n3\n1057.000473\n\n\n941\n3\n1059.000472\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n2\n1064.000000\n\n\n936\n3\n1064.000470\n\n\n936\n3\n1064.000470\n\n\n935\n2\n1065.000000\n\n\n935\n3\n1065.000470\n\n\n934\n2\n1066.000000\n\n\n932\n2\n1068.000000\n\n\n928\n2\n1072.000000\n\n\n928\n3\n1072.000466\n\n\n925\n2\n1075.000000\n\n\n925\n2\n1075.000000\n\n\n925\n3\n1075.000465\n\n\n925\n3\n1075.000465\n\n\n925\n3\n1075.000465\n\n\n924\n2\n1076.000000\n\n\n924\n3\n1076.000465\n\n\n924\n3\n1076.000465\n\n\n923\n2\n1077.000000\n\n\n923\n3\n1077.000464\n\n\n922\n2\n1078.000000\n\n\n919\n2\n1081.000000\n\n\n919\n3\n1081.000462\n\n\n918\n2\n1082.000000\n\n\n918\n3\n1082.000462\n\n\n914\n2\n1086.000000\n\n\n914\n2\n1086.000000\n\n\n914\n2\n1086.000000\n\n\n914\n3\n1086.000460\n\n\n3086\n3\n1086.000460\n\n\n3086\n4\n1086.001842\n\n\n913\n3\n1087.000460\n\n\n912\n2\n1088.000000\n\n\n912\n2\n1088.000000\n\n\n912\n2\n1088.000000\n\n\n912\n2\n1088.000000\n\n\n912\n2\n1088.000000\n\n\n912\n3\n1088.000460\n\n\n912\n3\n1088.000460\n\n\n912\n3\n1088.000460\n\n\n910\n3\n1090.000459\n\n\n909\n3\n1091.000458\n\n\n908\n2\n1092.000000\n\n\n907\n3\n1093.000458\n\n\n907\n3\n1093.000458\n\n\n907\n3\n1093.000458\n\n\n906\n2\n1094.000000\n\n\n904\n2\n1096.000000\n\n\n904\n1\n1096.000456\n\n\n904\n3\n1096.000456\n\n\n902\n2\n1098.000000\n\n\n901\n2\n1099.000000\n\n\n900\n3\n1100.000454\n\n\n900\n3\n1100.000454\n\n\n899\n3\n1101.000454\n\n\n898\n3\n1102.000454\n\n\n897\n3\n1103.000453\n\n\n896\n2\n1104.000000\n\n\n896\n2\n1104.000000\n\n\n894\n2\n1106.000000\n\n\n894\n2\n1106.000000\n\n\n894\n2\n1106.000000\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n894\n3\n1106.000452\n\n\n893\n2\n1107.000000\n\n\n892\n3\n1108.000451\n\n\n892\n3\n1108.000451\n\n\n892\n3\n1108.000451\n\n\n890\n3\n1110.000450\n\n\n889\n3\n1111.000450\n\n\n888\n3\n1112.000450\n\n\n887\n3\n1113.000449\n\n\n884\n2\n1116.000000\n\n\n884\n2\n1116.000000\n\n\n884\n2\n1116.000000\n\n\n882\n2\n1118.000000\n\n\n882\n2\n1118.000000\n\n\n882\n3\n1118.000447\n\n\n882\n1\n1118.000447\n\n\n882\n3\n1118.000447\n\n\n879\n2\n1121.000000\n\n\n879\n3\n1121.000446\n\n\n876\n2\n1124.000000\n\n\n876\n3\n1124.000445\n\n\n874\n3\n1126.000444\n\n\n874\n3\n1126.000444\n\n\n874\n3\n1126.000444\n\n\n872\n2\n1128.000000\n\n\n872\n3\n1128.000443\n\n\n869\n2\n1131.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n2\n1136.000000\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n864\n3\n1136.000440\n\n\n861\n1\n1139.000439\n\n\n3140\n4\n1140.001754\n\n\n858\n2\n1142.000000\n\n\n858\n3\n1142.000438\n\n\n856\n2\n1144.000000\n\n\n854\n2\n1146.000000\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n848\n1\n1152.000434\n\n\n846\n2\n1154.000000\n\n\n845\n2\n1155.000000\n\n\n845\n3\n1155.000433\n\n\n845\n1\n1155.000433\n\n\n845\n3\n1155.000433\n\n\n844\n2\n1156.000000\n\n\n841\n2\n1159.000000\n\n\n840\n2\n1160.000000\n\n\n840\n3\n1160.000431\n\n\n836\n2\n1164.000000\n\n\n835\n2\n1165.000000\n\n\n833\n3\n1167.000428\n\n\n832\n2\n1168.000000\n\n\n832\n2\n1168.000000\n\n\n827\n2\n1173.000000\n\n\n825\n2\n1175.000000\n\n\n825\n2\n1175.000000\n\n\n819\n2\n1181.000000\n\n\n816\n2\n1184.000000\n\n\n816\n2\n1184.000000\n\n\n816\n2\n1184.000000\n\n\n816\n2\n1184.000000\n\n\n816\n2\n1184.000000\n\n\n816\n3\n1184.000422\n\n\n816\n3\n1184.000422\n\n\n813\n2\n1187.000000\n\n\n810\n2\n1190.000000\n\n\n808\n2\n1192.000000\n\n\n808\n1\n1192.000419\n\n\n804\n2\n1196.000000\n\n\n803\n2\n1197.000000\n\n\n800\n1\n1200.000417\n\n\n797\n2\n1203.000000\n\n\n796\n2\n1204.000000\n\n\n796\n2\n1204.000000\n\n\n796\n2\n1204.000000\n\n\n796\n2\n1204.000000\n\n\n793\n2\n1207.000000\n\n\n792\n2\n1208.000000\n\n\n792\n2\n1208.000000\n\n\n792\n2\n1208.000000\n\n\n790\n2\n1210.000000\n\n\n789\n2\n1211.000000\n\n\n789\n2\n1211.000000\n\n\n784\n2\n1216.000000\n\n\n780\n2\n1220.000000\n\n\n773\n2\n1227.000000\n\n\n3228\n4\n1228.001629\n\n\n768\n2\n1232.000000\n\n\n768\n2\n1232.000000\n\n\n768\n2\n1232.000000\n\n\n768\n1\n1232.000406\n\n\n767\n1\n1233.000405\n\n\n765\n2\n1235.000000\n\n\n764\n2\n1236.000000\n\n\n759\n1\n1241.000403\n\n\n756\n2\n1244.000000\n\n\n754\n2\n1246.000000\n\n\n752\n2\n1248.000000\n\n\n747\n2\n1253.000000\n\n\n747\n1\n1253.000399\n\n\n733\n2\n1267.000000\n\n\n732\n2\n1268.000000\n\n\n725\n1\n1275.000392\n\n\n3279\n4\n1279.001564\n\n\n720\n2\n1280.000000\n\n\n720\n1\n1280.000391\n\n\n715\n2\n1285.000000\n\n\n704\n2\n1296.000000\n\n\n698\n2\n1302.000000\n\n\n694\n2\n1306.000000\n\n\n693\n2\n1307.000000\n\n\n691\n2\n1309.000000\n\n\n672\n2\n1328.000000\n\n\n670\n2\n1330.000000\n\n\n641\n2\n1359.000000\n\n\n630\n1\n1370.000365\n\n\n630\n1\n1370.000365\n\n\n630\n1\n1370.000365\n\n\n630\n1\n1370.000365\n\n\n630\n1\n1370.000365\n\n\n630\n1\n1370.000365\n\n\n3390\n5\n1390.003237\n\n\n605\n2\n1395.000000\n\n\n3395\n8\n1395.012903\n\n\n572\n2\n1428.000000\n\n\n3447\n4\n1447.001382\n\n\n540\n1\n1460.000342\n\n\n3493\n3\n1493.000335\n\n\n3500\n4\n1500.001333\n\n\n498\n1\n1502.000333\n\n\n480\n1\n1520.000329\n\n\n438\n1\n1562.000320\n\n\n407\n1\n1593.000314\n\n\n3608\n4\n1608.001244\n\n\n3672\n5\n1672.002691\n\n\n3820\n5\n1820.002473\n\n\n4316\n4\n2316.000864\n\n\n4676\n3\n2676.000187\n\n\n5095\n2\n3095.000000"
  },
  {
    "objectID": "slides/06-knn-workflows.html#original-data",
    "href": "slides/06-knn-workflows.html#original-data",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Original data",
    "text": "Original data\n\nggplot(ames_train, aes(x = Gr_Liv_Area)) +\n  geom_histogram() +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#centering",
    "href": "slides/06-knn-workflows.html#centering",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Centering",
    "text": "Centering\n\nWhat’s different between these two plots\n\n\n\nOriginal data\n\n\nCode\names_train |&gt; \nggplot(aes(x = Gr_Liv_Area)) +\n  geom_histogram() +\n  theme(text = element_text(size = 20)) \n\n\n\n\n\n\n\n\n\n\nCentered data\n\n\nCode\names_train |&gt; mutate(Gr_Liv_Area_Centered = Gr_Liv_Area - mean(Gr_Liv_Area)) |&gt; \nggplot(aes(x = Gr_Liv_Area_Centered)) +\n  geom_histogram() +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#scaling",
    "href": "slides/06-knn-workflows.html#scaling",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Scaling",
    "text": "Scaling\n\nWhat’s different between these two plots\n\n\n\nOriginal data\n\n\nCode\names_train |&gt; \nggplot(aes(x = Gr_Liv_Area)) +\n  geom_histogram() +\n  theme(text = element_text(size = 20)) \n\n\n\n\n\n\n\n\n\n\nNormalized (centered and scaled) data\n\n\nCode\names_train |&gt; mutate(Gr_Liv_Area_Centered_Scaled = (Gr_Liv_Area - mean(Gr_Liv_Area))/sd(Gr_Liv_Area)) |&gt; \nggplot(aes(x = Gr_Liv_Area_Centered_Scaled)) +\n  geom_histogram() +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#original-data-1",
    "href": "slides/06-knn-workflows.html#original-data-1",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Original Data",
    "text": "Original Data\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point()"
  },
  {
    "objectID": "slides/06-knn-workflows.html#centering-1",
    "href": "slides/06-knn-workflows.html#centering-1",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Centering",
    "text": "Centering\n\nWhat’s different between these two plots\n\n\n\nOriginal Data\n\n\nCode\names_train |&gt; \nggplot(aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point()+\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nCentered data\n\n\nCode\names_train |&gt; mutate(Gr_Liv_Area_Centered = Gr_Liv_Area - mean(Gr_Liv_Area),\n                     Bedroom_AbvGr_Centered = Bedroom_AbvGr - mean(Bedroom_AbvGr)) |&gt; \nggplot(aes(x = Gr_Liv_Area_Centered, y = Bedroom_AbvGr_Centered)) +\n  geom_point() +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#scaling-1",
    "href": "slides/06-knn-workflows.html#scaling-1",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Scaling",
    "text": "Scaling\n\nWhat’s different between these two plots\n\n\n\nOriginal data\n\n\nCode\names_train |&gt; \nggplot(aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point()+\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nNormalized data\n\n\nCode\names_train |&gt; mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area),\n                     Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr)) |&gt; \nggplot(aes(x = Gr_Liv_Area_Scaled, y = Bedroom_AbvGr_Scaled)) +\n  geom_point() +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#new-observation-1",
    "href": "slides/06-knn-workflows.html#new-observation-1",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "New Observation",
    "text": "New Observation\n\n\nCode\nnew_house &lt;- tibble(Gr_Liv_Area = 2000, Bedroom_AbvGr = 2)\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr, color = Sale_Price)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\")\n\n\n\n\n\n\n\n\n\n\nWhere should we put this point on the normalized plot?"
  },
  {
    "objectID": "slides/06-knn-workflows.html#plotting-new-observation-witout-normalizing",
    "href": "slides/06-knn-workflows.html#plotting-new-observation-witout-normalizing",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Plotting new observation witout normalizing",
    "text": "Plotting new observation witout normalizing\n\n\nCode\nnew_house &lt;- tibble(Gr_Liv_Area = 2000, Bedroom_AbvGr = 2)\names_train |&gt; mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area), Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr)) |&gt; \n  ggplot(aes(x = Gr_Liv_Area_Scaled, y = Bedroom_AbvGr_Scaled)) +\n  geom_point() +\n  geom_point(data = new_house, mapping = aes(Gr_Liv_Area, y = Bedroom_AbvGr), color = \"red\")\n\n\n\n\n\n\n\n\n\n\nDoes this look right?"
  },
  {
    "objectID": "slides/06-knn-workflows.html#normalized-using-training-data",
    "href": "slides/06-knn-workflows.html#normalized-using-training-data",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Normalized using training data",
    "text": "Normalized using training data\n\n\nOriginal Data\n\n\nCode\nnew_house &lt;- tibble(Gr_Liv_Area = 2000, Bedroom_AbvGr = 2)\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\") +\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nScaled data\n\n\nCode\names_train_scaled &lt;- \n  ames_train |&gt; mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area), Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr))\n\nnew_house &lt;- tibble(Gr_Liv_Area = 2000, Bedroom_AbvGr = 2) |&gt; \n  mutate(Gr_Liv_Area_Scaled = (Gr_Liv_Area - mean(ames_train$Gr_Liv_Area))/sd(ames_train$Gr_Liv_Area), \n         Bedroom_AbvGr_Scaled = (Bedroom_AbvGr - mean(ames_train$Bedroom_AbvGr))/ sd(ames_train$Bedroom_AbvGr))\n\names_train_scaled |&gt; \n  ggplot(aes(x = Gr_Liv_Area_Scaled, y = Bedroom_AbvGr_Scaled)) +\n  geom_point() +\n  geom_point(data = new_house, color = \"red\")+\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#compute-new-distances",
    "href": "slides/06-knn-workflows.html#compute-new-distances",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Compute new distances",
    "text": "Compute new distances\n\nnew_house |&gt; kable()\n\n\n\n\nGr_Liv_Area\nBedroom_AbvGr\nGr_Liv_Area_Scaled\nBedroom_AbvGr_Scaled\n\n\n\n\n2000\n2\n0.9900212\n-1.041808\n\n\n\n\names_dist &lt;- ames_train_scaled |&gt; \n  mutate(orig_dist = sqrt((Gr_Liv_Area - 2000)^2 + (Bedroom_AbvGr-2)^2),\n         scaled_dist = sqrt((Gr_Liv_Area_Scaled - 0.99)^2 + (Bedroom_AbvGr_Scaled-(-1.04))^2))\n\norig_closest &lt;- ames_dist |&gt; \n  arrange(orig_dist) |&gt; \n  head(10)\n\nscaled_closest &lt;- ames_dist |&gt; \n  arrange(scaled_dist) |&gt; \n  head(10)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#scaled-distances-look-better",
    "href": "slides/06-knn-workflows.html#scaled-distances-look-better",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Scaled distances look better",
    "text": "Scaled distances look better\n\n\n\norig_closest |&gt; select(Gr_Liv_Area, Bedroom_AbvGr, Gr_Liv_Area_Scaled, Bedroom_AbvGr_Scaled, orig_dist, scaled_dist) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\nGr_Liv_Area\nBedroom_AbvGr\nGr_Liv_Area_Scaled\nBedroom_AbvGr_Scaled\norig_dist\nscaled_dist\n\n\n\n\n2000\n3\n0.9900212\n0.1621766\n1.000000\n1.2021766\n\n\n2000\n3\n0.9900212\n0.1621766\n1.000000\n1.2021766\n\n\n2004\n4\n0.9979593\n1.3661617\n4.472136\n2.4061749\n\n\n1995\n4\n0.9800985\n1.3661617\n5.385165\n2.4061821\n\n\n2006\n2\n1.0019284\n-1.0418085\n6.000000\n0.0120647\n\n\n2007\n3\n1.0039129\n0.1621766\n7.071068\n1.2022571\n\n\n2007\n3\n1.0039129\n0.1621766\n7.071068\n1.2022571\n\n\n1992\n3\n0.9741448\n0.1621766\n8.062258\n1.2022812\n\n\n2008\n3\n1.0058975\n0.1621766\n8.062258\n1.2022817\n\n\n1991\n3\n0.9721603\n0.1621766\n9.055385\n1.2023090\n\n\n\n\n\n\n\nscaled_closest |&gt;  select(Gr_Liv_Area, Bedroom_AbvGr, Gr_Liv_Area_Scaled, Bedroom_AbvGr_Scaled, orig_dist, scaled_dist) |&gt;  kable()\n\n\n\n\n\n\n\n\n\n\n\n\nGr_Liv_Area\nBedroom_AbvGr\nGr_Liv_Area_Scaled\nBedroom_AbvGr_Scaled\norig_dist\nscaled_dist\n\n\n\n\n2006\n2\n1.0019284\n-1.041809\n6\n0.01206470\n\n\n1987\n2\n0.9642222\n-1.041809\n13\n0.02584121\n\n\n2014\n2\n1.0178047\n-1.041809\n14\n0.02786344\n\n\n1978\n2\n0.9463613\n-1.041809\n22\n0.04367615\n\n\n1976\n2\n0.9423922\n-1.041809\n24\n0.04764210\n\n\n2028\n2\n1.0455882\n-1.041809\n28\n0.05561764\n\n\n2034\n2\n1.0574955\n-1.041809\n34\n0.06751968\n\n\n2046\n2\n1.0813099\n-1.041809\n46\n0.09132782\n\n\n1932\n2\n0.8550726\n-1.041809\n68\n0.13493957\n\n\n1922\n2\n0.8352272\n-1.041809\n78\n0.15478340"
  },
  {
    "objectID": "slides/06-knn-workflows.html#which-plot-looks-better",
    "href": "slides/06-knn-workflows.html#which-plot-looks-better",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Which plot looks better",
    "text": "Which plot looks better\n\n\nOriginal distances\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr, color = Sale_Price)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\") +\n  geom_point(data = orig_closest, color = \"green\") +\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nScaled distances\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr, color = Sale_Price)) + \n  geom_point() +\n  geom_point(data = new_house, color = \"red\") +\n  geom_point(data = scaled_closest, color = \"green\") +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#scaling-test-set",
    "href": "slides/06-knn-workflows.html#scaling-test-set",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Scaling Test Set",
    "text": "Scaling Test Set\n\nSubtract mean of corresponding variable in training set\nDivide by sd of corresponding variable in training set"
  },
  {
    "objectID": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-untransformed",
    "href": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-untransformed",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Visualizing Training and Test Sets: Untransformed",
    "text": "Visualizing Training and Test Sets: Untransformed\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point(color = \"red\", alpha = 0.2) +\n  geom_point(data = ames_test, color = \"blue\", alpha = 0.2) +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-self-centered",
    "href": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-self-centered",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Visualizing Training and Test Sets: Self Centered",
    "text": "Visualizing Training and Test Sets: Self Centered\n\n\nOriginal Data\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point(color = \"red\", alpha = 0.2) +\n  geom_point(data = ames_test, color = \"blue\", alpha = 0.2) +\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nScaled Data\n\n\nCode\names_train_centered &lt;- ames_train |&gt; \n  mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area),\n         Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr))\names_test_self_centered &lt;- ames_test |&gt; \n  mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area),\n         Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr))\nggplot(ames_train_centered, aes(x = Gr_Liv_Area_Scaled, y = Bedroom_AbvGr_Scaled)) + \n  geom_point(color = \"red\", alpha = 0.2) +\n  geom_point(data = ames_test_self_centered, color = \"blue\", alpha = 0.2) +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-training-centered",
    "href": "slides/06-knn-workflows.html#visualizing-training-and-test-sets-training-centered",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Visualizing Training and Test Sets: Training Centered",
    "text": "Visualizing Training and Test Sets: Training Centered\n\n\nOriginal data\n\n\nCode\nggplot(ames_train, aes(x = Gr_Liv_Area, y = Bedroom_AbvGr)) + \n  geom_point(color = \"red\", alpha = 0.2) +\n  geom_point(data = ames_test, color = \"blue\", alpha = 0.2) +\n  theme(text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nScaled data\n\n\nCode\names_train_centered &lt;- ames_train |&gt; \n  mutate(Gr_Liv_Area_Scaled = scale(Gr_Liv_Area),\n         Bedroom_AbvGr_Scaled = scale(Bedroom_AbvGr))\names_test_self_centered &lt;- ames_test |&gt; \n  mutate(Gr_Liv_Area_Scaled = (Gr_Liv_Area - mean(ames_train$Gr_Liv_Area))/sd(ames_train$Gr_Liv_Area), \n         Bedroom_AbvGr_Scaled = (Bedroom_AbvGr - mean(ames_train$Bedroom_AbvGr))/ sd(ames_train$Bedroom_AbvGr))\nggplot(ames_train_centered, aes(x = Gr_Liv_Area_Scaled, y = Bedroom_AbvGr_Scaled)) + \n  geom_point(color = \"red\", alpha = 0.2) +\n  geom_point(data = ames_test_self_centered, color = \"blue\", alpha = 0.2) +\n  theme(text = element_text(size = 20))"
  },
  {
    "objectID": "slides/06-knn-workflows.html#preprocessing-feature-engineering",
    "href": "slides/06-knn-workflows.html#preprocessing-feature-engineering",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Preprocessing + Feature Engineering",
    "text": "Preprocessing + Feature Engineering\n\nPreprocessing: reformatting and transforming data so it can be used for modeling\n\nCentering and scaling\nConverting categorical data into dummy variables or the correct format\nDealing with missing values\n\nFeature Engineering: reformatting and transforming data to improve the performance of your model\n\nCombining variables and dimension reduction techniques\nTransforming variables (e.g. squaring or logging)\n\nFor recommended preprocessing steps see Appendix A of TMWR\nrecipes: combine feature engineering and preprocessing steps into single object that can be applied to different data sets"
  },
  {
    "objectID": "slides/06-knn-workflows.html#model-workflow",
    "href": "slides/06-knn-workflows.html#model-workflow",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Model workflow",
    "text": "Model workflow\n\nStart with raw data\nPreprocessing and feature engineering steps\nFit “model”\nMake predictions using model"
  },
  {
    "objectID": "slides/06-knn-workflows.html#where-does-my-model-start",
    "href": "slides/06-knn-workflows.html#where-does-my-model-start",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Where does my model start?",
    "text": "Where does my model start?\n\nStart with raw data\nModel starts here\n\nPreprocessing and feature engineering steps\nFit “model”\n\nMake predictions using model"
  },
  {
    "objectID": "slides/06-knn-workflows.html#model-assessment",
    "href": "slides/06-knn-workflows.html#model-assessment",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Model Assessment",
    "text": "Model Assessment\n\nStart with raw data\nSplit Data\nUsing training data\n\nPreprocessing and feature engineering steps\nFit “model”\n\nMake predictions on test set\nCompute error metrics"
  },
  {
    "objectID": "slides/06-knn-workflows.html#workflows-and-recipes-in-tidymodels",
    "href": "slides/06-knn-workflows.html#workflows-and-recipes-in-tidymodels",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "workflows and recipes in tidymodels",
    "text": "workflows and recipes in tidymodels\n\ntidymodels provides workflows that combine model fitting with preprocessing steps and consist of:\n\nA model object\nA recipe which combines all of the preprocessing steps"
  },
  {
    "objectID": "slides/06-knn-workflows.html#creating-a-knn-model",
    "href": "slides/06-knn-workflows.html#creating-a-knn-model",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Creating a KNN model",
    "text": "Creating a KNN model\n\nknn10_model &lt;- nearest_neighbor(neighbors = 10) |&gt;   # 10-nn regression\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"regression\")"
  },
  {
    "objectID": "slides/06-knn-workflows.html#recipes-in-tidymodels",
    "href": "slides/06-knn-workflows.html#recipes-in-tidymodels",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "recipe’s in tidymodels",
    "text": "recipe’s in tidymodels\n\nCall receipe with R formula and training set to assign features roles (predictor vs. outcome).\nSequence of step_* specifying the list of transformations.\nApply recipe to new data using predict."
  },
  {
    "objectID": "slides/06-knn-workflows.html#center-and-scale-recipe",
    "href": "slides/06-knn-workflows.html#center-and-scale-recipe",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Center and Scale Recipe",
    "text": "Center and Scale Recipe\n\names_preproc &lt;- recipe(Sale_Price ~ Bedroom_AbvGr + Gr_Liv_Area, data = ames_train) |&gt; \n  step_normalize(all_numeric_predictors())"
  },
  {
    "objectID": "slides/06-knn-workflows.html#create-workflow",
    "href": "slides/06-knn-workflows.html#create-workflow",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Create workflow",
    "text": "Create workflow\n\nknn10_workflow &lt;- workflow() |&gt; \n  add_model(knn10_model) |&gt; \n  add_recipe(ames_preproc)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#fitting-model",
    "href": "slides/06-knn-workflows.html#fitting-model",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Fitting Model",
    "text": "Fitting Model\n\nknnfit10 &lt;- knn10_workflow |&gt; \n  fit(data = ames_train)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#making-predictions-single-observation",
    "href": "slides/06-knn-workflows.html#making-predictions-single-observation",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Making predictions: Single observation",
    "text": "Making predictions: Single observation\nTest Point: Gr_Liv_area = 2000 square feet, and Bedroom_AbvGr = 3, then\n\nnew_house |&gt; kable()\n\n\n\n\nGr_Liv_Area\nBedroom_AbvGr\nGr_Liv_Area_Scaled\nBedroom_AbvGr_Scaled\n\n\n\n\n2000\n2\n0.9900212\n-1.041808\n\n\n\n\n# obtain 10-nn prediction\npredict(knnfit10, new_data = new_house) |&gt; kable()\n\n\n\n\n.pred\n\n\n\n\n342204.5"
  },
  {
    "objectID": "slides/06-knn-workflows.html#making-predictions-test-set",
    "href": "slides/06-knn-workflows.html#making-predictions-test-set",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Making Predictions: Test Set",
    "text": "Making Predictions: Test Set\n\n# obtain 10-nn prediction\npredict(knnfit10, new_data = ames_test) |&gt; head() |&gt; kable()\n\n\n\n\n.pred\n\n\n\n\n178219.0\n\n\n167930.0\n\n\n177574.5\n\n\n238122.2\n\n\n126330.0\n\n\n80109.5"
  },
  {
    "objectID": "slides/06-knn-workflows.html#linear-regression-vs-k-nearest-neighbors",
    "href": "slides/06-knn-workflows.html#linear-regression-vs-k-nearest-neighbors",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Linear Regression vs K-Nearest Neighbors",
    "text": "Linear Regression vs K-Nearest Neighbors\n\nLinear regression is a parametric approach (with restrictive assumptions), KNN is non-parametric.\nLinear regression works for regression problems (\\(Y\\) numerical), KNN can be used for both regression and classification - i.e. \\(Y\\) qualitative\nLinear regression is interpretable, KNN is not.\nLinear regression can accommodate qualitative predictors and can be extended to include interaction terms as well while KNN does not allow for qualitative predictors\nPerformance: KNN can be pretty good for small \\(p\\), that is, \\(p \\le 4\\) and large \\(n\\). Performance of KNN deteriorates as \\(p\\) increases - curse of dimensionality"
  },
  {
    "objectID": "slides/06-knn-workflows.html#linear-regression",
    "href": "slides/06-knn-workflows.html#linear-regression",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Linear regression",
    "text": "Linear regression\n\n# Best model from last time\nfit3 &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(Sale_Price ~ Gr_Liv_Area + Bedroom_AbvGr, data = ames_train)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#nn",
    "href": "slides/06-knn-workflows.html#nn",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "5-NN",
    "text": "5-NN\n\nknn5_model &lt;- nearest_neighbor(neighbors = 5) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\")\n\nknn5_fit &lt;- workflow() |&gt; \n  add_model(knn5_model) |&gt; \n  add_recipe(ames_preproc) |&gt; \n  fit(data = ames_train)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#making-test-predictions",
    "href": "slides/06-knn-workflows.html#making-test-predictions",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Making test predictions",
    "text": "Making test predictions\n\names_test_preds &lt;- ames_test |&gt; \n  mutate(lr_preds = predict(fit3, new_data = ames_test)$.pred,\n         knn5_preds = predict(knn5_fit, new_data = ames_test)$.pred,\n         knn10_preds = predict(knnfit10, new_data = ames_test)$.pred)"
  },
  {
    "objectID": "slides/06-knn-workflows.html#computing-error-metrics-rmse",
    "href": "slides/06-knn-workflows.html#computing-error-metrics-rmse",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Computing Error Metrics: RMSE",
    "text": "Computing Error Metrics: RMSE\n\nrmse(ames_test_preds, estimate = lr_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n52272.26\n\n\n\n\nrmse(ames_test_preds, estimate = knn5_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n50954.43\n\n\n\n\nrmse(ames_test_preds, estimate = knn10_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n48926.63"
  },
  {
    "objectID": "slides/06-knn-workflows.html#computing-error-metrics-r2",
    "href": "slides/06-knn-workflows.html#computing-error-metrics-r2",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "Computing Error Metrics: \\(R^2\\)",
    "text": "Computing Error Metrics: \\(R^2\\)\n\nrsq(ames_test_preds, estimate = lr_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.574702\n\n\n\n\nrsq(ames_test_preds, estimate = knn5_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.605208\n\n\n\n\nrsq(ames_test_preds, estimate = knn10_preds, truth = Sale_Price) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.6296263"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#computational-setup",
    "href": "slides/04-MultipleRegression.html#computational-setup",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "",
    "text": "library(tidyverse)\nlibrary(knitr)\nlibrary(readODS)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#question",
    "href": "slides/04-MultipleRegression.html#question",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Question!!!",
    "text": "Question!!!\nAs a model’s flexibility increases:\n\nQuestionsAnswers\n\n\n\nits variance (increases/decreases)\nits bias (increases/decreases)\nits training MSE (increases/decreases)\nits test MSE (describe)\n\n\n\n\nits variance (increases)\nits bias (decreases)\nits training MSE (decreases)\nits test MSE (decreases at first, then increases and the model starts to overfit, U-shaped)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#outlet-data",
    "href": "slides/04-MultipleRegression.html#outlet-data",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Outlet Data",
    "text": "Outlet Data\nSuppose the CEO of a restaurant franchise is considering opening new outlets in different cities. They would like to expand their business to cities that give them higher profits with the assumption that highly populated cities will probably yield higher profits.\nThey have data on the population (in 100,000) and profit (in $1,000) at 97 cities where they currently have outlets.\n\noutlets &lt;- readRDS(\"../data/outlets.rds\")   # load dataset\nhead(outlets) |&gt; kable() # first six observations of the dataset\n\n\n\n\npopulation\nprofit\n\n\n\n\n6.1101\n17.5920\n\n\n5.5277\n9.1302\n\n\n8.5186\n13.6620\n\n\n7.0032\n11.8540\n\n\n5.8598\n6.8233\n\n\n8.3829\n11.8860"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#r-as-an-open-source-language",
    "href": "slides/04-MultipleRegression.html#r-as-an-open-source-language",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "R as an open-source language",
    "text": "R as an open-source language\n\nR is an open source language\n\nAdvantages:\n\nPackages for almost anything you want\n“Cutting edge” methods rolled out quickly and early\n\nDisadvantages\n\nMany packages (especially new ones) may have bugs\nLots of syntactical diversity\nSyntax is frequently dependent on the needs of the person who wrote the package and conventions at the time the package was created"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#enter-tidyverse",
    "href": "slides/04-MultipleRegression.html#enter-tidyverse",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Enter tidyverse",
    "text": "Enter tidyverse\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\n\ntidyverse is for manipulating and visualizing data\nthe tidyverse is a meta-package meaning it is a collection of a bunch of other packages"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#enter-tidymodels",
    "href": "slides/04-MultipleRegression.html#enter-tidymodels",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Enter tidymodels",
    "text": "Enter tidymodels\n\nThe tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n\ntidymodels creates a unified framework for building models in R\nEric’s opinion: similar idea to scikit-learn in Python"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#back-to-linear-regression",
    "href": "slides/04-MultipleRegression.html#back-to-linear-regression",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Back to linear regression",
    "text": "Back to linear regression\n\nggplot(data = outlets) +\n  geom_point(mapping = aes(x = population, y = profit)) +    # create scatterplot\n  geom_smooth(mapping = aes(x = population, y = profit), \n              method = \"lm\", se = FALSE)   # add the regression line"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#classic-linear-regression-in-r",
    "href": "slides/04-MultipleRegression.html#classic-linear-regression-in-r",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Classic Linear Regression in R",
    "text": "Classic Linear Regression in R\n\n\n\noutlets_model &lt;- lm(profit ~ population, data = outlets)\noutlets_model\n\n\nCall:\nlm(formula = profit ~ population, data = outlets)\n\nCoefficients:\n(Intercept)   population  \n     -3.896        1.193  \n\n\n\nThis corresponds to the model:\n\\[\n\\begin{aligned}\n\\text{Profit}  &= -3.90 + 1.19\\times\\text{Population}\\\\\n\\hat{Y}_i &= -3.90 + 1.19X_i\n\\end{aligned}\n\\] i.e. \\(\\hat{\\beta}_0 = -3.90\\) and \\(\\hat{\\beta}_1 = 1.19\\)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#modeling-with-tidymodels",
    "href": "slides/04-MultipleRegression.html#modeling-with-tidymodels",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Modeling with tidymodels",
    "text": "Modeling with tidymodels\n\nSpecify mathematical structure of model (e.g. linear regression, logistic regression)\nSpecify the engine for fitting the model. (e.g. lm, stan, glmnet).\nWhen required, declare the mode of the model (i.e. regression or classification)."
  },
  {
    "objectID": "slides/04-MultipleRegression.html#linear-regression-with-tidymodels",
    "href": "slides/04-MultipleRegression.html#linear-regression-with-tidymodels",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Linear Regression with tidymodels",
    "text": "Linear Regression with tidymodels\n\n# Usually put these at the top\nlibrary(tidymodels) # load tidymodels package\ntidymodels_prefer() # avoid common conflicts\n\nlm_model &lt;- linear_reg() |&gt; # Step 1\n  set_engine(\"lm\") # Step 2\n\n# Step 3 not required since linear regression can't be used for classification\n\n# Fit the model\nlm_model_fit &lt;- lm_model |&gt; \n  fit(profit ~ population, data = outlets)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#linear-regression-with-tidymodels-1",
    "href": "slides/04-MultipleRegression.html#linear-regression-with-tidymodels-1",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Linear Regression with tidymodels",
    "text": "Linear Regression with tidymodels\n\nlm_model_fit |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-3.895781\n0.7194828\n-5.414696\n5e-07\n\n\npopulation\n1.193034\n0.0797439\n14.960806\n0e+00\n\n\n\n\n\nSame model as before:\n\\[\n\\begin{aligned}\n\\text{Profit}  &= -3.90 + 1.19\\times\\text{Population}\\\\\n\\hat{Y}_i &= -3.90 + 1.19X_i\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#linear-regression-in-r-prediction",
    "href": "slides/04-MultipleRegression.html#linear-regression-in-r-prediction",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Linear Regression in R: Prediction",
    "text": "Linear Regression in R: Prediction\n\nnew_cities &lt;- tibble(population = rnorm(100, 7, 3))\n\nlm_model_fit |&gt; \npredict(new_data = new_cities) |&gt; \n  kable()\n\n\n\n\n.pred\n\n\n\n\n4.2987846\n\n\n4.3494028\n\n\n1.6358812\n\n\n2.8371943\n\n\n8.6964415\n\n\n1.2610114\n\n\n4.4093826\n\n\n7.0132467\n\n\n8.9081681\n\n\n-3.7261162\n\n\n-0.3152552\n\n\n-0.0578742\n\n\n3.1103344\n\n\n-1.4511631\n\n\n3.2056127\n\n\n7.2094157\n\n\n5.2694317\n\n\n9.8741950\n\n\n5.3986612\n\n\n5.7894116\n\n\n6.5817409\n\n\n6.4489003\n\n\n1.5635853\n\n\n-5.8145557\n\n\n0.3096059\n\n\n4.7217335\n\n\n8.0782299\n\n\n6.4065191\n\n\n7.0495725\n\n\n10.5119132\n\n\n3.2281542\n\n\n5.3756384\n\n\n10.5424872\n\n\n2.6444235\n\n\n3.1483294\n\n\n5.7659904\n\n\n3.3551593\n\n\n-1.1774409\n\n\n9.5361487\n\n\n-0.3336581\n\n\n4.1063193\n\n\n2.7105932\n\n\n6.1253040\n\n\n3.6662178\n\n\n6.1309169\n\n\n9.2088409\n\n\n-2.1660582\n\n\n4.9800410\n\n\n5.5140171\n\n\n3.3714057\n\n\n5.0758817\n\n\n2.9767839\n\n\n4.4891785\n\n\n-2.2982990\n\n\n-2.0242922\n\n\n4.4403419\n\n\n4.0335077\n\n\n6.8410161\n\n\n3.6835670\n\n\n-0.6670421\n\n\n6.1270222\n\n\n7.1611647\n\n\n3.2563676\n\n\n11.3135720\n\n\n-0.8720586\n\n\n7.4979481\n\n\n5.4139803\n\n\n7.3006650\n\n\n3.3803037\n\n\n5.1760916\n\n\n4.7255493\n\n\n5.3377241\n\n\n8.0237716\n\n\n5.0616701\n\n\n-1.7427836\n\n\n5.5563838\n\n\n9.1823534\n\n\n3.3094120\n\n\n4.7223157\n\n\n3.3083360\n\n\n4.0882384\n\n\n-0.3834378\n\n\n0.9586050\n\n\n6.1677633\n\n\n-0.9134436\n\n\n6.9813206\n\n\n-1.8743103\n\n\n10.6155073\n\n\n1.7708313\n\n\n10.9242269\n\n\n4.7897426\n\n\n5.2228631\n\n\n5.1455980\n\n\n-0.9701960\n\n\n5.7970445\n\n\n0.3254730\n\n\n1.5828592\n\n\n-1.7630027\n\n\n-3.9350027\n\n\n8.2198551\n\n\n\n\n\n\nnew_cities &lt;- tibble(population = rnorm(100, 7, 3))\n\nnew_cities &lt;- new_cities |&gt; \n  bind_cols(predict(lm_model_fit, new_data = new_cities, type = \"pred_int\")) |&gt; \n  kable()\n\nNote: New data must be a data frame with the same columns names as the training data"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-1",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-1",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\n\nResponse: \\(Y\\)\nPredictor Variables: \\(X_1, X_2, \\ldots, X_p\\)\nAssume true relationship:\n\n\\[\n\\begin{aligned}\nY&=f(\\mathbf{X}) + \\epsilon\\\\\n&=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p + \\epsilon\n\\end{aligned}\n\\] where \\(\\beta_j\\) quantifies the association between the \\(j^{th}\\) predictor and the response."
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-estimating-parameters",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-estimating-parameters",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression: Estimating Parameters",
    "text": "Multiple Linear Regression: Estimating Parameters\n\nSuppose \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) are estimates of \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)\nTraining Data:\n\nObserved response: \\(y_i\\) for \\(i=1,\\ldots,n\\)\nObserved predictors: \\(x_{1i}, x_{2i}, \\ldots x_{pi}\\) for \\(i=1,\\ldots, n\\)\n\nPredicted response: \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{1i} + \\ldots + \\hat{\\beta}_px_{pi} \\text{ for } i=1, \\ldots, n\\]\nResiduals: \\(e_i = \\hat{y}_i - y_i\\) for \\(i=1, \\ldots, n\\)\nMean Squared Error (MSE): \\(MSE =\\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n}\\)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-estimating-parameters-1",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-estimating-parameters-1",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression: Estimating Parameters",
    "text": "Multiple Linear Regression: Estimating Parameters\n\nGoal: Use training data to find \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) that minimizes MSE\n\n\\(\\hat{\\beta}_i\\)’s called least-squares estimators\nSince minimizing MSE \\(\\implies\\) MSE is called cost/loss function\n\nCan use calculus or gradient descent to find \\(\\hat{\\beta}_i\\)’s"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#house-prices-dataset",
    "href": "slides/04-MultipleRegression.html#house-prices-dataset",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "House Prices dataset",
    "text": "House Prices dataset\n\nsize is in square feet\nnum_bedrooms is a count\nprice is in $1,000’s\n\n\nhouse_prices &lt;- readRDS(\"../data/house_prices.rds\")   # load dataset\nhead(house_prices, 6) |&gt; kable()  # print first 6 observations\n\n\n\n\nsize\nnum_bedrooms\nprice\n\n\n\n\n2104\n3\n399.9\n\n\n1600\n3\n329.9\n\n\n2400\n3\n369.0\n\n\n1416\n2\n232.0\n\n\n3000\n4\n539.9\n\n\n1985\n4\n299.9"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-2",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-2",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nSome Exploratory Data Analysis (EDA)\n\nlibrary(GGally)\nggpairs(data = house_prices)   # correlation plot"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-in-r",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-in-r",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression in R",
    "text": "Multiple Linear Regression in R\n\nmlr_model &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")\n\nhouse_price_mlr &lt;- mlr_model |&gt; \n  fit(price ~ size + num_bedrooms, data = house_prices)   # fit the model\n\nhouse_price_mlr |&gt; \n  tidy() |&gt;   # produce result summaries of the model\n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n89.5977660\n41.7674230\n2.1451591\n0.0374991\n\n\nsize\n0.1392106\n0.0147951\n9.4092391\n0.0000000\n\n\nnum_bedrooms\n-8.7379154\n15.4506975\n-0.5655353\n0.5745825"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-interpreting-parameters",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-interpreting-parameters",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression: Interpreting Parameters",
    "text": "Multiple Linear Regression: Interpreting Parameters\n\n\\(\\hat{\\beta}_0=89.5978\\): The intercept \\(\\implies\\) a house with 0 square feet and 0 bedrooms would cost approximately $89,598.80. Is this meaningful in context? Not really\n\\(\\hat{\\beta}_1=0.1392\\): With num_bedrooms remaining fixed, an additional 1 square foot of size leads to an increase in price by approximately $139.20.\n\\(\\hat{\\beta}_2=-8.7379\\): With size remaining fixed, an additional bedroom leads to an decrease in price of approximately $8,737.90.\n\n. . .\n\nHmm…. that’s a little weird…\nSimpson’s Paradox: when relationship between two variables disappears or reverses when controlling for a third, confounding variable"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-interpreting-parameters-1",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-interpreting-parameters-1",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression: Interpreting Parameters",
    "text": "Multiple Linear Regression: Interpreting Parameters\n\n\nWrite down our model in mathematical notation\n\\(\\text{price} = 89.5978 + 0.1392\\times\\text{size} - 8.7379\\times\\text{num_bedrooms}\\)\n\\(Y = 89.5978 + 0.1392X_1 - 8.7379X_2\\)"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#multiple-linear-regression-prediction",
    "href": "slides/04-MultipleRegression.html#multiple-linear-regression-prediction",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Multiple Linear Regression: Prediction",
    "text": "Multiple Linear Regression: Prediction\n\nPrediction of price when size is 2000 square feet for a house with 3 bedrooms\n\\(\\text{sales} = 89.5978 + 0.1392\\times2000 - 8.7379\\times3 = 341.7841\\)\n\n. . .\n\npredict(house_price_mlr, new_data = tibble(size = 2000, num_bedrooms = 3))   # obtain prediction\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1  342.\n\n\n\nWhy don’t these match exactly? rounding"
  },
  {
    "objectID": "slides/04-MultipleRegression.html#linear-regression-comparing-models",
    "href": "slides/04-MultipleRegression.html#linear-regression-comparing-models",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "Linear Regression: Comparing Models",
    "text": "Linear Regression: Comparing Models\n\n\n\nMany methods for comparing regression models from your regression course\nToday: Data splitting\nFirst: New Data\n\n\n\n\n\names housing data\n\nMany variables\nFocus on:\n\nSale_Price: in dollars\nGr_Liv_Area: size in square feet\nBedroom_AbvGr: number of bedrooms above grade"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#pre-survey",
    "href": "slides/02-StatisticalLearning.html#pre-survey",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Pre-Survey",
    "text": "Pre-Survey\nTake 10 minute and fill our this survey.\nIf you chose to opt-out please read as much of this article as you an in 8 minutes and then fill out this survey."
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#data-generating-process",
    "href": "slides/02-StatisticalLearning.html#data-generating-process",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Data Generating Process",
    "text": "Data Generating Process\nSuppose we have\n\nFeatures: \\(\\mathbf{X}\\)\nTarget: \\(Y\\)\nGoal: Predict \\(Y\\) using \\(\\mathbf{X}\\)\n\n\n\nData generating process: underlying, unseen and unknowable process that generates \\(Y\\) given \\(\\mathbf{X}\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#population",
    "href": "slides/02-StatisticalLearning.html#population",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Population",
    "text": "Population\nMore mathematically, the “true”/population model can be represented by\n\\[Y=f(\\mathbf{X}) + \\epsilon\\]\nwhere \\(\\epsilon\\) is a random error term (includes measurement error, other discrepancies) independent of \\(\\mathbf{X}\\) and has mean zero.\n\nGOAL: Estimate \\(f\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#why-estimate-fmathbfx",
    "href": "slides/02-StatisticalLearning.html#why-estimate-fmathbfx",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Why Estimate \\(f(\\mathbf{X})\\)?",
    "text": "Why Estimate \\(f(\\mathbf{X})\\)?\nWe wish to know about \\(f(\\mathbf{X})\\) for two reasons:\n\nPrediction: make an educated guess for what \\(y\\) should be given a new \\(x_0\\): \\[\\hat{y}_0=\\hat{f}(x_0) \\ \\ \\ \\text{or} \\ \\ \\ \\hat{y}_0=\\hat{C}(x_0)\\]\nInference: Understand the relationship between \\(\\mathbf{X}\\) and \\(Y\\).\n\n\n\nAn ML algorithm that is developed mainly for predictive purposes is often termed as a Black Box algorithm."
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#prediction",
    "href": "slides/02-StatisticalLearning.html#prediction",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Prediction",
    "text": "Prediction\nThere are two types of prediction problems:\n\nRegression (response \\(Y\\) is quantitative): Build a model \\(\\hat{Y} = \\hat{f}(\\mathbf{X})\\)\nClassification (response \\(Y\\) is qualitative/categorical): Build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\)\n\n\n\nNote: a “hat”, \\(\\hat{\\phantom{f}}\\), over an object represents an estimate of that object\n\nE.g. \\(\\hat{Y}\\) is an estimate of \\(Y\\) and \\(\\hat{f}\\) is an estimate of \\(f\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#prediction-and-inference",
    "href": "slides/02-StatisticalLearning.html#prediction-and-inference",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Prediction and Inference",
    "text": "Prediction and Inference\nIncome dataset\n\nWhy ML? (from ISLR2)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#prediction-and-inference-1",
    "href": "slides/02-StatisticalLearning.html#prediction-and-inference-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Prediction and Inference",
    "text": "Prediction and Inference\nIncome dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy ML? (from ISLR2)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#question",
    "href": "slides/02-StatisticalLearning.html#question",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Question!!!",
    "text": "Question!!!\nBased on the previous two slides, which of the following statements are correct?\n\nQuestionsAnswers\n\n\n\nAs Years of Education increases, Income increases, keeping Seniority fixed.\nAs Years of Education increases, Income decreases, keeping Seniority fixed.\nAs Years of Education increases, Income increases.\nAs Seniority increases, Income increases, keeping Years of Education fixed.\nAs Seniority increases, Income decreases, keeping Years of Education fixed.\nAs Seniority increases, Income increases.\n\n\n\n\nAs Years of Education increases, Income increases, keeping Seniority fixed. TRUE\nAs Years of Education increases, Income decreases, keeping Seniority fixed. FALSE\nAs Years of Education increases, Income increases. TRUE\nAs Seniority increases, Income increases, keeping Years of Education fixed. TRUE\nAs Seniority increases, Income decreases, keeping Years of Education fixed. FALSE\nAs Seniority increases, Income increases. TRUE"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#discussion",
    "href": "slides/02-StatisticalLearning.html#discussion",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Discussion",
    "text": "Discussion\nWhat’s the difference between these two statements:\n\nAs Years of Education increases, Income increases, keeping Seniority fixed.\nAs Years of Education increases, Income increases."
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#how-do-we-estimate-fmathbfx",
    "href": "slides/02-StatisticalLearning.html#how-do-we-estimate-fmathbfx",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "How Do We Estimate \\(f(\\mathbf{X})\\)?",
    "text": "How Do We Estimate \\(f(\\mathbf{X})\\)?\nBroadly speaking, we have two approaches.\n\nParametric methods\nNon-parametric methods"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#parametric-methods",
    "href": "slides/02-StatisticalLearning.html#parametric-methods",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Parametric Methods",
    "text": "Parametric Methods\n\nAssume a functional form for \\(f(\\mathbf{X})\\)\n\nLinear Regression: \\(f(\\mathbf{X})=\\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\ldots + \\beta_p \\mathbf{x}_p\\)\nEstimate the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using labeled data\n\nChoosing \\(\\beta\\)’s that minimize some error metrics is called fitting the model\nThe data we use to fit the model is called our training data"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#parametric-methods-1",
    "href": "slides/02-StatisticalLearning.html#parametric-methods-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Parametric Methods",
    "text": "Parametric Methods\n\nParametric model fit (from ISLR2)\n\nWhat are some potential parametric models that could result in this picture?\nNote: Right line is the true relationship"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#parametric-methods-2",
    "href": "slides/02-StatisticalLearning.html#parametric-methods-2",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Parametric Methods",
    "text": "Parametric Methods\nIncome dataset\n\n\n\n\n\n\n\n\n\nTrue relationship\n\n\n\n\n\n\n\nParametric model\n\n\n\n\n\n\nFrom ISLR2\n\n\n\n\n\nWhat are some functions that could have resulted in the model on the right?\n\\(\\text{Income} \\approx \\beta_0 + \\beta_1\\times\\text{Years of Education} + \\beta_2\\times\\text{Seniority}\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#non-parametric-methods",
    "href": "slides/02-StatisticalLearning.html#non-parametric-methods",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Non-parametric Methods",
    "text": "Non-parametric Methods\n\nNon-parametric approach: no explicit assumptions about the functional form of \\(f(\\mathbf{X})\\)\nMuch more observations (compared to a parametric approach) required to fit non-parametric model\n\nIdea: parametric model restricts space of possible answers\n\n\nIncome dataset\n\n\n\n\n\n\n\n\n\nTrue relationship\n\n\n\n\n\n\n\nNon-parametric model fit\n\n\n\n\n\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-flexibility-of-models",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-flexibility-of-models",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Flexibility of Models",
    "text": "Supervised Learning: Flexibility of Models\n\nFlexibility: smoothness of functions\nMore theoretically: how many parameters are there to estimate?\n\n\n[More flexible \\(\\implies\\) More complex \\(\\implies\\) Less Smooth \\(\\implies\\) Less Restrictive \\(\\implies\\) Less Interpretable"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-some-trade-offs",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-some-trade-offs",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Some Trade-offs",
    "text": "Supervised Learning: Some Trade-offs\n\nPrediction Accuracy versus Interpretability\nGood Fit versus Over-fit or Under-fit\n\n\nTrade-off between flexibility and interpretability (from ISLR2)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-selecting-a-model",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-selecting-a-model",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Selecting a Model",
    "text": "Supervised Learning: Selecting a Model\n\nWhy so many different ML techniques?\nThere is no free lunch in statistics: All methods have different pros and cons\n\nMust select correct model for each use-case\n\nRelevant questions in model selection:\n\nHow much observations \\(n\\) and variables \\(p\\)?\nWhat is the relative importance is prediction, interpretability, and inference?\nDo we expect relationship to be non-linear?\nRegression or classification?"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\nWhen we estimate \\(f(\\mathbf{X})\\) using \\(\\hat{f}(\\mathbf{X})\\), then,\n\n\\[\\underbrace{E\\left[Y-\\hat{Y}\\right]^2}_{Error}=E\\left[f(\\mathbf{X})+\\epsilon - \\hat{f}(\\mathbf{X})\\right]^2=\\underbrace{E\\left[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})\\right]^2}_{Reducible} + \\underbrace{Var(\\epsilon)}_{Irreducible}\\]\n\n\\(E\\left[Y-\\hat{Y}\\right]^2\\): Expected (average) squared difference between predicted and actual (observed) response, Mean Squared Error (MSE)\nGoal: find an estimate of \\(f(\\mathbf{X})\\) to minimize the reducible error"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance-1",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\n\nLabeled training data \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\)\n\ni.e. \\(n\\) training observations\n\nFit/train a model from training data\n\n\\(\\hat{y}=\\hat{f}(x)\\), regression\n\\(\\hat{y}=\\hat{C}(x)\\), classification\n\n\nObtain estimates \\(\\hat{f}(x_1), \\hat{f}(x_2), \\ldots, \\hat{f}(x_n)\\) (or, \\(\\hat{C}(x_1), \\hat{C}(x_2), \\ldots, \\hat{C}(x_n)\\)) of training data\nCompute error:\n\nRegression \\[\\text{Training MSE}=\\text{Average}_{Training} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\left(y_i-\\hat{f}(x_i)\\right)^2\\]\nClassification \\[\n\\begin{aligned}\n\\text{Training Error Rate}\n&=\\text{Average}_{Training} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]\\\\\n&= \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\ I\\left(y_i \\ne \\hat{C}(x_i)\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#recap",
    "href": "slides/02-StatisticalLearning.html#recap",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Recap",
    "text": "Recap\n\nRegression vs. Classification\nParametric vs. non-parametric models\nTraining v. test data\nAssessing regression models: Mean-Squared Error\nTrade-offs:\n\nFlexibility vs. interpretability\nBias vs. variance\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/00-welcome.html#meet-prof.-friedlander",
    "href": "slides/00-welcome.html#meet-prof.-friedlander",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Meet Prof. Friedlander!",
    "text": "Meet Prof. Friedlander!\n\n\nEducation and career journey\n\nGrew up outside New York City\nBS in Math & Statistics from Rice University (Houston, TX)\nBusiness Analyst at Capital One (Plano, TX)\nMS and PhD in Statistics & Operations Research from UNC-Chapel Hill\nPostdoc in Population Genetics at University of Chicago\nAssistant Professor of Math at St. Norbert College (Green Bay, WI)\n\nWork focuses on statistics education, queuing theory, and population genetics\nBig sports fan: NY Knicks, Giants, Rangers, Yankees, UNC Tarheels\nDad of three cute dogs: Allie, Miriam, Tony"
  },
  {
    "objectID": "slides/00-welcome.html#meet-prof.-friedlander-1",
    "href": "slides/00-welcome.html#meet-prof.-friedlander-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Meet Prof. Friedlander!",
    "text": "Meet Prof. Friedlander!"
  },
  {
    "objectID": "slides/00-welcome.html#tell-me-about-yourself-github",
    "href": "slides/00-welcome.html#tell-me-about-yourself-github",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Tell me about yourself + GitHub",
    "text": "Tell me about yourself + GitHub\nCreate a GitHub account. You may want to use this to show-off work to future employers so I recommend using something professional (like your name) as your user name.\nSend me an email with answers to the following questions:\n\nWhat is the GitHub username you just created?\nWhat would you like me to call you?\nWhy are you taking this class?\nWhat are your career goals?\nIs there anything else you would like me to know about you? E.g. athlete, preferred pronouns, accommodations, etc…\nPlease recommend at least one and up to infinity songs for the class playlist."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq",
    "href": "slides/00-welcome.html#course-faq",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What background is assumed for the course?\nA - Familiarity with concepts from statistical inference, linear regression, and logistic regression. A solid grounding in R, including the tidyverse and ggplot."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-1",
    "href": "slides/00-welcome.html#course-faq-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - Will we be doing computing?\nA - Yes. We will use the computing language R for analysis, Quarto for writing up results, and GitHub for version control and collaboration"
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-2",
    "href": "slides/00-welcome.html#course-faq-2",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - Will we learn the mathematical theory?\nA - Yes and No. The course is primarily focused on application; however, we will discuss some of the mathematics occasionally."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-3",
    "href": "slides/00-welcome.html#course-faq-3",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What distinguishes this from a 300-level course?\nA - I expect a high level of independence from you. You should not be relying on me to teach you every small detail from this course. For example, if you tell you about an R function, I expect that you will be able to figure out how to use it yourself."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-4",
    "href": "slides/00-welcome.html#course-faq-4",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - Is there anything else I should know?\nA - Machine learning is a RAPIDLY evolving field. If you want to be successful in this field going forward, you will need to be able to learn things for yourself and SELF-ASSESS whether you know them. There are portions of this course that I have intentionally designed to not give you enough information to solve on your own."
  },
  {
    "objectID": "slides/00-welcome.html#course-learning-objectives",
    "href": "slides/00-welcome.html#course-learning-objectives",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to…\n\ntackle predictive modeling problems arising from real data.\nuse R to fit and evaluate machine learning models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate results results through writing and oral presentations."
  },
  {
    "objectID": "slides/00-welcome.html#course-toolkit",
    "href": "slides/00-welcome.html#course-toolkit",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nCourse website\n\nCentral hub for the course!\nTour of the website\n\nCanvas\n\nGradebook\nAnnouncements\n\nGitHub\n\nDistribute assignments\nPlatform for version control and collaboration"
  },
  {
    "objectID": "slides/00-welcome.html#computing-toolkit",
    "href": "slides/00-welcome.html#computing-toolkit",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nWrite reproducible reports in Quarto\nAccess RStudio through your personal computer (preferred) or the RStudio Server (email me ASAP if you are doing this)\n\n\n\n\n\n\n\n\n\n\n\n\nAccess assignments\nFacilitates version control and collaboration\nAll work in MAT 427 course classroom"
  },
  {
    "objectID": "slides/00-welcome.html#prepare-participate-practice-perform",
    "href": "slides/00-welcome.html#prepare-participate-practice-perform",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Prepare, Participate, Practice, Perform",
    "text": "Prepare, Participate, Practice, Perform\n\n\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with team-based homework graded for completion\nPerform: Put together what you’ve learned to analyze real-world data\n\nTwo Job Applications/Portfolios (individual)\nTwo Job Interviews (individual)\nOne Hack-a-thon/Presentation (individual-ish)\nOne Project & Presentation (team)"
  },
  {
    "objectID": "slides/00-welcome.html#grading",
    "href": "slides/00-welcome.html#grading",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n10%\n\n\nJob Application 1\n15%\n\n\nJob Application 2\n15%\n\n\nJob Interview 1\n15%\n\n\nJob Interview 2\n15%\n\n\nHack-a-thon & Presentation\n15%\n\n\nFinal Project\n15%\n\n\n\nSee the syllabus for details on how the final letter grade will be calculated."
  },
  {
    "objectID": "slides/00-welcome.html#support",
    "href": "slides/00-welcome.html#support",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Support",
    "text": "Support\n\nAttend office hours\n\nProf. Friedlander office hours\n\nDedicated homework help session\nUse email for questions regarding personal matters and/or grades\nSee the Course Support page for more details"
  },
  {
    "objectID": "slides/00-welcome.html#late-homework",
    "href": "slides/00-welcome.html#late-homework",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Late Homework",
    "text": "Late Homework\n\nOne week late (no grade penalty)\nAfter I start grading (no feedback)\nWhy should I care about feedback?\n\nIt’s how you learn… duh\nYou will be repurposing your homeworks for your job applications"
  },
  {
    "objectID": "slides/00-welcome.html#school-sponsored-events",
    "href": "slides/00-welcome.html#school-sponsored-events",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "School-Sponsored Events",
    "text": "School-Sponsored Events\n\nExcused absences for event? Email me at least a week in advance\nSick or injured? Email me as soon as it is safe to do so.\n\nDon’t get me sick…\n\nFailure to adhere to this policy gets you a 35% point reduction"
  },
  {
    "objectID": "slides/00-welcome.html#academic-integrity",
    "href": "slides/00-welcome.html#academic-integrity",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nThe College of Idaho maintains that academic honesty and integrity are essential values in the educational process. Operating under an Honor Code philosophy, the College expects conduct rooted in honesty, integrity, and understanding, allowing members of a diverse student body to live together and interact and learn from one another in ways that protect both personal freedom and community standards. Violations of academic honesty are addressed primarily by the instructor and may be referred to the Student Judicial Board.\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the College of Idaho Honor Code."
  },
  {
    "objectID": "slides/00-welcome.html#collaboration-sharing-code",
    "href": "slides/00-welcome.html#collaboration-sharing-code",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Collaboration & sharing code",
    "text": "Collaboration & sharing code\n\nI have policies!"
  },
  {
    "objectID": "slides/00-welcome.html#use-of-artificial-intelligence-ai",
    "href": "slides/00-welcome.html#use-of-artificial-intelligence-ai",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n\nYou should treat AI tools, such as ChatGPT, the same as other online resources.\nThere are two guiding principles that govern how you can use AI in this course:1\n\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎"
  },
  {
    "objectID": "slides/00-welcome.html#use-of-artificial-intelligence-ai-1",
    "href": "slides/00-welcome.html#use-of-artificial-intelligence-ai-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n\nUnderstand everything you write down\nTell me where you got it from\nDon’t lie about it\n\n\n\n\n\n\n\nImportant\n\n\nIn general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content. Any code or content from your homework is eligible for inclusion during your job interview."
  },
  {
    "objectID": "slides/00-welcome.html#in-class-agreements",
    "href": "slides/00-welcome.html#in-class-agreements",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "In class agreements",
    "text": "In class agreements\nIf we discuss/agree to something in class or office hours which requires action from me (e.g. “you may turn in your homework late due to a sporting event”), you MUST send me a follow-up message. If you don’t, I will almost certainly forget, and our agreement will be considered null and void."
  },
  {
    "objectID": "slides/00-welcome.html#five-tips-for-success",
    "href": "slides/00-welcome.html#five-tips-for-success",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work (readings and videos) before class.\nAsk questions, come to office hours and help session.\nDo the homework; get started on homework early when possible.\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStay up-to-date on announcements on Canvas and sent via email."
  },
  {
    "objectID": "slides/00-welcome.html#emails-for-help",
    "href": "slides/00-welcome.html#emails-for-help",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Emails for help",
    "text": "Emails for help\nIf you email me about an error please include a screenshot of the error and the code causing the error."
  },
  {
    "objectID": "prepare/prep-07.html",
    "href": "prepare/prep-07.html",
    "title": "Preparation for Classification and Logistic Regression",
    "section": "",
    "text": "Read Chapter 4.1-4.3 from ISLR2\nRead Chapters 7 of TMWR"
  },
  {
    "objectID": "prepare/prep-07.html#assigned-readings-videos",
    "href": "prepare/prep-07.html#assigned-readings-videos",
    "title": "Preparation for Classification and Logistic Regression",
    "section": "",
    "text": "Read Chapter 4.1-4.3 from ISLR2\nRead Chapters 7 of TMWR"
  },
  {
    "objectID": "prepare/prep-05.html",
    "href": "prepare/prep-05.html",
    "title": "Preparation for Data Splitting and KNN",
    "section": "",
    "text": "Read Chapter 3.5-4.1 from ISLR2\nRead Chapters 9 of TMWR"
  },
  {
    "objectID": "prepare/prep-05.html#assigned-readings-videos",
    "href": "prepare/prep-05.html#assigned-readings-videos",
    "title": "Preparation for Data Splitting and KNN",
    "section": "",
    "text": "Read Chapter 3.5-4.1 from ISLR2\nRead Chapters 9 of TMWR"
  },
  {
    "objectID": "prepare/prep-03.html",
    "href": "prepare/prep-03.html",
    "title": "Preparation for Linear Regression & Gradient Descent",
    "section": "",
    "text": "Read Chapter 3.1 from ISLR2\nIf you haven’t taken Multivariable Calculus or don’t remember what a partial derivative or gradient is watch this\nRead this article on gradient descent"
  },
  {
    "objectID": "prepare/prep-03.html#assigned-readings-videos",
    "href": "prepare/prep-03.html#assigned-readings-videos",
    "title": "Preparation for Linear Regression & Gradient Descent",
    "section": "",
    "text": "Read Chapter 3.1 from ISLR2\nIf you haven’t taken Multivariable Calculus or don’t remember what a partial derivative or gradient is watch this\nRead this article on gradient descent"
  },
  {
    "objectID": "prepare/prep-01.html",
    "href": "prepare/prep-01.html",
    "title": "Preparation for Big Picture Discussion",
    "section": "",
    "text": "Review Syllabus\nRead Chapter 1 from ISLR2"
  },
  {
    "objectID": "prepare/prep-01.html#assigned-reading",
    "href": "prepare/prep-01.html#assigned-reading",
    "title": "Preparation for Big Picture Discussion",
    "section": "",
    "text": "Review Syllabus\nRead Chapter 1 from ISLR2"
  },
  {
    "objectID": "hw/02-hw-mlr.html",
    "href": "hw/02-hw-mlr.html",
    "title": "Homework 2: Intro to Regression",
    "section": "",
    "text": "In this homework, you will practice multiple linear regression by working with the data set Carseats from the package ISLR2. In addition, you will practice collaborating with a team over GitHub.\n\n\nBy the end of the homework, you will…\n\nBe able to collaborate with teammates on the same document using GitHub\nGain practice writing a reproducible report using Quarto\nFit and interpret linear models\nSplit data using tidymodels\nCompare and evaluate different linear models"
  },
  {
    "objectID": "hw/02-hw-mlr.html#learning-goals",
    "href": "hw/02-hw-mlr.html#learning-goals",
    "title": "Homework 2: Intro to Regression",
    "section": "",
    "text": "By the end of the homework, you will…\n\nBe able to collaborate with teammates on the same document using GitHub\nGain practice writing a reproducible report using Quarto\nFit and interpret linear models\nSplit data using tidymodels\nCompare and evaluate different linear models"
  },
  {
    "objectID": "hw/02-hw-mlr.html#teams",
    "href": "hw/02-hw-mlr.html#teams",
    "title": "Homework 2: Intro to Regression",
    "section": "Teams",
    "text": "Teams\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW2. Your group will consist of 2-3 people and has been randomly generated. But first, some rules:\n\nYou and your team members should be in the same physical room when you complete this assignment.\nYou should all contribute to each problem, but to receive credit you must rotate who actually writes down the answers.\nIn order to receive credit, you must have a commit after each exercise by the correct member of your team.\nFor now, don’t try to edit the same document at the same time. We will cover that in later homeworks."
  },
  {
    "objectID": "hw/02-hw-mlr.html#clone-the-repo-start-new-rstudio-project",
    "href": "hw/02-hw-mlr.html#clone-the-repo-start-new-rstudio-project",
    "title": "Homework 2: Intro to Regression",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\nThe following directions will guide you through the process of setting up your homework to work as a group."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-0.1",
    "href": "hw/02-hw-mlr.html#exercise-0.1",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 0.1",
    "text": "Exercise 0.1\n\n\n\n\n\n\nQuestion\n\n\n\nIn your group, decide on a team name. Then have one member of your group:\n\nClick this link to accept the assignment and enter your team name.\nRepeat the directions for creating a project from HW 1 with the HW 2 repository.\n\nOnce this is complete, the other two members can do the same thing, being careful to join the already created team on GitHub classroom."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-0.2",
    "href": "hw/02-hw-mlr.html#exercise-0.2",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 0.2",
    "text": "Exercise 0.2\n\n\n\n\n\n\nQuestion\n\n\n\nHave the first member of your group fill in the blanks below and then render, commit, and push the changes back to your GitHub repository.\n\n\n\nTeam Name: [Make one up and enter here]\nMember 1: [Insert Name]\nMember 2: [Insert Name]\nMember 3: [Insert Name/Delete line if you only have two members]"
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-1",
    "href": "hw/02-hw-mlr.html#exercise-1",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nAll members of your group should install the ISLR2 package if they haven’t. Remember that you only need to do this once on your computer.\nHave the second member of your group pull the changes made by member 1. Before completing this question.\nLoad the ISLR2 package. You should be able to access the dataset Carseats. Based on your knowledge of the world, which features do you think will be most predictive of Sales. Hint: ?Carseats will give you more information on the data set and the variables.\nOnce you have completed this, have the second member render, commit, and push the changes to GitHub."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-2",
    "href": "hw/02-hw-mlr.html#exercise-2",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nHave the second member of your group pull the changes made by member 1 and member 2. Before completing this question.\nUse tidymodels to create a training and test set from the Carseats data using a 70-30 split. Note that this is a random process so you will get different partitions every time you split your data. As a result, it is considered good practice to set your seed so that the results a reproducible. For this homework please use the seed 427. For the training set, what quantitative variable is most highly correlated with Sales, our target variable?\nOnce you have completed this, have the third member render, commit, and push the changes to GitHub."
  },
  {
    "objectID": "hw/02-hw-mlr.html#committing-changes",
    "href": "hw/02-hw-mlr.html#committing-changes",
    "title": "Homework 2: Intro to Regression",
    "section": "Committing Changes",
    "text": "Committing Changes\n\nYou should continue in this manner, rotating who completes each question in order from member 1 to member 2 to member 3 and back to member 1 for the remainder of the assignment. In order to receive credit, you must have a commit after each exercise by the correct member of your team."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-3",
    "href": "hw/02-hw-mlr.html#exercise-3",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nFit a linear regression model predicting Sales using the variable you identified in Exercise 2. Write down the resulting model in the form: \\[Price = \\beta_0 + \\beta_1\\times Variable\\] Don’t forget to use your training set rather than the full data to train your model."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-4-hard",
    "href": "hw/02-hw-mlr.html#exercise-4-hard",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 4 (Hard)",
    "text": "Exercise 4 (Hard)\n\n\n\n\n\n\nQuestion\n\n\n\nRe-estimate the parameters (i.e. the intercept and slope) for the model above using gradient descent. Your estimates should be similar to those in Exercise 3 but likely won’t be EXACTLY the same. Your solution should include the following:\n\nInitialize the values of your \\(\\beta\\)’s.\nInitialize the step size.\nInitialize your tolerance (i.e. the stopping criteria).\nEnter a loop. For each iteration in the loop:\n\nCompute the partial derivatives for \\(\\beta_0\\) and \\(\\beta_1\\).\nUpdate the values of \\(\\beta_0\\) and \\(\\beta_1\\).\nCheck to see if the change in your estimates is smaller than your tolerance. If it is, exit the loop, otherwise continue."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-5",
    "href": "hw/02-hw-mlr.html#exercise-5",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nCompute the RMSE for your baseline model on both the training and test set and report them. Try and report your results inline.\n\n\nThe second primary metric that we can use to assess the accuracy of regression models is called the coefficient of determination, denoted \\(R^2\\). \\(R^2\\) is the proportion of variance (information) in our target variable that is explained by our model and can be computed by squaring \\(R\\), the correlation coefficient between the target variable \\(y\\) and the predicted target \\(\\hat{y}\\). The lm function actually computes the \\(R^2\\) of our training data for us which we can access using the glance function from the broom package which is included in tidymodels so you don’t need to load it."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-6",
    "href": "hw/02-hw-mlr.html#exercise-6",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nWhat proportion of the variation in Sales is explained by our baseline model for the training and validation sets?"
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-7",
    "href": "hw/02-hw-mlr.html#exercise-7",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nUsing tidymodels, build a two-input linear model for Sales by adding US in addition to the variable you selected above. Save your model as lmfit1. Use tidy and kable to output the model. Is the coefficient for Price the same or different than it was in our baseline model?\n\n\nNotice that the only coefficient added for US is called USYes. When you build a linear model with a categorical variable, R will introduce dummy variables which encode each category as a vector of 0’s and 1’s. In data science, this is sometimes called one-hot encoding. One level is always lumped into the intercept coefficient and is called the reference level. In this case, the reference level is No. When including a categorical variable in a linear model, you can interpret the resulting line being shifted up or down based on the category of a given observation.\nLet’s now assess the accuracy of our new model. To make computation of RMSE and \\(R^2\\) easier let’s take advantage of the rmse and r2 functions in the yardstick package (also included in tidymodels)."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-8",
    "href": "hw/02-hw-mlr.html#exercise-8",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nUse the rmse and rsq functions from theyardstick package to compute the RMSE and \\(R^2\\) values for this new model on both the training and validation sets. How do these compare to the baseline model?"
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-9",
    "href": "hw/02-hw-mlr.html#exercise-9",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nFit a model using all of the predictors in your training data. Call the model lmfull. Assess the model’s accuracy on the training data and the test data, comparing it to the previous models we’ve fit, and comment on your results.\n\n\nYou should notice that while the accuracy metrics on the training data drastically improve, there is little to no difference in the metrics for the test set. This is because of a phenomenon known as overfitting. Overfitting occurs when your model starts matching the training TOO well. A good visualization of an overfit model is Figure 2 in the Wikipedia article for overfitting. As you include more variables/information in your model, your performance will ALWAYS increase on your training data. This is one of the reasons we always use holdout sets. Eventually your model will begin to over-align to the noise in your training data and the accuracy on holdout sets will be level off and in most cases begin to degrade.\nWhen modeling there are two related trade-offs that you need to consider. The first is the trade-off between prediction accuracy and interpretability. In general, one can typically create models with better prediction accuracy by sacrificing interpretability (e.g. by including more variables in your model, transforming these variables, etc.). Another trade-off is something called the bias-variance trade-off. As we increase the complexity of a model, we allow it to account for more and more intricate patterns in our data. In theory, this allows the model to mimic more complex relationships between our predictors and our target variables, reducing bias. On the other hand, more complex models typically have more parameters which need to be estimated which require more data to estimate accurately. When you increase the complexity of a model you usually increase the variance of the estimates of model parameters and the predictions the model makes. In other words, the model will be much more sensitive to the noise in the data that you have. Bias and variance will both decrease the accuracy of your model so you should try to minimize both. However, past a certain point, it will be a trade-off between the two."
  },
  {
    "objectID": "hw/02-hw-mlr.html#exercise-10",
    "href": "hw/02-hw-mlr.html#exercise-10",
    "title": "Homework 2: Intro to Regression",
    "section": "Exercise 10",
    "text": "Exercise 10\n\n\n\n\n\n\nQuestion\n\n\n\nFind and fit a model on the training data which outperforms all the models we fit so far when evaluated on the test set. Briefly summarize your results. The “best” model will receive a high-five from Dr. Friedlander. Feel free to use any techniques we’ve learned in this class, up to this point. I encourage you to try out different data transformations like polynomials."
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#books-free-online",
    "href": "computing-r-resources.html#books-free-online",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#rstudio-primers",
    "href": "computing-r-resources.html#rstudio-primers",
    "title": "Resources for learning R",
    "section": "RStudio Primers",
    "text": "RStudio Primers\n\nInteractive LearnR Tutorial\nAnother set of tutorials\nR Primers for full list of tutorials. Note that we will be using a simplified version of ggplot2 in this course.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nRStudio Cheatsheets\nR Fun workshops and videos by Duke Center for Data and Visualization Sciences.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "Not that we will go through this as a call and on your first homework. Feel free to revisit it here if you need to.",
    "crumbs": [
      "Computing",
      "Computing access"
    ]
  },
  {
    "objectID": "computing-access.html#r-rstudio",
    "href": "computing-access.html#r-rstudio",
    "title": "Computing access",
    "section": "R & RStudio",
    "text": "R & RStudio\nIt is highly recommended that you install R and RStudio on your own personal computer. Follow the directions here to install R and RStudio on your computer\nIf, for some reason, you are unable to use R or RStudio on your personal computer, you may use the College of Idaho’s RStudio Server. However, I do not recommend you do this as I want you to practice installing packages in this course and you do not have the permissions to do that on the server. If you must use the RStudio Server, please notify Dr. Friedlander immediately.",
    "crumbs": [
      "Computing",
      "Computing access"
    ]
  },
  {
    "objectID": "computing-access.html#git-github",
    "href": "computing-access.html#git-github",
    "title": "Computing access",
    "section": "Git & Github",
    "text": "Git & Github\nYou need to create a GitHub account. You may want to use this to show-off work to future employers so I recommend using something professional (like your name) as your user name. Once you have done this, email it to Dr. Friedlander so he can add you to the GitHub Classroom.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system.\n\n\n\nStep 0: Type install.packages(\"credentials\") into your console.\nStep 1: Type credentials::ssh_setup_github() into your console.\nStep 2: R will ask “No SSH key found. Generate one now?” Click 1 for yes.\nStep 3: You will generate a key. It will begin with “ssh-rsa….” R will then ask “Would you like to open a browser now?” Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used, e.g., mat427)\n\nYou can find more detailed instructions here if you’re interested.\n\n\nConfigure git\nThere is one more thing we need to do before getting started on the assignment. Specifically, we need to configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package. You will need to install the usethis package in the same way you installed the credentials packages above.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"EricFriedlander\",\n  user.email = \"efriedlander@collegeofidaho.edu\")\n\nYou are now ready interact between GitHub and RStudio!",
    "crumbs": [
      "Computing",
      "Computing access"
    ]
  },
  {
    "objectID": "hw/01-hw-eda.html",
    "href": "hw/01-hw-eda.html",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "",
    "text": "Adapted from “Start teaching with R,” created by R Pruim, N J Horton, and D Kaplan, 2013, “Interactive and Dynamic Graphics for Data Analysis,” by Dianne Cook and Deborah F. Swayne, Colby Long’s DATA 325 Course at Wooster College and Maria Tackett’s STA-210 course at Duke University."
  },
  {
    "objectID": "hw/01-hw-eda.html#introduction",
    "href": "hw/01-hw-eda.html#introduction",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Introduction",
    "text": "Introduction\nIn this homework we will familiarize ourselves with the tools that we’ll use throughout the course and refresh ourselves on topic related to exploratory data analysis."
  },
  {
    "objectID": "hw/01-hw-eda.html#course-toolkit",
    "href": "hw/01-hw-eda.html#course-toolkit",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Course Toolkit",
    "text": "Course Toolkit\nThe primary tools we’ll be using in this course are R, RStudio, git, and GitHub. We will be using them throughout the course both to learn the concepts discussed in the course and to analyze real data and come to informed conclusions.\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like “Track Changes” features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like Dropbox but much better for code).\n\n\nTo make versioning simpler, this homework will be completed individually. In the future, you’ll learn about collaborating on GitHub and producing a single homework for your team, but for now, concentrate on getting the basics down."
  },
  {
    "objectID": "hw/01-hw-eda.html#exploratory-data-analysis",
    "href": "hw/01-hw-eda.html#exploratory-data-analysis",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nOne of the most important components of data science is exploratory data analysis. I really like the following definition, which comes from this article (though it’s probably not the original source).\n\nExploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns, spot anomalies, to test hypotheses and to check assumptions with the help of summary statistics and graphical representations.\n\nBefore you begin your exploratory analysis, you may already have a particular question in mind. For example, you might work for an online retailer and want to develop a model to predict which purchased items will be returned. Or, you may not have a particular question in mind. Instead, you might just be asked to look at browsing data for several customers and figure out some way to increase purchases. In either case, before you construct a fancy model, you need to explore and understand your data. This is how you gain new insights and determine if an idea is worth pursuing."
  },
  {
    "objectID": "hw/01-hw-eda.html#learning-goals",
    "href": "hw/01-hw-eda.html#learning-goals",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the homework, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to create numerical and visual summaries of data\nUse those summaries"
  },
  {
    "objectID": "hw/01-hw-eda.html#clone-the-repo-start-new-rstudio-project",
    "href": "hw/01-hw-eda.html#clone-the-repo-start-new-rstudio-project",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/mat427sp25 organization on GitHub. Click on the repo with the prefix hw-01-. It contains the starter documents you need to complete the lab.\n\nIf you do not see your hw-01 repo, click here to create your repo.\n\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick 01-hw-eda.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "hw/01-hw-eda.html#r-and-r-studio",
    "href": "hw/01-hw-eda.html#r-and-r-studio",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It is rumored that it stood for “Yet Another Markup Language” but it officially stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments I will nudge you when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "hw/01-hw-eda.html#understanding-your-data",
    "href": "hw/01-hw-eda.html#understanding-your-data",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Understanding your data",
    "text": "Understanding your data\nToday we will be working with the TIPS data set which is in the regclass package. The data in the TIPS dataset is information recorded by one waiter about each tip he received over a period of a few months working in a restaurant. We would like to use this data to address the question, “What factors affect tipping behavior?”\n\nExercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nInstall the regclass package by either typing install.packages(\"regclass\") in the console or by clicking “Tools &gt; Install Packages” and selecting the package. Once you have done this, the code chunk below will load the package and data set. Notice that a bunch of unnecessary output is included when you knit the document. Change the Quarto chunk options so that this is not displayed.\n\n\n\nlibrary(regclass)\n\nLoading required package: bestglm\n\n\nLoading required package: leaps\n\n\nLoading required package: VGAM\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\nLoading required package: rpart\n\n\nLoading required package: randomForest\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nImportant regclass change from 1.3:\nAll functions that had a . in the name now have an _\nall.correlations -&gt; all_correlations, cor.demo -&gt; cor_demo, etc.\n\ndata(\"TIPS\")\n\nWhen exploring a new data set, it’s important to first understand the basics. What format is our data in? What types of information are included in the data set? How many observations are there?\n\n\nExercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nIn R, data sets are usually stored in a 2-dimensional structure called a data frame. The tidyverse provides a lot of useful functions for a variety of applications including data exploration and the particular flavor of data frame that the tidyverse uses is called a tibble. After loading the tidyverse library, you can get an idea of the structure of a data set using the syntax str(dataset) or glimpse(data), and you can peak at the first few rows and columns with head(dataset). Create a code chunk below, and use these functions (and others) in the R chunk below to better understand the data. How many tips are recorded in this data set? Which days of the week did the waiter work?\n\n\nOften, a data set will come with a code book which gives more complete information about the structure of the data, the meaning of variables, and how the data were collected. In this case, most of the column names are pretty self explanatory.\n\n\n\nVariable\nDescription\n\n\n\n\nTipPercentage\nthe gratuity, as a percentage of the bill\n\n\nBill\nthe cost of the meal in US dollars\n\n\nTip\nthe tip in US dollars\n\n\nGender\ngender of the bill payer\n\n\nSmoker\nwhether the party included smokers\n\n\nWeekday\nday of the week\n\n\nTime\ntime the bill was paid\n\n\nPartySize\nsize of the party\n\n\n\n\n\nExercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nEven though the column names are self-explanatory, we might have more questions about the data. For example, we might conjecture that people tip differently for breakfast and lunch, but our data only tells us if the bill was paid at “Day” or “Night.” State another reasonable conjecture about a factor that might affect tipping behavior. What additional information would be helpful to explore that conjecture?\n\n\n\n\n\n\n\n\nRender-Commit-Push\n\n\n\nThis is a good place to render, commit, and push changes to your hw-eda repo on GitHub. Write an informative commit message (e.g. “Completed exercises 1 - 3”), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "hw/01-hw-eda.html#graphical-summaries",
    "href": "hw/01-hw-eda.html#graphical-summaries",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Graphical Summaries",
    "text": "Graphical Summaries\nGraphical summaries are a key tool in exploratory data analysis to to help you understand your data. They also help you communicate insights about your data to others. For example, we might want to display relationships about some of our categorical variables. So we could start by graphing different party sizes in our data set.\n\nTIPS |&gt; \n  ggplot(aes(x = PartySize)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nOr we could explore the question about the percentage of tables that are smokers on different days of the week visually.\n\nTIPS |&gt; \n  ggplot(aes(x = Weekday, fill = Smoker)) +\n    geom_bar()\n\n\n\n\n\n\n\nTIPS |&gt; \n  ggplot(aes(x = Weekday, fill = Smoker)) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nWe might summarize a numerical variable with a histogram. For example, here is a histogram of all of the tips in the data set.\n\nTIPS |&gt; \n  ggplot(aes(x = Tip)) +\n    geom_histogram(bins = 100)\n\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nNotice that there are a few “spikes” in the histogram above. What do you think is causing this?\n\n\nWe can also summarize this numerical data broken down by one of the categorical variables using boxplots, violin plots, or sina plots. Note that to create sina plots we need the ggforce package.\n\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_boxplot() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_violin() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\nlibrary(ggforce)\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_sina() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\n\nOr we can visualize the relationship between a lot of our numerical variables at once.\n\n# Using pairs (only numerical allowed)\npairs(~ Bill + TipPercentage + Tip\n    , data = TIPS\n    , main=\"Scatterplot Matrix for TIPS\")\n\n\n\n\n\n\n\n# Using ggpairs from GGally package (preferable even though more syntax)\nlibrary(GGally)\nTIPS |&gt; \n  select(Bill, TipPercentage, Tip, Weekday) |&gt; \n  ggpairs()\n\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nAre there any clear linear relationships in the scatterplots above? What do you think is the explanation for these relationships?\n\n\nThere are lots of other interesting graphical summaries available for interpreting and displaying data. In addition, there are lots of R packages that allow you to draw these graphics and to further customize some of the ones we discussed here. In your projects, you are welcome to use any of these that you think are appropriate.\n\n\n\n\n\n\nRender-Commit-Push\n\n\n\nThis is a good place to render, commit, and push changes to your hw-eda repo on GitHub. Write an informative commit message (e.g. “Completed exercises 7 - 8”), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\n\nExercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nState a reasonable conjecture about tipping behavior that you would like to explore in the data set. For example, you might think that people on dates tip more or that the waiter gets smaller tips when he has too many tables. Give at least one numerical and one graphical summary to explore this conjecture. Is there any evidence to support your conjecture?\nIt’s okay if your conjecture is not supported or if you are just wrong–that’s often the case in exploratory data analysis.\n\n\n\n\n\n\n\n\nRender-Commit-Push\n\n\n\nThis is a good place to render, commit, and push changes to your hw-eda repo on GitHub. Write an informative commit message (e.g. “Completed exercises 9”), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nBefore you’re done with you work, make sure you look it over one last time to make sure the rendered document looks like you want it to! I can’t tell you how often students turn in work and their output doesn’t match their prose or the output definitely doesn’t look the way they wanted it to."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MAT 427: Statistical Machine Learning",
    "section": "",
    "text": "A study of methods to construct and evaluate predictive models. Topics may include lasso and ridge regression, naive Bayes, random forests, support vector machines, gradient boosting, and neural networks. The course will require a significant data analysis and modeling project."
  },
  {
    "objectID": "index.html#catalog-description",
    "href": "index.html#catalog-description",
    "title": "MAT 427: Statistical Machine Learning",
    "section": "",
    "text": "A study of methods to construct and evaluate predictive models. Topics may include lasso and ridge regression, naive Bayes, random forests, support vector machines, gradient boosting, and neural networks. The course will require a significant data analysis and modeling project."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "MAT 427: Statistical Machine Learning",
    "section": "Instructor",
    "text": "Instructor\nEric is an Assistant Professor in the Department of Mathematics and Physical Sciences at the College of Idaho. His expertise lies in the areas of probability, statistics, and statistics education. He joined the College of Idaho faculty in 2024 after spending three years as an Assistant Professor at St. Norbert College in De Pere, Wisconsin. He received his bachelor’s degree in mathematics and statistics from Rice University in 2011. After earning his degree, he worked for Capital One for two years in their home loans division before enrolling in graduate school. In 2018, Eric received his Ph.D. in statistics and operations research from the University of North Carolina at Chapel Hill studying under professor Amarjit Budhiraja. His dissertation work focused on modeling and analyzing large systems which arise in industrial engineering (e.g. large server and communication networks). Following his Ph.D., Eric did a postdoc in the Department of Ecology & Evolution at the University of Chicago under the direction of professor Matthias Steinrücken where he used stochastic processes to study natural selection and population genetics.\nOutside of school, Eric is an avid fan of the New York Giants, New York Knicks, and North Carolina Tarheels. In addition, he enjoys comic books, the Fast and the Furious franchise, and spending time with his lovely wife Maria and lovable dogs Allie, Tony, and Miriam."
  },
  {
    "objectID": "prepare/prep-02.html",
    "href": "prepare/prep-02.html",
    "title": "Preparation for Introduction to Statistical Learning",
    "section": "",
    "text": "Read Chapter 2 from ISLR2"
  },
  {
    "objectID": "prepare/prep-02.html#assigned-reading",
    "href": "prepare/prep-02.html#assigned-reading",
    "title": "Preparation for Introduction to Statistical Learning",
    "section": "",
    "text": "Read Chapter 2 from ISLR2"
  },
  {
    "objectID": "prepare/prep-04.html",
    "href": "prepare/prep-04.html",
    "title": "Preparation for Multiple Linear Regression & Data Splitting",
    "section": "",
    "text": "Read Chapter 3.2, 3.3, and (optionally) 3.4 from ISLR2\nIf you you don’t have experience with the tidyverse please read Chapter 2 of TMWR\nRead Chapters 3-6 of TMWR"
  },
  {
    "objectID": "prepare/prep-04.html#assigned-readings-videos",
    "href": "prepare/prep-04.html#assigned-readings-videos",
    "title": "Preparation for Multiple Linear Regression & Data Splitting",
    "section": "",
    "text": "Read Chapter 3.2, 3.3, and (optionally) 3.4 from ISLR2\nIf you you don’t have experience with the tidyverse please read Chapter 2 of TMWR\nRead Chapters 3-6 of TMWR"
  },
  {
    "objectID": "prepare/prep-06.html",
    "href": "prepare/prep-06.html",
    "title": "Preparation for KNN and Preprocessing",
    "section": "",
    "text": "Read Chapters 8 of TMWR"
  },
  {
    "objectID": "prepare/prep-06.html#assigned-readings-videos",
    "href": "prepare/prep-06.html#assigned-readings-videos",
    "title": "Preparation for KNN and Preprocessing",
    "section": "",
    "text": "Read Chapters 8 of TMWR"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlecture\ndow\ndate\nweek\ntopic\nprepare\nslides\nhw\nproject\nnotes\n\n\n\n\n0\nM\nFeb 3\n1\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nW\nFeb 5\n1\nBig Picture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1 Assigned\n\n\n2\nF\nFeb 7\n1\nWhat is Statistical Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\nM\nFeb 10\n2\nIntro to Linear Regression and Gradient Descent\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\nW\nFeb 12\n2\nMultiple Linear Regression and Data Splitting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1 Due/ HW 2 Assigned\n\n\n5\nF\nFeb 14\n2\nData Splitting and KNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\nM\nFeb 17\n3\nKNN + Preprocessing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\nW\nFeb 19\n3\nClassification and Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 2 Due/ HW 3 Assigned",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "slides/01-big-picture.html#five-tips-for-success",
    "href": "slides/01-big-picture.html#five-tips-for-success",
    "title": "MATH 427: The Big Picture",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work (readings and videos) before class.\nAsk questions, come to office hours and help session.\nDo the homework; get started on homework early when possible.\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStay up-to-date on announcements on Canvas and sent via email."
  },
  {
    "objectID": "slides/01-big-picture.html#emails-for-help",
    "href": "slides/01-big-picture.html#emails-for-help",
    "title": "MATH 427: The Big Picture",
    "section": "Emails for help",
    "text": "Emails for help\nIf you email me about an error please include a screenshot of the error and the code causing the error."
  },
  {
    "objectID": "slides/01-big-picture.html#what-is-machine-learning",
    "href": "slides/01-big-picture.html#what-is-machine-learning",
    "title": "MATH 427: The Big Picture",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nMachine Learning is the study of tools/techniques for extracting information and making predictions from complex datasets\nThe name machine learning was coined in 1959 by Arthur Samuel\n\n“Field of study that gives computers the ability to learn without being explicitly programmed”"
  },
  {
    "objectID": "slides/01-big-picture.html#what-is-machine-learning-1",
    "href": "slides/01-big-picture.html#what-is-machine-learning-1",
    "title": "MATH 427: The Big Picture",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\nTom M. Mitchell (1998):\n\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
  },
  {
    "objectID": "slides/01-big-picture.html#what-is-machine-learning-2",
    "href": "slides/01-big-picture.html#what-is-machine-learning-2",
    "title": "MATH 427: The Big Picture",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nMNIST handwritten digits (from ISLR, James et al.)"
  },
  {
    "objectID": "slides/01-big-picture.html#question",
    "href": "slides/01-big-picture.html#question",
    "title": "MATH 427: The Big Picture",
    "section": "Question!!!",
    "text": "Question!!!\nSuppose your email program watches which emails you do or do not mark as spam, and based on that learns how to better filter spam. According to Tom Mitchell’s definition, which of the following is the task T, experience E, and performance measure P in this setting?\n\nP The number (or fraction) of emails correctly classified as spam/ham (not spam)\nT Classifying emails as spam or ham\nE Watching you label emails as spam or ham\n\n\n\n\n−+\n01:00"
  },
  {
    "objectID": "slides/01-big-picture.html#statistical-learning-vs-machine-learning-vs-data-science",
    "href": "slides/01-big-picture.html#statistical-learning-vs-machine-learning-vs-data-science",
    "title": "MATH 427: The Big Picture",
    "section": "Statistical Learning vs Machine Learning vs Data Science",
    "text": "Statistical Learning vs Machine Learning vs Data Science\n\nMachine learning arose as a sub-field of Artificial Intelligence which is a sub-fields of Computer Science\nStatistical learning arose as a sub-field of Statistics\nThere is much overlap, a great deal of “cross-fertilization”\n“Data Science” - Reflects the fact that both statistical and machine learning are about data\n“Machine Learning” or “Data Science” are “fancier” terms"
  },
  {
    "objectID": "slides/01-big-picture.html#statistics-vs-machine-learning",
    "href": "slides/01-big-picture.html#statistics-vs-machine-learning",
    "title": "MATH 427: The Big Picture",
    "section": "Statistics vs Machine Learning",
    "text": "Statistics vs Machine Learning\n\nStatistics: more concerned with answering why and how things work, making inferences\nMachine/Statistical learning: more concerned with making predictions"
  },
  {
    "objectID": "slides/01-big-picture.html#terminologynotation",
    "href": "slides/01-big-picture.html#terminologynotation",
    "title": "MATH 427: The Big Picture",
    "section": "Terminology/Notation",
    "text": "Terminology/Notation\nAmes Housing dataset - Contains data on 881 houses in Ames, IA. We are interested in predicting sale price.\nThe first ten observations are shown below.\n\n\n\n\n\nSale_Price\nGr_Liv_Area\nGarage_Type\nGarage_Cars\nGarage_Area\nStreet\nUtilities\nPool_Area\nNeighborhood\n\n\n\n\n244000\n2110\nAttchd\n2\n522\nPave\nAllPub\n0\nNorth_Ames\n\n\n213500\n1338\nAttchd\n2\n582\nPave\nAllPub\n0\nStone_Brook\n\n\n185000\n1187\nAttchd\n2\n420\nPave\nAllPub\n0\nGilbert\n\n\n394432\n1856\nAttchd\n3\n834\nPave\nAllPub\n0\nStone_Brook\n\n\n190000\n1844\nAttchd\n2\n546\nPave\nAllPub\n0\nNorthwest_Ames\n\n\n149000\nNA\nAttchd\n2\n480\nPave\nAllPub\n0\nNorth_Ames\n\n\n149900\nNA\nAttchd\n2\n500\nPave\nAllPub\n0\nNorth_Ames\n\n\n127500\n1069\nAttchd\n2\n440\nPave\nAllPub\n0\nNorthpark_Villa\n\n\n395192\n1940\nAttchd\n3\n606\nPave\nAllPub\n0\nNorthridge_Heights\n\n\n290941\n1544\nAttchd\n3\n868\nPave\nAllPub\n0\nNorthridge_Heights"
  },
  {
    "objectID": "slides/01-big-picture.html#terminologynotation-1",
    "href": "slides/01-big-picture.html#terminologynotation-1",
    "title": "MATH 427: The Big Picture",
    "section": "Terminology/Notation",
    "text": "Terminology/Notation\nDefault dataset - Contains credit card default data on 10,000 individuals. We are interested in predicting whether somebody will default or not.\nTen observations are shown below.\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n939.0985\n45519.02\n\n\nNo\nYes\n397.5425\n22710.87\n\n\nYes\nNo\n1511.6110\n53506.94\n\n\nNo\nNo\n301.3194\n51539.95\n\n\nNo\nNo\n878.4461\n29561.78\n\n\nYes\nNo\n1673.4863\n49310.33\n\n\nNo\nNo\n310.1302\n37697.22\n\n\nNo\nNo\n1272.0539\n44895.59\n\n\nNo\nNo\n887.2014\n41641.45\n\n\nNo\nNo\n230.8689\n32798.78"
  },
  {
    "objectID": "slides/01-big-picture.html#terminologynotation-2",
    "href": "slides/01-big-picture.html#terminologynotation-2",
    "title": "MATH 427: The Big Picture",
    "section": "Terminology/Notation",
    "text": "Terminology/Notation\n\nResponse/Target/Outcome - variable we are interested in predicting, denoted as \\(Y\\)\nFeatures/Inputs/Predictors - variables used to predict the response, denoted as \\(X\\)\nFeature Matrix - all features taken together, denoted as \\(\\mathbf{X}\\)\nNumber of data points/observations denoted as \\(n\\)\nNumber of features/inputs/predictors denotes as \\(p\\)\nMissing entries in R are denoted as NA"
  },
  {
    "objectID": "slides/01-big-picture.html#question-1",
    "href": "slides/01-big-picture.html#question-1",
    "title": "MATH 427: The Big Picture",
    "section": "Question!!!",
    "text": "Question!!!\nFor the Ames Housing and Default datasets:\n\nQuestionsAnswers\n\n\n\nWhat are the corresponding values of \\(n\\) and \\(p\\)?\nWhat will be the dimension of the corresponding response vector \\(Y\\)?\nWhat is the value of the 3rd feature for the 2nd observation?\n\n\n\n\nWhat are the corresponding values of \\(n\\) and \\(p\\)?\n\nAmes: \\(n = 881\\) and \\(p = 9\\)\nDefault: \\(n = 10000\\) and \\(p = 4\\)\n\nWhat will be the dimension of the corresponding response vector \\(Y\\)?\n\nAmes: \\(881\\times 1\\)\nDefault: \\(10000\\times 1\\)\n\nWhat is the value of the 3rd feature for the 2nd observation?\n\nAmes: Attchd\nDefault: 397.5425"
  },
  {
    "objectID": "slides/01-big-picture.html#question-2",
    "href": "slides/01-big-picture.html#question-2",
    "title": "MATH 427: The Big Picture",
    "section": "Question!!!",
    "text": "Question!!!\nSuppose you have information about 867 cancer patients on their age, tumor size, clump thickness of the tumor, uniformity of cell size, and whether the tumor is malignant or benign. Based on these data, you are interested in building a model to predict the type of tumor (malignant or benign) for future cancer patients.\n\nWhat are the values of \\(n\\) and \\(p\\) in this dataset? \\(n = 867, p = 5\\)\nWhat are the inputs/features?"
  },
  {
    "objectID": "slides/01-big-picture.html#supervised-vs-unsupervised-learning",
    "href": "slides/01-big-picture.html#supervised-vs-unsupervised-learning",
    "title": "MATH 427: The Big Picture",
    "section": "Supervised vs Unsupervised Learning",
    "text": "Supervised vs Unsupervised Learning\n\nMachine Learning Tasks (from Bunker and Fayez, 2017)"
  },
  {
    "objectID": "slides/01-big-picture.html#supervised-learning",
    "href": "slides/01-big-picture.html#supervised-learning",
    "title": "MATH 427: The Big Picture",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nWe have access to labeled data\nObjective: learn overall pattern of relationship between the inputs (\\(\\mathbf{X}\\)) and response (\\(Y\\)) in order to\n\nInvestigate the relationship between inputs and response\nPredict for potential unseen test cases\nAssess the quality of predictions"
  },
  {
    "objectID": "slides/01-big-picture.html#types-of-supervised-learning",
    "href": "slides/01-big-picture.html#types-of-supervised-learning",
    "title": "MATH 427: The Big Picture",
    "section": "Types of Supervised Learning",
    "text": "Types of Supervised Learning\nSupervised Learning problems can be categorized into:\n\n\nRegression problems (response is quantitative, continuous)\nClassification problems (response is qualitative, categorical)\nWhat if I have discrete quantitative data (e.g. counts)?\n\nCan use either but one is probably better…\nHow many discrete values are there?\n\nEnough we can think of the response as continuous?\n\nWhat is our goal?\n\nHow important is being exactly correct?"
  },
  {
    "objectID": "slides/01-big-picture.html#unsupervised-learning",
    "href": "slides/01-big-picture.html#unsupervised-learning",
    "title": "MATH 427: The Big Picture",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nNo response/outcome variable, just \\(\\mathbf{X}\\)\nUnderstand structure within data\n\nfind similar groups of observations based on features (clustering)\nfind a smaller subset of features with the most variation (dimensionality reduction)\n\nNo gold-standard\nEasier to collect unlabeled data\nUseful pre-processing step for supervised learning"
  },
  {
    "objectID": "slides/01-big-picture.html#unsupervised-learning-1",
    "href": "slides/01-big-picture.html#unsupervised-learning-1",
    "title": "MATH 427: The Big Picture",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nUS Arrests dataset - Data on arrests for 50 US states.\nThe first ten observations are shown below.\n\n\n\n\n\n\nMurder\nAssault\nUrbanPop\nRape\n\n\n\n\nAlabama\n13.2\n236\n58\n21.2\n\n\nAlaska\n10.0\n263\n48\n44.5\n\n\nArizona\n8.1\n294\n80\n31.0\n\n\nArkansas\n8.8\n190\n50\n19.5\n\n\nCalifornia\n9.0\n276\n91\n40.6\n\n\nColorado\n7.9\n204\n78\n38.7\n\n\nConnecticut\n3.3\n110\n77\n11.1\n\n\nDelaware\n5.9\n238\n72\n15.8\n\n\nFlorida\n15.4\n335\n80\n31.9\n\n\nGeorgia\n17.4\n211\n60\n25.8"
  },
  {
    "objectID": "slides/01-big-picture.html#question-3",
    "href": "slides/01-big-picture.html#question-3",
    "title": "MATH 427: The Big Picture",
    "section": "Question!!!",
    "text": "Question!!!\n\nFor each of the following, identify whether the problem belongs to the supervised or unsupervised learning paradigm\n\nExamine the statistics of two football teams, and predict which team will win tomorrow’s match (given historical data of teams’ wins/losses to learn from) supervised\nGiven genetic (DNA) data from a person, predict the probability of the person developing diabetes over the next 10 years supervised\nTake a collection of 1000 essays written on the US economy, and find a way to automatically group these essays into a small number of groups of essays that are somehow “similar” or “related” unsupervised\nExamine data on the income and years of education of adults in a neighborhood and build a model to predict the income from years of education supervised"
  },
  {
    "objectID": "slides/01-big-picture.html#recap",
    "href": "slides/01-big-picture.html#recap",
    "title": "MATH 427: The Big Picture",
    "section": "Recap",
    "text": "Recap\n\nWhat is a Machine Learning algorithm?\n\nT: has task\nP: performance is measured\nE: improves with experience\n\nTerminology:\n\nFeatures\n\n\\(p\\)\n\nTarget\nObservations\n\n\\(n\\)\n\n\nSupervised vs. unsupervised learning\n\nSupervised: data is labeled\nUnsupervised: data is unlabeled\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#concept-check",
    "href": "slides/03-SLR-Gradient_Desc.html#concept-check",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Concept Check",
    "text": "Concept Check\nWhat do we call these:\n\n\\(\\mathbf{X}\\)\n\\(Y\\)\n\nWhat is our goal in supervised learning?"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#concept-check-1",
    "href": "slides/03-SLR-Gradient_Desc.html#concept-check-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Concept Check",
    "text": "Concept Check\n\nFeatures: \\(\\mathbf{X}\\)\nTarget: \\(Y\\)\nGoal: Predict \\(Y\\) using \\(\\mathbf{X}\\)\n\n\n\nWhat do we call this process if \\(Y\\) is numerical? What about categorical?"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#concept-check-2",
    "href": "slides/03-SLR-Gradient_Desc.html#concept-check-2",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat do we call this process if \\(Y\\) is numerical? What about categorical?\n\nNumerical: regression\nCategorical: classification\n\n\n\n\nWhat is the difference between a training and a test set?\nHow do we evaluate the performance of a regression model?"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#supervised-learning-assessing-model-performance",
    "href": "slides/03-SLR-Gradient_Desc.html#supervised-learning-assessing-model-performance",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\n\nLabeled training data \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\)\n\ni.e. \\(n\\) training observations\n\nFit/train a model from training data\n\n\\(\\hat{y}=\\hat{f}(x)\\), regression\n\\(\\hat{y}=\\hat{C}(x)\\), classification\n\n\nObtain estimates \\(\\hat{f}(x_1), \\hat{f}(x_2), \\ldots, \\hat{f}(x_n)\\) (or, \\(\\hat{C}(x_1), \\hat{C}(x_2), \\ldots, \\hat{C}(x_n)\\)) of training data\nCompute error:\n\nRegression \\[\\text{Training MSE}=\\text{Average}_{Training} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\left(y_i-\\hat{f}(x_i)\\right)^2\\]\nClassification \\[\n\\begin{aligned}\n\\text{Training Error Rate}\n&=\\text{Average}_{Training} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]\\\\\n&= \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\ I\\left(y_i \\ne \\hat{C}(x_i)\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#supervised-learning-assessing-model-performance-1",
    "href": "slides/03-SLR-Gradient_Desc.html#supervised-learning-assessing-model-performance-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\nIn general, not interested in performance on training data\nWant: performance on unseen test data… why?\nFresh test data: \\((x_1^{test},y_1^{test}), (x_2^{test},y_2^{test}), \\ldots, (x_m^{test},y_m^{test})\\).\nCompute test error:\n\nRegression \\[\\text{Test MSE}=\\text{Average}_{Test} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\left(y_i^{test}-\\hat{f}(x_i^{test})\\right)^2\\]\nClassification \\[\\text{Test Error Rate}=\\text{Average}_{Test} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]= \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\ I\\left(y_i^{test} \\ne \\hat{C}(x_i^{test})\\right)\\]"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#supervised-learning-bias-variance-trade-off",
    "href": "slides/03-SLR-Gradient_Desc.html#supervised-learning-bias-variance-trade-off",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Supervised Learning: Bias-Variance Trade-off",
    "text": "Supervised Learning: Bias-Variance Trade-off\n\nModel fit on training data \\(\\hat{f}(x)\\)\n“True” relationship: \\(Y=f(x)+\\epsilon\\)\n\\((x_0^{test}, y_0^{test})\\): test observation\nBias-Variance Trade-Off (Theoretical) \\[\\underbrace{E\\left(y_0^{test}-\\hat{f}(x_0^{test})\\right)^2}_{total \\ error}=\\underbrace{Var\\left(\\hat{f}(x_0^{test})\\right)}_{source \\ 1} + \\underbrace{\\left[Bias\\left(\\hat{f}(x_0^{test})\\right)\\right]^2}_{source \\ 2}+\\underbrace{Var(\\epsilon)}_{source \\ 3}\\] where \\(Bias\\left(\\hat{f}(x_0)\\right)=E\\left(\\hat{f}(x_0)\\right)-f(x_0)\\)\n\n\n\nQuestion: Where is \\(\\hat{y}_0^{test}\\)?"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#supervised-learning-bias-variance-trade-off-1",
    "href": "slides/03-SLR-Gradient_Desc.html#supervised-learning-bias-variance-trade-off-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Supervised Learning: Bias-Variance Trade-off",
    "text": "Supervised Learning: Bias-Variance Trade-off\n\nReducible Error:\n\nSource 1: how \\(\\hat{f}(x)\\) varies among different randomly selected possible training data (Variance)\nSource 2: how \\(\\hat{f}(x)\\) (when predicting the test data) differs from its target \\(f(x)\\) (Bias)\n\nIrreducible Error:\n\nSource 3: how \\(y\\) differs from “true” \\(f(x)\\)"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example",
    "href": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFor now: focus on regression problems (ideas extend to classification)\nConsider: three different examples of simulated “toy” datasets and three types of models (\\(\\hat{f}_i(.)\\))\n\nLinear Regression orange\nSmoothing Spline 1 blue\nMore flexible Smoothing Spline 2 green\n\n“True” (simulated) function \\(f(.)\\) black\nTraining Error\nTest Error"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-1",
    "href": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-2",
    "href": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-2",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-3",
    "href": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-3",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-4",
    "href": "slides/03-SLR-Gradient_Desc.html#bias-variance-trade-off-example-4",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#a-familiar-supervised-learning-model",
    "href": "slides/03-SLR-Gradient_Desc.html#a-familiar-supervised-learning-model",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "A Familiar Supervised Learning Model",
    "text": "A Familiar Supervised Learning Model\n\nAssume relationship between \\(\\mathbf{X}\\) and \\(Y\\) is: \\[Y=f(\\mathbf{X}) + \\epsilon\\] where \\(\\epsilon\\) is a random error term (includes measurement error, other discrepancies) independent of \\(\\mathbf{X}\\) and has mean zero.\nObjective: To approximate/estimate \\(f(\\mathbf{X})\\)\nLinear Regression: assume that \\(f(\\mathbf{X})\\) is a linear function of \\(\\mathbf{X}\\)\n\nFor \\(p=1\\): \\(f(\\mathbf{X}) = \\beta_0 + \\beta_1 X_1\\)\nFor \\(p &gt; 1\\): \\(f(\\mathbf{X}) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\\)"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#linear-regression-1",
    "href": "slides/03-SLR-Gradient_Desc.html#linear-regression-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Linear Regression",
    "text": "Linear Regression\nSuppose the CEO of a restaurant franchise is considering opening new outlets in different cities. They would like to expand their business to cities that give them higher profits with the assumption that highly populated cities will probably yield higher profits.\nThey have data on the population (in 100,000) and profit (in $1,000) at 97 cities where they currently have outlets.\n\noutlets &lt;- readRDS(\"../data/outlets.rds\")   # load dataset\nhead(outlets) |&gt; kable() # first six observations of the dataset\n\n\n\n\npopulation\nprofit\n\n\n\n\n6.1101\n17.5920\n\n\n5.5277\n9.1302\n\n\n8.5186\n13.6620\n\n\n7.0032\n11.8540\n\n\n5.8598\n6.8233\n\n\n8.3829\n11.8860"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#linear-regression-2",
    "href": "slides/03-SLR-Gradient_Desc.html#linear-regression-2",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nObjective: choose “best” \\(\\beta_0\\) and \\(\\beta_1\\) if we assume \\[\\text{profit} = \\beta_0 + \\beta_1\\times\\text{population}\\]\nOnce this is done, we can:\n\npredict the profit for a new city with a given population\nunderstand the relationship between population and profit better"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#outlet-eda",
    "href": "slides/03-SLR-Gradient_Desc.html#outlet-eda",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Outlet EDA",
    "text": "Outlet EDA\n\noutlets |&gt; ggpairs() # ggpairs is from GGAlley package"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#linear-regression-estimating-parameters",
    "href": "slides/03-SLR-Gradient_Desc.html#linear-regression-estimating-parameters",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Linear Regression: Estimating Parameters",
    "text": "Linear Regression: Estimating Parameters\n\n\nSuppose we have \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)\nObserved response: \\(y_i\\) for \\(i=1,\\ldots,n\\)\nPredicted response: \\(\\hat{y}_i\\) for \\(i=1, \\ldots, n\\)\nResidual: \\(e_i = \\hat{y}_i - y_i\\) for \\(i=1, \\ldots, n\\)\nMean Squared Error (MSE): \\(MSE =\\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n}\\) also known as the loss/cost function\nGOAL: Find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) which minimizes \\(MSE\\)"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\nHow do we minimize the \\(MSE\\)?\n\nCan be done “analytically” but most ML models can’t be fit that way\n\nToday: popular optimization algorithm called gradient descent.\nNOTE: Gradient Descent is not a machine learning model/algorithm. It is an optimization technique that helps to fit machine learning models."
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-1",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\nThink of the MSE as a function of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)\n\n\\(\\text{MSE} = \\frac{\\sum (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1)^2}{n}\\)"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-2",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-2",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\nUpdates to the parameter: \\[\n\\begin{aligned}\n\\text{new value of parameters} &= \\text{old value of parameters}\\\\\n&\\qquad- \\text{step size} \\times \\text{gradient of function w.r.t. parameters}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-3",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-3",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-4",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-algorithm-4",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\n\n\n\n\n\n\n\n\nStep size too small\n\n\n\n\n\n\n\nStep size too too big"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent for Linear Regression",
    "text": "Gradient Descent for Linear Regression\n\nObjective: We want to find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) which minimizes\n\\[\\begin{aligned}\nMSE &= \\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n}\\\\\n&= \\dfrac{(\\hat{y}_1 - y_1)^2 +  (\\hat{y}_2 - y_2)^2 + \\ldots + (\\hat{y}_n - y_n)^2}{n}\n\\end{aligned}\\]\n\\[MSE = \\dfrac{(\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_1 - y_1)^2 +  (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_2 - y_2)^2 + \\ldots + (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_n - y_n)^2}{n}\\]\n\\[MSE = \\displaystyle \\dfrac{1}{n}\\sum_{i=1}^{n} (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_i - y_i)^2 = \\displaystyle \\dfrac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\displaystyle \\dfrac{1}{n}\\sum_{i=1}^{n} (e_i)^2\\]"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression-1",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression-1",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent for Linear Regression",
    "text": "Gradient Descent for Linear Regression\n\nTo compute gradient, need partial derivatives of \\(MSE\\) with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\n\\[\\text{gradient of MSE with respect to} \\ \\hat{\\beta}_0 = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_i - y_i) = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} (\\hat{y}_i - y_i)\\]\n\\[\\text{gradient of MSE with respect to} \\ \\hat{\\beta}_1 = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} x_i (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_i - y_i) = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} x_i (\\hat{y}_i - y_i)\\]"
  },
  {
    "objectID": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression-2",
    "href": "slides/03-SLR-Gradient_Desc.html#gradient-descent-for-linear-regression-2",
    "title": "MATH 427: Simple Linear Regression + Gradient Descent",
    "section": "Gradient Descent for Linear Regression",
    "text": "Gradient Descent for Linear Regression\n\nGradient descent update:\n\nFor \\(\\hat{\\beta}_0\\): \\[\n\\begin{aligned}\n\\hat{\\beta}_0 \\ \\text{(new)}\n&= \\hat{\\beta}_0 \\ \\text{(old)}- \\bigg(\\text{step size} \\times \\text{derivative w.r.t} \\ \\hat{\\beta}_0\\bigg)\\\\\n&= \\hat{\\beta}_0 \\ \\text{(old)}- \\bigg(\\text{step size} \\times \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} (\\hat{y}_i - y_i)\\bigg)\n\\end{aligned}\n\\]\nFor \\(\\hat{\\beta}_1\\): \\[\n\\begin{aligned}\n\\hat{\\beta}_1  \\ \\text{(new)} &= \\hat{\\beta}_1  \\ \\text{(old)} - \\bigg(\\text{step size} \\times \\text{derivative w.r.t} \\ \\hat{\\beta}_1 \\bigg)\\\\\n&= \\hat{\\beta}_1  \\ \\text{(old)} - \\bigg(\\text{step size} \\times \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} x_i (\\hat{y}_i - y_i)\\bigg)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n🔗 MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/05-knn.html#computational-setup",
    "href": "slides/05-knn.html#computational-setup",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(readODS)\nlibrary(modeldata) # contains ames dataset\n\ntidymodels_prefer()\n\nmlr_model &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")"
  },
  {
    "objectID": "slides/05-knn.html#comparing-models-data-splitting",
    "href": "slides/05-knn.html#comparing-models-data-splitting",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Comparing Models: Data Splitting",
    "text": "Comparing Models: Data Splitting\n\nSplit ames data set into two parts\n\nTraining set: randomly selected proportion \\(p\\) (typically 50-90%) of data used for fitting model\nTest set: randomly selected proportion \\(1-p\\) of data used for estimating prediction error\n\nIf comparing A LOT of models, split into three parts to prevent information leakage\n\nTraining set: randomly selected proportion \\(p\\) (typically 50-90%) of data used for fitting model\nValidation set: randomly selected proportion \\(q\\) (typically 20-30%) of data used to choosing tuning parameters\nTest set: randomly selected proportion \\(1-p-q\\) of data used for estimating prediction error\n\nIdea: use data your model hasn’t seen to get more accurate estimate of error and prevent overfitting"
  },
  {
    "objectID": "slides/05-knn.html#comparing-models-data-splitting-with-tidymodels",
    "href": "slides/05-knn.html#comparing-models-data-splitting-with-tidymodels",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Comparing Models: Data Splitting with tidymodels",
    "text": "Comparing Models: Data Splitting with tidymodels\n\nset.seed(427) # Why?\n\names_split &lt;- initial_split(ames, prop = 0.70, strata = Sale_Price) # initialize 70/30\names_split\n\n&lt;Training/Testing/Total&gt;\n&lt;2049/881/2930&gt;\n\names_train &lt;- training(ames_split) # get training data\names_test &lt;- testing(ames_split) # get test data\n\n\nstrata not necessary but good practice\n\nstrata will use stratified sampling on the variable you specify (very little downside)"
  },
  {
    "objectID": "slides/05-knn.html#linear-regression-comparing-models",
    "href": "slides/05-knn.html#linear-regression-comparing-models",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Linear Regression: Comparing Models",
    "text": "Linear Regression: Comparing Models\n\nLet’s create three models with Sale_Price as the response:\n\nfit1: a linear regression model with Bedroom_AbvGr as the only predictor\nfit2: a linear regression model with Gr_Liv_Area as the only predictor\nfit3 (similar to model in previous slides): a multiple regression model with Gr_Liv_Area and Bedroom_AbvGr as predictors\nfit4: super flexible model which fits a 10th degree polynomial to Gr_Liv_Area and a 2nd degree polynomial to Bedroom_AbvGr\n\n\n\nfit1 &lt;- mlr_model |&gt; fit(Sale_Price ~ Bedroom_AbvGr, data = ames_train) # Use only training set\nfit2 &lt;- mlr_model |&gt; fit(Sale_Price ~ Gr_Liv_Area, data = ames_train)\nfit3 &lt;- mlr_model |&gt; fit(Sale_Price ~ Gr_Liv_Area + Bedroom_AbvGr, data = ames_train)\nfit4 &lt;- mlr_model |&gt; fit(Sale_Price ~ poly(Gr_Liv_Area, degree = 10) + poly(Bedroom_AbvGr, degree = 2), data = ames_train)"
  },
  {
    "objectID": "slides/05-knn.html#computing-mse",
    "href": "slides/05-knn.html#computing-mse",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Computing MSE",
    "text": "Computing MSE\n\n# Fit 1\nfit1_train_mse &lt;- mean((ames_train$Sale_Price - predict(fit1, new_data = ames_train)$.pred)^2)\nfit1_test_mse &lt;- mean((ames_test$Sale_Price - predict(fit1, new_data = ames_test)$.pred)^2)\n\n# Fit 2\nfit2_train_mse &lt;- mean((ames_train$Sale_Price - predict(fit2, new_data = ames_train)$.pred)^2)\nfit2_test_mse &lt;- mean((ames_test$Sale_Price - predict(fit2, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit3_train_mse &lt;- mean((ames_train$Sale_Price - predict(fit3, , new_data = ames_train)$.pred)^2)\nfit3_test_mse &lt;- mean((ames_test$Sale_Price - predict(fit3, new_data = ames_test)$.pred)^2)\n\n# Fit \nfit4_train_mse &lt;- mean((ames_train$Sale_Price - predict(fit4, , new_data = ames_train)$.pred)^2)\nfit4_test_mse &lt;- mean((ames_test$Sale_Price - predict(fit4, new_data = ames_test)$.pred)^2)"
  },
  {
    "objectID": "slides/05-knn.html#question",
    "href": "slides/05-knn.html#question",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Question",
    "text": "Question\nWithout looking at the numbers\n\nDo we know which of the following is the smallest: fit1_train_mse, fit2_train_mse, fit3_train_mse, fit4_train_mse? Yes, fit4_train_mse\nDo we know which of the following is the smallest: fit1_test_mse, fit2_test_mse, fit3_test_mse, fit4_test_mse? No"
  },
  {
    "objectID": "slides/05-knn.html#choosing-a-model",
    "href": "slides/05-knn.html#choosing-a-model",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Choosing a Model",
    "text": "Choosing a Model\n\n\n\n# Training Errors\nc(fit1_train_mse, fit2_train_mse, \n  fit3_train_mse, fit4_train_mse)\n\n[1] 6213135279 3188099910 2781293767 2472424544\n\nwhich.min(c(fit1_train_mse, fit2_train_mse, \n            fit3_train_mse, fit4_train_mse))\n\n[1] 4\n\n# test Errors\nc(fit1_test_mse, fit2_test_mse, \n  fit3_test_mse, fit4_test_mse)\n\n[1] 6.329031e+09 3.203895e+09 2.732389e+09 2.726084e+12\n\nwhich.min(c(fit1_test_mse, fit2_test_mse, \n            fit3_test_mse, fit4_test_mse))\n\n[1] 3\n\n\n\n\nfit4 has the lowest training MSE (to be expected)\nfit3 has the lowest test MSE\n\nWe would choose fit3\n\nAnything else interesting we see?"
  },
  {
    "objectID": "slides/05-knn.html#regression-conditional-averaging",
    "href": "slides/05-knn.html#regression-conditional-averaging",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Regression: Conditional Averaging",
    "text": "Regression: Conditional Averaging\nRestaurant Outlets Profit dataset\n\n\n\n\n\n\n\n\n\nWhat is a good value of \\(\\hat{f}(x)\\) (expected profit), say at \\(x=6\\)?\nA possible choice is the average of the observed responses at \\(x=6\\). But we may not observe responses for certain \\(x\\) values."
  },
  {
    "objectID": "slides/05-knn.html#k-nearest-neighbors-knn-regression",
    "href": "slides/05-knn.html#k-nearest-neighbors-knn-regression",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "K-Nearest Neighbors (KNN) Regression",
    "text": "K-Nearest Neighbors (KNN) Regression\n\nNon-parametric approach\nFormally: Given a value for \\(K\\) and a test data point \\(x_0\\), \\[\\hat{f}(x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} y_i=\\text{Average} \\ \\left(y_i \\ \\text{for all} \\ i:\\ x_i \\in \\mathcal{N}_0\\right) \\] where \\(\\mathcal{N}_0\\) is the set of the \\(K\\) training observations closest to \\(x_0\\).\nInformally, average together the \\(K\\) “closest” observations in your training set\n“Closeness”: usually use the Euclidean metric to measure distance\nEuclidean distance between \\(\\mathbf{X}_i=(x_{i1}, x_{i2}, \\ldots, x_{ip})\\) and \\(\\mathbf{x}_j=(x_{j1}, x_{j2}, \\ldots, x_{jp})\\): \\[||\\mathbf{x}_i-\\mathbf{x}_j||_2 = \\sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \\ldots + (x_{ip}-x_{jp    })^2}\\]"
  },
  {
    "objectID": "slides/05-knn.html#knn-regression-single-predictor-fit",
    "href": "slides/05-knn.html#knn-regression-single-predictor-fit",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "KNN Regression (single predictor): Fit",
    "text": "KNN Regression (single predictor): Fit\n\n\n\\(K=1\\)\n\nknnfit1 &lt;- nearest_neighbor(neighbors = 1) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\") |&gt; \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit1, new_data = tibble(population = 6)) |&gt; kable()  # 1-nn prediction\n\n\n\n\n.pred\n\n\n\n\n0.92695\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(K=5\\)\n\nknnfit5 &lt;- nearest_neighbor(neighbors = 5) |&gt; \n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\") |&gt; \n  fit(profit ~ population, data = outlets)   # 1-nn regression\npredict(knnfit5, new_data = tibble(population = 6)) |&gt; kable()  # 1-nn prediction\n\n\n\n\n.pred\n\n\n\n\n4.113736"
  },
  {
    "objectID": "slides/05-knn.html#regression-methods-comparison",
    "href": "slides/05-knn.html#regression-methods-comparison",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Regression Methods: Comparison",
    "text": "Regression Methods: Comparison"
  },
  {
    "objectID": "slides/05-knn.html#question-1",
    "href": "slides/05-knn.html#question-1",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Question!!!",
    "text": "Question!!!\nAs \\(K\\) in KNN regression increases:\n\nthe flexibility of the fit (decreases /increases)\nthe bias of the fit (decreases/increases )\nthe variance of the fit (decreases/increases)"
  },
  {
    "objectID": "slides/05-knn.html#k-nearest-neighbors-regression-multiple-predictors",
    "href": "slides/05-knn.html#k-nearest-neighbors-regression-multiple-predictors",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "K-Nearest Neighbors Regression (multiple predictors)",
    "text": "K-Nearest Neighbors Regression (multiple predictors)\n\nLet’s look at the ames data\n\n\names |&gt;\n  select(Sale_Price, Gr_Liv_Area, Bedroom_AbvGr) |&gt;\n  head() |&gt; \n  kable()\n\n\n\n\nSale_Price\nGr_Liv_Area\nBedroom_AbvGr\n\n\n\n\n215000\n1656\n3\n\n\n105000\n896\n2\n\n\n172000\n1329\n3\n\n\n244000\n2110\n3\n\n\n189900\n1629\n3\n\n\n195500\n1604\n3\n\n\n\n\n\n\n\n\nShould 1 square foot count the same as 1 bedroom?\nNeed to center and scale (freq. just say scale)\n\nsubtract mean from each predictor\ndivide by standard deviation of each predictor\ncompares apples-to-apples"
  },
  {
    "objectID": "slides/05-knn.html#scaling-in-r",
    "href": "slides/05-knn.html#scaling-in-r",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Scaling in R",
    "text": "Scaling in R\n\n# scale predictors\names_scaled &lt;- tibble(size_scaled = scale(ames$Gr_Liv_Area),\n                                  num_bedrooms_scaled = scale(ames$Bedroom_AbvGr),\n                                  price = ames$Sale_Price)\n\nhead(ames_scaled) |&gt; kable()  # first six observations\n\n\n\n\nsize_scaled\nnum_bedrooms_scaled\nprice\n\n\n\n\n0.3092123\n0.1760642\n215000\n\n\n-1.1942232\n-1.0320576\n105000\n\n\n-0.3376606\n0.1760642\n172000\n\n\n1.2073172\n0.1760642\n244000\n\n\n0.2558008\n0.1760642\n189900\n\n\n0.2063456\n0.1760642\n195500"
  },
  {
    "objectID": "slides/05-knn.html#question-2",
    "href": "slides/05-knn.html#question-2",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Question…",
    "text": "Question…\n\n\nWhat about the training and test sets?\nNeed to scale BOTH sets based on the mean and standard deviation of the training set…\nDiscussion: Why?\nDiscussion: Why don’t I need to center and scale Sale_Price?"
  },
  {
    "objectID": "slides/05-knn.html#scaling-revisited",
    "href": "slides/05-knn.html#scaling-revisited",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "Scaling Revisited",
    "text": "Scaling Revisited\n\names_train_scaled &lt;- tibble(size_scaled = scale(ames_train$Gr_Liv_Area),\n                                  num_bedrooms_scaled = scale(ames_train$Bedroom_AbvGr),\n                                  price = ames_train$Sale_Price)\n\names_test_scaled &lt;- tibble(size_scaled = (ames_test$Gr_Liv_Area - mean(ames_train$Gr_Liv_Area)/sd(ames_train$Gr_Liv_Area)),\n                                  num_bedrooms_scaled = (ames_test$Bedroom_AbvGr - mean(ames_train$Bedroom_AbvGr))/sd(ames_train$Bedroom_AbvGr),\n                                  price = ames_test$Sale_Price)\n\n\nNext time: using recipe’s in tidymodels to simplify this process"
  },
  {
    "objectID": "slides/05-knn.html#k-nearest-neighbors-regression-multiple-predictors-1",
    "href": "slides/05-knn.html#k-nearest-neighbors-regression-multiple-predictors-1",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "K-Nearest Neighbors Regression (multiple predictors)",
    "text": "K-Nearest Neighbors Regression (multiple predictors)\n\nknnfit10 &lt;- nearest_neighbor(neighbors = 10) |&gt;   # 10-nn regression\n  set_engine(\"kknn\") |&gt; \n  set_mode(\"regression\") |&gt; \n  fit(price ~ size_scaled + num_bedrooms_scaled, data = ames_train_scaled)\n\n\nTest Point: Gr_Liv_area = 2000 square feet, and Bedroom_AbvGr = 3, then\n\n\n# obtain 10-nn prediction\n\npredict(knnfit10, new_data = tibble(size_scaled = (2000 - mean(ames_train$Gr_Liv_Area))/sd(ames_train$Gr_Liv_Area),\n                                     num_bedrooms_scaled = (3 - mean(ames_train$Bedroom_AbvGr))/sd(ames_train$Bedroom_AbvGr)))\n\n# A tibble: 1 × 1\n   .pred\n   &lt;dbl&gt;\n1 256380"
  },
  {
    "objectID": "slides/07-classification-logistic.html#classification-problems",
    "href": "slides/07-classification-logistic.html#classification-problems",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "",
    "text": "Response \\(Y\\) is qualitative (categorical).\nObjective: build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\)\n\nassigns class label to a future unlabeled (unseen) observations\nunderstand the relationship between the predictors and response\n\nTwo ways to make predictions\n\nClass probabilities\nClass labels"
  },
  {
    "objectID": "slides/07-classification-logistic.html#default-dataset",
    "href": "slides/07-classification-logistic.html#default-dataset",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Default Dataset",
    "text": "Default Dataset\nA simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.\n\nlibrary(ISLR2)   # load library\nhead(Default) |&gt; kable()  # print first six observations\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n729.5265\n44361.625\n\n\nNo\nYes\n817.1804\n12106.135\n\n\nNo\nNo\n1073.5492\n31767.139\n\n\nNo\nNo\n529.2506\n35704.494\n\n\nNo\nNo\n785.6559\n38463.496\n\n\nNo\nYes\n919.5885\n7491.559\n\n\n\n\n\nWe will consider default as the response variable."
  },
  {
    "objectID": "slides/07-classification-logistic.html#split-the-data",
    "href": "slides/07-classification-logistic.html#split-the-data",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Split the data",
    "text": "Split the data\n\nset.seed(427)\n\ndefault_split &lt;- initial_split(Default, prop = 0.6, strata = default)\ndefault_split\n\n&lt;Training/Testing/Total&gt;\n&lt;6000/4000/10000&gt;\n\ndefault_train &lt;- training(default_split)\ndefault_test &lt;- testing(default_split)"
  },
  {
    "objectID": "slides/07-classification-logistic.html#summarizing-our-response-variable",
    "href": "slides/07-classification-logistic.html#summarizing-our-response-variable",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Summarizing our response variable",
    "text": "Summarizing our response variable\n\nlibrary(janitor)\nDefault |&gt; tabyl(default) |&gt; kable()  # class frequencies\n\n\n\n\ndefault\nn\npercent\n\n\n\n\nNo\n9667\n0.9667\n\n\nYes\n333\n0.0333\n\n\n\n\ndefault_train |&gt; tabyl(default) |&gt; kable()\n\n\n\n\ndefault\nn\npercent\n\n\n\n\nNo\n5796\n0.966\n\n\nYes\n204\n0.034\n\n\n\n\ndefault_test |&gt; tabyl(default) |&gt; kable()\n\n\n\n\ndefault\nn\npercent\n\n\n\n\nNo\n3871\n0.96775\n\n\nYes\n129\n0.03225"
  },
  {
    "objectID": "slides/07-classification-logistic.html#data-types-in-r",
    "href": "slides/07-classification-logistic.html#data-types-in-r",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Data Types in R",
    "text": "Data Types in R\n\nDefault |&gt; glimpse()\n\nRows: 10,000\nColumns: 4\n$ default &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, No…\n$ student &lt;fct&gt; No, Yes, No, No, No, Yes, No, Yes, No, No, Yes, Yes, No, No, N…\n$ balance &lt;dbl&gt; 729.5265, 817.1804, 1073.5492, 529.2506, 785.6559, 919.5885, 8…\n$ income  &lt;dbl&gt; 44361.625, 12106.135, 31767.139, 35704.494, 38463.496, 7491.55…\n\n\n\nfct = factor which is the data type you want to use for categorical data\nas_factor will typically transform things (including numbers) into factors for you\nchr can also be used but factors are better because they store all possible levels for your categorical data\nfactors are helpful for plotting because you can reorder the levels to help you plot things"
  },
  {
    "objectID": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier",
    "href": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier",
    "text": "K-Nearest Neighbors Classifier\n\nGiven a value for \\(K\\) and a test data point \\(x_0\\): \\[P(Y=j | X=x_0)=\\dfrac{1}{K} \\sum_{x_i \\in \\mathcal{N}_0} I(y_i = j)\\] where \\(\\mathcal{N}_0\\) is the set of the \\(K\\) “closest” neighbors.\nFor classification: neighbors “vote” for class (unlike in regression where predictions are obtained by averaging) \\[P(Y=j | X=x_0)=\\text{Proportion of neighbors in class }j\\]"
  },
  {
    "objectID": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier-build-model",
    "href": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier-build-model",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier: Build Model",
    "text": "K-Nearest Neighbors Classifier: Build Model\n\nknn_default_fit &lt;- nearest_neighbor(neighbors = 10) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"classification\") |&gt;\n  fit(default ~ balance, data = default_train)   # fit 10-nn model\n\n\nWhy don’t I need to worry about centering and scaling?"
  },
  {
    "objectID": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier-predictions",
    "href": "slides/07-classification-logistic.html#k-nearest-neighbors-classifier-predictions",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "K-Nearest Neighbors Classifier: Predictions",
    "text": "K-Nearest Neighbors Classifier: Predictions\n\npredict with a categorical response: documentation\nTwo different ways of making predictions"
  },
  {
    "objectID": "slides/07-classification-logistic.html#predicting-a-class",
    "href": "slides/07-classification-logistic.html#predicting-a-class",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Predicting a class",
    "text": "Predicting a class\n\nknn_class_preds &lt;- predict(knn_default_fit, new_data = default_test, type = \"class\")   # obtain default class label predictions\n\nknn_class_preds |&gt; head() |&gt; kable()\n\n\n\n\n.pred_class\n\n\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo\n\n\nNo"
  },
  {
    "objectID": "slides/07-classification-logistic.html#predicting-a-probability",
    "href": "slides/07-classification-logistic.html#predicting-a-probability",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Predicting a probability",
    "text": "Predicting a probability\n\nCan anyone pick-out what’s wrong here? Hint: \\(k = 10\\)\n\n\nknn_prob_preds &lt;- predict(knn_default_fit, new_data = default_test, type = \"prob\")   # obtain predictions as probabilities\nknn_prob_preds |&gt; filter(.pred_No*.pred_Yes &gt;0) |&gt; head() |&gt; kable()\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n0.8685\n0.1315\n\n\n0.8685\n0.1315\n\n\n0.8685\n0.1315\n\n\n0.8505\n0.1495\n\n\n0.7105\n0.2895\n\n\n0.4775\n0.5225"
  },
  {
    "objectID": "slides/07-classification-logistic.html#ive-been-lying-to-you",
    "href": "slides/07-classification-logistic.html#ive-been-lying-to-you",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "I’ve been lying to you",
    "text": "I’ve been lying to you\n\nkknn actually takes a weighted average of the nearest neighbors\n\nI.e. closer observations get more weight\n\nTo use unweighted KNN need weight_func = \"rectangular\""
  },
  {
    "objectID": "slides/07-classification-logistic.html#unweighted-knn",
    "href": "slides/07-classification-logistic.html#unweighted-knn",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Unweighted KNN",
    "text": "Unweighted KNN\n\nknn_default_unw_fit &lt;- nearest_neighbor(neighbors = 10, weight_fun = \"rectangular\") |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"classification\") |&gt;\n  fit(default ~ balance, data = default_train)   # fit 10-nn model\n\nknn_uw_prob_preds &lt;- predict(knn_default_unw_fit, new_data = default_test, type = \"prob\")   # obtain predictions as probabilities\nknn_uw_prob_preds |&gt; filter(.pred_No*.pred_Yes &gt;0) |&gt; head() |&gt; kable()\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n0.9\n0.1\n\n\n0.9\n0.1\n\n\n0.9\n0.1\n\n\n0.9\n0.1\n\n\n0.7\n0.3\n\n\n0.5\n0.5"
  },
  {
    "objectID": "slides/07-classification-logistic.html#why-not-linear-regression",
    "href": "slides/07-classification-logistic.html#why-not-linear-regression",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\nDefault_lr &lt;- default_train |&gt; \n  mutate(default_0_1 = if_else(default == \"Yes\", 1, 0))\n\nlrfit &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(default_0_1 ~ balance, data = Default_lr)   # fit SLR\n\nlrfit |&gt; predict(new_data = default_train) |&gt; head() |&gt; kable()\n\n\n\n\n.pred\n\n\n\n\n0.0316011\n\n\n0.0655518\n\n\n-0.0065293\n\n\n0.0274263\n\n\n0.0451629\n\n\n0.0327046"
  },
  {
    "objectID": "slides/07-classification-logistic.html#why-not-linear-regression-1",
    "href": "slides/07-classification-logistic.html#why-not-linear-regression-1",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\n\n\n\n\n\n\n\n\n\n\nLinear regression: does not model probabilities well\n\nmight produce probabilities less than zero or bigger than one\ntreats increase from 0.41 to 0.5 as same as 0.01 to 0.1 (bad)"
  },
  {
    "objectID": "slides/07-classification-logistic.html#why-not-linear-regression-2",
    "href": "slides/07-classification-logistic.html#why-not-linear-regression-2",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Why Not Linear Regression?",
    "text": "Why Not Linear Regression?\nSuppose we have a response, \\[Y=\\begin{cases}\n1 & \\text{if stroke} \\\\\n2 & \\text{if drug overdose} \\\\\n3  & \\text{if epileptic seizure}\n\\end{cases}\\]\n\nLinear regression suggests an ordering, and in fact implies that the differences between classes have meaning\n\ne.g. drug overdose \\(-\\) stroke \\(= 1\\)? 🤔"
  },
  {
    "objectID": "slides/07-classification-logistic.html#logistic-regression-1",
    "href": "slides/07-classification-logistic.html#logistic-regression-1",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nConsider a one-dimensional binary classification problem:\n\nTransform the linear model \\(\\beta_0 + \\beta_1 \\ X\\) so that the output is a probability\nUse logistic function: \\[g(t)=\\dfrac{e^t}{1+e^t} \\ \\ \\ \\text{for} \\ t \\in \\mathcal{R}\\]\nThen: \\[p(X)=P(Y=1|X)=g\\left(\\beta_0 + \\beta_1 \\ X\\right)=\\dfrac{e^{\\beta_0 + \\beta_1 \\ X}}{1+e^{\\beta_0 + \\beta_1 \\ X}}\\]"
  },
  {
    "objectID": "slides/07-classification-logistic.html#other-important-quantities",
    "href": "slides/07-classification-logistic.html#other-important-quantities",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Other important quantities",
    "text": "Other important quantities\n\nOdds: \\(\\dfrac{p(x)}{1-p(x)}\\)\nLog-Odds (logit): \\(\\log\\left(\\dfrac{p(x)}{1-p(x)}\\right) = \\beta_0 + \\beta_1 \\ X\\)\n\nLinear function of predictors"
  },
  {
    "objectID": "slides/07-classification-logistic.html#logistic-regression-2",
    "href": "slides/07-classification-logistic.html#logistic-regression-2",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression"
  },
  {
    "objectID": "slides/07-classification-logistic.html#fitting-the-model",
    "href": "slides/07-classification-logistic.html#fitting-the-model",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Fitting the model",
    "text": "Fitting the model\nFitting a logistic regression model with default as the response and balance as the predictor:\n\nlogregfit &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\") |&gt; \n  fit(default ~ balance, data = default_train)   # fit logistic regression model\n\ntidy(logregfit) |&gt; kable()  # obtain results\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.6926385\n0.4659035\n-22.95033\n0\n\n\nbalance\n0.0055327\n0.0002841\n19.47329\n0"
  },
  {
    "objectID": "slides/07-classification-logistic.html#interpreting-coefficients",
    "href": "slides/07-classification-logistic.html#interpreting-coefficients",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients\n\nAs \\(X\\) increases by 1, the log-odds increase by\\(\\hat{\\beta}_1\\)\n\nI.e. probability of default increases but NOT linearly\nChange in the probability of default due to a one-unit change in balance depends on the current balance value"
  },
  {
    "objectID": "slides/07-classification-logistic.html#interpreting-coefficients-1",
    "href": "slides/07-classification-logistic.html#interpreting-coefficients-1",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients"
  },
  {
    "objectID": "slides/07-classification-logistic.html#making-predictions-theory",
    "href": "slides/07-classification-logistic.html#making-predictions-theory",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Making predictions: Theory",
    "text": "Making predictions: Theory\nFor balance=$700,\n\n\n\\[\\hat{p}(X)=\\dfrac{e^{\\hat{\\beta}_0+\\hat{\\beta}_1 X}}{1+e^{\\hat{\\beta}_0+\\hat{\\beta}_1 X}}=\\dfrac{e^{-10.69 + (0.005533 \\times 700)}}{1+e^{-10.69 + (0.005533 \\times 700)}}=0.0011\\]\n\\[\\textbf{Odds}(X) = \\dfrac{\\hat{p}(X)}{1-\\hat{p}(X)} = \\dfrac{0.0011}{1-0.0011}\\approx 0.0011\\]\n\\[\\textbf{Log-Odds}(X)=\\log\\left(\\dfrac{\\hat{p}(X)}{1-\\hat{p}(X)}\\right) = \\log(0.0011) = -6.80\\]"
  },
  {
    "objectID": "slides/07-classification-logistic.html#making-predictions-in-r",
    "href": "slides/07-classification-logistic.html#making-predictions-in-r",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "Making predictions in R",
    "text": "Making predictions in R\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"class\") |&gt; kable()   # obtain class predictions\n\n\n\n\n.pred_class\n\n\n\n\nNo\n\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"raw\") |&gt; kable()   # obtain log-odds predictions\n\n\n\n\nx\n\n\n\n\n-6.819727\n\n\n\n\npredict(logregfit, new_data = tibble(balance = 700), type = \"prob\") |&gt; kable()  # obtain probability predictions\n\n\n\n\n.pred_No\n.pred_Yes\n\n\n\n\n0.9989092\n0.0010908\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;  –&gt;  –&gt;\n\n\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n\n\n–&gt;\n\n\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;\n\n–&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;\n\n–&gt;  –&gt;  –&gt;"
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#lectures",
    "href": "support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#office-hours-homework-lab",
    "href": "support.html#office-hours-homework-lab",
    "title": "Course support",
    "section": "Office hours + Homework Lab",
    "text": "Office hours + Homework Lab\nDr. Friedlander is here to help you be successful in the course. You are encouraged to attend office hours and the homework lab during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters or course content, you may email Dr. Friedlander at efriedlander@collegeofidaho.edu. If you email me about an error please include a screenshot of the error and the code causing the error. Barring extenuating circumstances, I will respond to MAT 427 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "hw/03-hw-knn.html",
    "href": "hw/03-hw-knn.html",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "",
    "text": "In this homework, you will practice applying the \\(K\\)-Nearest Neighbors (KNN) method which is capable of performing both classification and regression. You will also practice collaborating with a team over GitHub.\n\n\nBy the end of the homework, you will…\n\nBe able to work simultaneously with teammates on the same document using GitHub\nFit and interpret linear models\nSplit data using tidymodels\nCompare and evaluate different linear models\n\n\n\n\nThe basic idea is that predictions are made based on the \\(K\\) observation in the training data which are “closest” to the observation that we’re making a prediction for. While many different metrics (i.e. measures of distance) can be used, we will work exclusively with the Euclidean metric: \\[\\text{dist}(x, y) = \\sqrt{\\sum_{i=1}^p(x_i-y_i)^2}\\] for vectors \\(x = (x_1,\\ldots, x_p)\\) and \\(y = (y_1,\\ldots,y_p)\\)."
  },
  {
    "objectID": "hw/03-hw-knn.html#data",
    "href": "hw/03-hw-knn.html#data",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Data",
    "text": "Data\nWe will be working with the famous iris data set which consists of four measurements (in centimeters) for 150 plants belonging to three species of iris. This data set was first published in a classic 1936 paper by English statistician, and notable racist/proponent of Eugenics, Ronald Fisher. In that paper, multivariate linear models were applied to classify these plants. Of course, back then, model fitting was an extremely laborious process that was done without the aid of calculators or statistical software. To access this data first load the package datasets and then load the data set iris."
  },
  {
    "objectID": "hw/03-hw-knn.html#the-knn-algorithm",
    "href": "hw/03-hw-knn.html#the-knn-algorithm",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "The KNN algorithm",
    "text": "The KNN algorithm"
  },
  {
    "objectID": "hw/03-hw-knn.html#data-dummy-variables",
    "href": "hw/03-hw-knn.html#data-dummy-variables",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Data & Dummy Variables",
    "text": "Data & Dummy Variables\nFor this portion of the homework, we’ll return to using the Carseat data from the ISLR package that we worked with in homework 4. Frequently, when working with categorical data, you will be required to transform that data into dummy variables. Namely, you’ll create a unique variable for each column which gets a 1 if the corresponding observation is from that category and a 0 otherwise. In data science, this format is sometimes referred to as one-hot encoding."
  },
  {
    "objectID": "hw/03-hw-knn.html#using-knn-for-regression",
    "href": "hw/03-hw-knn.html#using-knn-for-regression",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Using KNN for Regression",
    "text": "Using KNN for Regression\nThe function knnreg from the caret package will apply KNN to a regression problem by taking the average of the \\(K\\) nearest neighbors to the observation you’re looking to make a prediction for. knnreg works very similar to knn3 and previous models that we’ve fit in R."
  },
  {
    "objectID": "slides/06-knn-workflows.html",
    "href": "slides/06-knn-workflows.html",
    "title": "MAT-427: KNN + Preprocessing",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(modeldata) # contains ames dataset\n\ntidymodels_prefer()"
  },
  {
    "objectID": "slides/07-classification-logistic.html",
    "href": "slides/07-classification-logistic.html",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "",
    "text": "Response \\(Y\\) is qualitative (categorical).\nObjective: build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\)\n\nassigns class label to a future unlabeled (unseen) observations\nunderstand the relationship between the predictors and response\n\nTwo ways to make predictions\n\nClass probabilities\nClass labels"
  },
  {
    "objectID": "slides/08-classifiction-metrics.html",
    "href": "slides/08-classifiction-metrics.html",
    "title": "MATH 427: Classification + Logistic Regression",
    "section": "",
    "text": "Response \\(Y\\) is qualitative (categorical).\n\nOnly two classes \\(\\implies\\) Binary Classification Problem\n\nObjectives: build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\) that:\n\nassigns a class label to a future unlabeled (unseen) observation\nhelps understand the relationship between the predictors and response\n\nThere can be two types of predictions based on the research problem.\n\nClass probabilities\nClass labels"
  },
  {
    "objectID": "slides/05-knn.html",
    "href": "slides/05-knn.html",
    "title": "MAT-427: Data Splitting + KNN",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(readODS)\nlibrary(modeldata) # contains ames dataset\n\ntidymodels_prefer()\n\nmlr_model &lt;- linear_reg() |&gt; \n  set_engine(\"lm\")"
  },
  {
    "objectID": "hw/03-hw-knn.html#learning-goals",
    "href": "hw/03-hw-knn.html#learning-goals",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "",
    "text": "By the end of the homework, you will…\n\nBe able to work simultaneously with teammates on the same document using GitHub\nFit and interpret linear models\nSplit data using tidymodels\nCompare and evaluate different linear models"
  },
  {
    "objectID": "hw/03-hw-knn.html#knn-for-classification",
    "href": "hw/03-hw-knn.html#knn-for-classification",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "KNN for Classification",
    "text": "KNN for Classification\nWe’ll start by using KNN for classification."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-1",
    "href": "hw/03-hw-knn.html#exercise-1",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nImport the iris data set from the datasets package and take a look at the columns. We would like to visualize the \\(K\\)-nearest neighbor method in two dimensions, so make a data set called iris2 consisting of the columns Sepal.Width, Petal.Width, and Species. Split your data into training and test sets using a 70-30 split. IMPORTANT: Make sure that each species is represented proportionally in the training set by using the strata argument in the initial_split function! Once again, set your seed to 301."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-2",
    "href": "hw/03-hw-knn.html#exercise-2",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nPlot the points in your training set in the xy-plane colored by the Species labels."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-3",
    "href": "hw/03-hw-knn.html#exercise-3",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nAs the name suggests, the \\(K\\)-nearest neighbors (KNN) method classifies a point based on the classification of the observations in the training set that are nearest to that point. If \\(k &gt; 1\\), then the neighbors essentially “vote” on the classification of the point. Based on your graph, if \\(k = 1\\), how would KNN classify a flower that had sepal width 3cm and petal width 1cm?"
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-4",
    "href": "hw/03-hw-knn.html#exercise-4",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n\n\n\n\n\nQuestion\n\n\n\nJust to verify that we are correct, find the sepal width, petal width, and species of the observation in your training set that is closest to our flower with sepal width 3cm and petal width 1cm."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-5",
    "href": "hw/03-hw-knn.html#exercise-5",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n\n\n\n\n\nQuestion\n\n\n\nCenter and scale your data sets. You should be using the mean and standard deviation from your training set to center and scale both data sets."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-6",
    "href": "hw/03-hw-knn.html#exercise-6",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 6",
    "text": "Exercise 6\n\n\n\n\n\n\nQuestion\n\n\n\nRather than implementing this method by hand, we can use the function knn3 in the caret package.\n\n\nWe would like to understand how the method of \\(K\\)-nearest neighbors will classify points in the plane. That is, we would like to view the decision boundaries of this model. To do this, we will use our model to classify a large grid of points in the plane, and color them by their classification. The code below creates a data frame called grid consisting of 6.25^{4} points in the plane.\n\n# g1 &lt;- rep((200:450)*(1/100), 250)\n# g2 &lt;- rep((0:250)*(1/100), each = 250)\n# grid &lt;- tibble(  x1 = g1\n#                    , x2 = g2)"
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-7",
    "href": "hw/03-hw-knn.html#exercise-7",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 7",
    "text": "Exercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nUncomment the code above and classify the points in grid using your training data and \\(k = 1\\). Then, plot the points in grid colored by their classification. Don’t forget to center and scale the grid."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-8",
    "href": "hw/03-hw-knn.html#exercise-8",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 8",
    "text": "Exercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nNotice that the decision boundary between versicolor and virginica looks a little strange. What do you observe? Why do you think this is happening? Does using \\(k = 2\\) make things better or worse? Why do you think that is?"
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-9",
    "href": "hw/03-hw-knn.html#exercise-9",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 9",
    "text": "Exercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nDetermine which value of \\(K\\), the number of neighbors selected, gives the highest accuracy on the test set. Test all \\(K\\)s between 1 and 40. Note that there may be ties because our data set is a little bit too small. To break ties just choose the smallest \\(K\\) among the ones which are tied."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-10",
    "href": "hw/03-hw-knn.html#exercise-10",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 10",
    "text": "Exercise 10\n\n\n\n\n\n\nQuestion\n\n\n\nUse your value of \\(K\\) to classify the points in test based on this combined data set. You must recenter and scale your data based on this combination. Generate a confusion matrix and report the accuracy of your method? Note. You should “set the seed” in R by specifying set.seed(301) before you build your model. Because R randomly breaks ties, if you do not set the seed, you may get a different result the next time you knit your document (and your answer won’t match your code).\nAwesome!! Your model probably did pretty well, because KNN performs really well on the iris data set. However, this isn’t a very challenging data set for most classification methods. More challenging data sets have data on different scales and class imbalance where there are very few observations belonging to a particular class."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-11",
    "href": "hw/03-hw-knn.html#exercise-11",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 11",
    "text": "Exercise 11\n\n\n\n\n\n\nQuestion\n\n\n\nKNN can also be modified to give probabilistic predictions like we get from logistic regression. How would you modify the method to give a probabilistic prediction instead of a classification?"
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-12",
    "href": "hw/03-hw-knn.html#exercise-12",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 12",
    "text": "Exercise 12\n\n\n\n\n\n\nQuestion\n\n\n\nLoad the Carseat data from the ISLR package. Use the dummyVars function from the caret package to create dummy variables for the categorical variables in Carseat. Then, split the data into a training, validation, and test set using a 60-20-20 split and a seed of 301 (as usual). Explain what the fullRank options does and why we can create dummy variables before splitting our data.\n\n\nThere is no standard way of handling categorical variables when applying KNN to predictors that are categorical. This is because we need to answer the fundamental question of “What is the distance between two observations that belong to different categories?”. There are several ways of approaching this. One is to use different metrics and another is to just convert your categorical variables into numerical variables by using this one-hot encoding. Past that, you’ll need to decide whether to center and standardize your dummy variables."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-13",
    "href": "hw/03-hw-knn.html#exercise-13",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 13",
    "text": "Exercise 13\n\n\n\n\n\n\nQuestion\n\n\n\nFit a KNN model to predict Sales from the data we have. Fit your model on the training data and use the validation set to choose the appropriate variable and the number of neighbors to include. You may find it useful to plot the \\(R^2\\) and RMSE against the number of neighbors you include in your model. Once you select a \\(K\\) and the variables you want to include in your model, see how it performs on the test set and compare your result to the linear model you fit at the end of homework 4. Which model works better? You will find that the RMSE and \\(R^2\\) disagree on what the best model is. You will have to make a judgement call on which model is “best”. One thing that can be helpful is looking at plots of your target variables (Sales in this case) against the model residuals. When you do this, what pattern do you see?"
  },
  {
    "objectID": "slides/04-MultipleRegression.html",
    "href": "slides/04-MultipleRegression.html",
    "title": "MAT-427: Multiple Linear Regression + Data Splitting",
    "section": "",
    "text": "library(tidyverse)\nlibrary(knitr)\nlibrary(readODS)"
  },
  {
    "objectID": "hw/03-hw-knn.html#k-nearest-neighbors",
    "href": "hw/03-hw-knn.html#k-nearest-neighbors",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "",
    "text": "The basic idea is that predictions are made based on the \\(K\\) observation in the training data which are “closest” to the observation that we’re making a prediction for. While many different metrics (i.e. measures of distance) can be used, we will work exclusively with the Euclidean metric: \\[\\text{dist}(x, y) = \\sqrt{\\sum_{i=1}^p(x_i-y_i)^2}\\] for vectors \\(x = (x_1,\\ldots, x_p)\\) and \\(y = (y_1,\\ldots,y_p)\\)."
  },
  {
    "objectID": "hw/03-hw-knn.html#teams",
    "href": "hw/03-hw-knn.html#teams",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Teams",
    "text": "Teams\nYou can find your team for this assignment on Canvas in the People section. The group set is called HW3. Your group will consist of 2-3 people and has been randomly generated. But first, some rules:\n\nYou and your team members should be in the same physical room when you complete this assignment.\nYou should all contribute to each problem, but to receive credit you must rotate who actually writes down the answers.\nIn order to receive credit, you must have a commit after each exercise by the correct member of your team.\nFor now, don’t try to edit the same document at the same time. We will cover that in later homeworks."
  },
  {
    "objectID": "hw/03-hw-knn.html#clone-the-repo-start-new-rstudio-project",
    "href": "hw/03-hw-knn.html#clone-the-repo-start-new-rstudio-project",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\nThe following directions will guide you through the process of setting up your homework to work as a group."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-0.1",
    "href": "hw/03-hw-knn.html#exercise-0.1",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 0.1",
    "text": "Exercise 0.1\n\n\n\n\n\n\nQuestion\n\n\n\nIn your group, decide on a team name. Then have one member of your group:\n\nClick this link to accept the assignment and enter your team name.\nRepeat the directions for creating a project from HW 1 with the HW 3 repository.\n\nOnce this is complete, the other members can do the same thing, being careful to join the already created team on GitHub classroom."
  },
  {
    "objectID": "hw/03-hw-knn.html#exercise-0.2",
    "href": "hw/03-hw-knn.html#exercise-0.2",
    "title": "Homework 3: \\(K\\)-Nearest Neighbors",
    "section": "Exercise 0.2",
<<<<<<< HEAD
    "text": "Exercise 0.2\nWe will now learn how to collaborate on the same document at the same time.\n\n\n\n\n\n\nQuestion\n\n\n\nHave Member 1 fill in the Team Name and their name below and have Members 2 & 3 fill in their names. If you only have two members Member 2 should delete the line for Member 3.\n\n\n\nTeam Name: Test\nMember 1: [Insert Name]\nMember 2: My name"
=======
    "text": "Exercise 0.2\nWe will now learn how to collaborate on the same document at the same time.\n\n\n\n\n\n\nQuestion\n\n\n\nHave Member 1 fill in the Team Name and their name below and have Members 2 & 3 fill in their names. If you only have two members Member 2 should delete the line for Member 3.\n\n\n\nTeam Name: Test\nMember 1: Leader Name\nMember 2: [Insert Name]\nMember 3: [Insert Name/Delete line if you only have two members] git pull"
>>>>>>> c7f797c (Testing merge)
  }
]