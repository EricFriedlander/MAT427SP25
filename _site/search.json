[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MAT 427: Statistical Machine Learning",
    "section": "",
    "text": "A study of methods to construct and evaluate predictive models. Topics may include lasso and ridge regression, naive Bayes, random forests, support vector machines, gradient boosting, and neural networks. The course will require a significant data analysis and modeling project."
  },
  {
    "objectID": "index.html#catalog-description",
    "href": "index.html#catalog-description",
    "title": "MAT 427: Statistical Machine Learning",
    "section": "",
    "text": "A study of methods to construct and evaluate predictive models. Topics may include lasso and ridge regression, naive Bayes, random forests, support vector machines, gradient boosting, and neural networks. The course will require a significant data analysis and modeling project."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "MAT 427: Statistical Machine Learning",
    "section": "Instructor",
    "text": "Instructor\nEric is an Assistant Professor in the Department of Mathematics and Physical Sciences at the College of Idaho. His expertise lies in the areas of probability, statistics, and statistics education. He joined the College of Idaho faculty in 2024 after spending three years as an Assistant Professor at St.¬†Norbert College in De Pere, Wisconsin. He received his bachelor‚Äôs degree in mathematics and statistics from Rice University in 2011. After earning his degree, he worked for Capital One for two years in their home loans division before enrolling in graduate school. In 2018, Eric received his Ph.D.¬†in statistics and operations research from the University of North Carolina at Chapel Hill studying under professor Amarjit Budhiraja. His dissertation work focused on modeling and analyzing large systems which arise in industrial engineering (e.g.¬†large server and communication networks). Following his Ph.D., Eric did a postdoc in the Department of Ecology & Evolution at the University of Chicago under the direction of professor Matthias Steinr√ºcken where he used stochastic processes to study natural selection and population genetics.\nOutside of school, Eric is an avid fan of the New York Giants, New York Knicks, and North Carolina Tarheels. In addition, he enjoys comic books, the Fast and the Furious franchise, and spending time with his lovely wife Maria and lovable dogs Allie, Tony, and Miriam."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nMWF 3:00 - 3:50pm\nCruzen-Murray Library (CML) 208\n\n\n(Optional) Homework Lab\nW 4:00 - 4:50pm (Tentative)\nCruzen-Murray Library (CML) 208\n\n\nOffice Hours\nM 9:30 - 10:30am\nBoone 126B\n\n\nOffice Hours\nT 9:00 - 10:00am\nBoone 126B\n\n\nOffice Hours\nW 1:30 - 2:30pm\nBoone 126B\n\n\nOffice Hours\nTH 10:00 - 11:00am\nBoone 126B\n\n\n\nOffice hours are also available by appointment, just email me!\n\n\n\n\nInstructor: Dr.¬†Eric Friedlander\nOffice: Boone 126B\nEmail: efriedlander@collegeofidaho.edu",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nMWF 3:00 - 3:50pm\nCruzen-Murray Library (CML) 208\n\n\n(Optional) Homework Lab\nW 4:00 - 4:50pm (Tentative)\nCruzen-Murray Library (CML) 208\n\n\nOffice Hours\nM 9:30 - 10:30am\nBoone 126B\n\n\nOffice Hours\nT 9:00 - 10:00am\nBoone 126B\n\n\nOffice Hours\nW 1:30 - 2:30pm\nBoone 126B\n\n\nOffice Hours\nTH 10:00 - 11:00am\nBoone 126B\n\n\n\nOffice hours are also available by appointment, just email me!\n\n\n\n\nInstructor: Dr.¬†Eric Friedlander\nOffice: Boone 126B\nEmail: efriedlander@collegeofidaho.edu",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of the semester, you will be able to‚Ä¶\n\ntackle predictive modeling problems arising from real data.\nuse R to fit and evaluate machine learning models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate results results through writing and oral presentations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nCollege of Idaho Honor Code\n\nThe College of Idaho maintains that academic honesty and integrity are essential values in the educational process. Operating under an Honor Code philosophy, the College expects conduct rooted in honesty, integrity, and understanding, allowing members of a diverse student body to live together and interact and learn from one another in ways that protect both personal freedom and community standards. Violations of academic honesty are addressed primarily by the instructor and may be referred to the Student Judicial Board.\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the College of Idaho Honor Code.\n\n\nDisability Accommodation Statement\nThe College of Idaho seeks to provide an educational environment that is accessible to the needs of students with disabilities. The College provides reasonable services to enrolled students who have a documented permanent or temporary physical, psychological, learning, intellectual, or sensory disability that qualifies the student for accommodations under the Americans with Disabilities Act or section 504 of the Rehabilitation Act of 1973. If you have, or think you may have, a disability that impacts your performance as a student in this class, you are encouraged to arrange support services and/or accommodations through the Department of Accessibility and Learning Excellence located in McCain 201B and available via email at accessibility@collegeofidaho.edu. Reasonable academic accommodations may be provided to students who submit appropriate and current documentation of their disability. Accommodations can be arranged only through this process and are not retroactively applied. More information can be found on the DALE webpage (https://www.collegeofidaho.edu/accessibility).\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, mat427fa25.netlify.app.\nPeriodic announcements will be sent via email and will also be available through Canvas and grades will be stored in the Canvas gradebook. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nIn class agreements\nIf we discuss/agree to something in class or office hours which requires action from me (e.g.¬†‚Äúyou may turn in your homework late due to a sporting event‚Äù), you MUST send me a follow-up message. If you don‚Äôt, I will almost certainly forget, and our agreement will be considered null and void.\n\n\nGetting help in the course\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nI am here to help you be successful in the course. You are encouraged to attend office hours and the homework lab to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them!\nOutside of class and office hours, any general questions about course content or assignments can be emailed to me.\n\n\n\nEmail\nIf you have questions about assignment extensions or accommodations, please email efriedlander@collegeofidaho.edu. Please see Late work policy for more information. If you email me about an error please include a screenshot of the error and the code causing the error. Barring extenuating circumstances, I will respond to MAT 427 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\nCheck out the Support page for more resources.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook",
    "text": "Textbook\nThe official textbook for this course is:\n\nAn Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hatie, and Robert Tibshirani\n\nColloquiually referred to as ‚ÄúISLR‚Äù, it is considered one of the bibles of machine learning\nIt‚Äôs free!",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assignments",
    "href": "syllabus.html#assignments",
    "title": "Syllabus",
    "section": "Assignments",
    "text": "Assignments\nYou will be assessed based on five components: homework, job applications, job interviews, a hack-a-thon, and project.\n\nHomework\nIn homework, you will apply what you‚Äôve learned during lecture to complete data analysis tasks. Homework will completed in teams of three, must be typed up using Quarto, and submitted as .qmd and .pdf files in via GitHub.\n\n\nJob Applications & Job Interviews\nDuring this course you will apply to two ‚Äújobs‚Äù. I will generate the job advertisements including real companies and base the job description on the course content and similar job advertisements that I get from online or professional collaborators. Each job application will have three components:\n\nA cover letter.\nA resume.\nA portfolio.\n\nAll three of these should be tailored to the job description and the company to which you are applying. Your portfolio will consist of self-contained data analyses of your choosing. The most straight forward method of creating th to repurpose your homeworks, converting them from a format in which you are responding to exercises to something where you are telling a narrative and demonstrating that you meet the job criteria. To create your portfolio, you will be required to create a website. More details on this will be given during the semester, however the idea of this project is that you will be able to use the things you general when you are applying for jobs.\nAfter you submit your job applications, you will be invited to schedule a one-hour long job interview. It is your job to schedule your job interview with Dr.¬†Friedlander. Each job interview will have three portions. The first, lasting 10-15 minutes, will include typical questions that apply to almost any job interview (e.g.¬†‚ÄúWhat are your biggest strengths and weaknesses‚Äù). The second, lasting 20-30 minutes, will include questions about the portfolio you submitted and your understanding of the required skills described in the job advertising. The third section will mimic what is called a ‚Äúcase interview‚Äù. Case interviews are extremely common for many jobs, especially those requiring quantitative or computational skills, and can be intimidating. During the case interview portion, you will be presented with a ‚Äúcase study‚Äù and asked questions on how you would go about approaching it. The cases themselves will be designed so that they can be solved using the content from class. The goal of this whole exercise is to assess your knowledge of the course content in a way that is authentic while also preparing you to get a job.\n\n\nHack-a-thon\nAt some point in the semester we will participate in a ‚ÄúHack-a-thon‚Äù as a class. Namely, you will be given a short period of time (1-3 days) to build a model and make a set of predictions. After the competition is over, you will be required to present on your model. Part of your score will be determine by how well your model performs and extra credit will be given to the top scoring individuals.\n\n\nProject\nDuring the latter portion of the course, you will complete a final project that involves a deep exploration of a problem. More details for the final project will be provided later in the course.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n10%\n\n\nJob Application 1\n15%\n\n\nJob Application 2\n15%\n\n\nJob Interview 1\n15%\n\n\nJob Interview 2\n15%\n\n\nHack-a-thon & Presentation\n15%\n\n\nFinal Project\n15%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nTL;DR: Don‚Äôt cheat!\n\nThe job application assignments must be completed individually but you are welcome to discuss the assignment with classmates (e.g., discuss what‚Äôs the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share (i.e.¬†via copy/paste or copying) any code or prose with anyone other than myself.\nFor the hack-a-thon, everyone will submit their predictions and give their own presentations. However, you are encouraged to work together. You are allowed to share code with one another. However, everyone should be able to explain what they did and everyone‚Äôs projects should be unique in some way. Point reductions will be given if two individuals submit the exact same predictions.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g.¬†StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate‚Äîrather than hinder‚Äîlearning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity. In general if the following two things are not true, you are cheating:\n\nYou understand and can explain all of the code you have written down or you don‚Äôt and you have cited the source of that code.\nAll of your prose and narrative were written by yourself.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, just ask.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all College of Idaho policies, including academic integrity (e.g., completing one‚Äôs own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the Honor Code.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. I understand that things come up periodically that could make it difficult to submit an assignment by the deadline.\n\nLate Homework: Homework is completion based and well be accepted without penalty for a week. However, if your homework is turned in after I begin grading it, you will not receive any feedback.\nSchool-Sponsored Events/Illness: If an assignment or meeting must be missed due to a school-sponsored event, you must let me know at least a week ahead of time so that we can schedule a time for you to make up the work before you leave. If an assignment or meeting must be missed due to illness, you must let me know as soon as it is safe for you to do so and before the assignment or meeting if possible. Failure to adhere to this policy will result in a 35% penalty on the corresponding assignment.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.‚Ü©Ô∏é‚Ü©Ô∏é",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "hw/01-hw-eda.html",
    "href": "hw/01-hw-eda.html",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "",
    "text": "Adapted from ‚ÄúStart teaching with R,‚Äù created by R Pruim, N J Horton, and D Kaplan, 2013, ‚ÄúInteractive and Dynamic Graphics for Data Analysis,‚Äù by Dianne Cook and Deborah F. Swayne, Colby Long‚Äôs DATA 325 Course at Wooster College and Maria Tackett‚Äôs STA-210 course at Duke University."
  },
  {
    "objectID": "hw/01-hw-eda.html#introduction",
    "href": "hw/01-hw-eda.html#introduction",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Introduction",
    "text": "Introduction\nIn this homework we will familiarize ourselves with the tools that we‚Äôll use throughout the course and refresh ourselves on topic related to exploratory data analysis."
  },
  {
    "objectID": "hw/01-hw-eda.html#course-toolkit",
    "href": "hw/01-hw-eda.html#course-toolkit",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Course Toolkit",
    "text": "Course Toolkit\nThe primary tools we‚Äôll be using in this course are R, RStudio, git, and GitHub. We will be using them throughout the course both to learn the concepts discussed in the course and to analyze real data and come to informed conclusions.\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient interface.\n\n\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like ‚ÄúTrack Changes‚Äù features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like Dropbox but much better for code).\n\n\nTo make versioning simpler, this homework will be completed individually. In the future, you‚Äôll learn about collaborating on GitHub and producing a single homework for your team, but for now, concentrate on getting the basics down."
  },
  {
    "objectID": "hw/01-hw-eda.html#exploratory-data-analysis",
    "href": "hw/01-hw-eda.html#exploratory-data-analysis",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nOne of the most important components of data science is exploratory data analysis. I really like the following definition, which comes from this article (though it‚Äôs probably not the original source).\n\nExploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns, spot anomalies, to test hypotheses and to check assumptions with the help of summary statistics and graphical representations.\n\nBefore you begin your exploratory analysis, you may already have a particular question in mind. For example, you might work for an online retailer and want to develop a model to predict which purchased items will be returned. Or, you may not have a particular question in mind. Instead, you might just be asked to look at browsing data for several customers and figure out some way to increase purchases. In either case, before you construct a fancy model, you need to explore and understand your data. This is how you gain new insights and determine if an idea is worth pursuing."
  },
  {
    "objectID": "hw/01-hw-eda.html#learning-goals",
    "href": "hw/01-hw-eda.html#learning-goals",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the homework, you will‚Ä¶\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to create numerical and visual summaries of data\nUse those summaries"
  },
  {
    "objectID": "hw/01-hw-eda.html#clone-the-repo-start-new-rstudio-project",
    "href": "hw/01-hw-eda.html#clone-the-repo-start-new-rstudio-project",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/mat427sp25 organization on GitHub. Click on the repo with the prefix hw-01-. It contains the starter documents you need to complete the lab.\n\nIf you do not see your hw-01 repo, click here to create your repo.\n\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick 01-hw-eda.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "hw/01-hw-eda.html#r-and-r-studio",
    "href": "hw/01-hw-eda.html#r-and-r-studio",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of a Quarto (.qmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It is rumored that it stood for ‚ÄúYet Another Markup Language‚Äù but it officially stands for ‚ÄúYAML Ain‚Äôt Markup Language‚Äù. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you‚Äôre happy with these changes, we‚Äôll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don‚Äôt have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments I will nudge you when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let‚Äôs make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it‚Äôs time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "hw/01-hw-eda.html#understanding-your-data",
    "href": "hw/01-hw-eda.html#understanding-your-data",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Understanding your data",
    "text": "Understanding your data\nToday we will be working with the TIPS data set which is in the regclass package. The data in the TIPS dataset is information recorded by one waiter about each tip he received over a period of a few months working in a restaurant. We would like to use this data to address the question, ‚ÄúWhat factors affect tipping behavior?‚Äù\n\nExercise 1\n\n\n\n\n\n\nQuestion\n\n\n\nInstall the regclass package by either typing install.packages(\"regclass\") in the console or by clicking ‚ÄúTools &gt; Install Packages‚Äù and selecting the package. Once you have done this, the code chunk below will load the package and data set. Notice that a bunch of unnecessary output is included when you knit the document. Change the Quarto chunk options so that this is not displayed.\n\n\n\nlibrary(regclass)\n\nLoading required package: bestglm\n\n\nLoading required package: leaps\n\n\nLoading required package: VGAM\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\nLoading required package: rpart\n\n\nLoading required package: randomForest\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nImportant regclass change from 1.3:\nAll functions that had a . in the name now have an _\nall.correlations -&gt; all_correlations, cor.demo -&gt; cor_demo, etc.\n\ndata(\"TIPS\")\n\nWhen exploring a new data set, it‚Äôs important to first understand the basics. What format is our data in? What types of information are included in the data set? How many observations are there?\n\n\nExercise 2\n\n\n\n\n\n\nQuestion\n\n\n\nIn R, data sets are usually stored in a 2-dimensional structure called a data frame. The tidyverse provides a lot of useful functions for a variety of applications including data exploration and the particular flavor of data frame that the tidyverse uses is called a tibble. After loading the tidyverse library, you can get an idea of the structure of a data set using the syntax str(dataset) or glimpse(data), and you can peak at the first few rows and columns with head(dataset). Create a code chunk below, and use these functions (and others) in the R chunk below to better understand the data. How many tips are recorded in this data set? Which days of the week did the waiter work?\n\n\nOften, a data set will come with a code book which gives more complete information about the structure of the data, the meaning of variables, and how the data were collected. In this case, most of the column names are pretty self explanatory.\n\n\n\nVariable\nDescription\n\n\n\n\nTipPercentage\nthe gratuity, as a percentage of the bill\n\n\nBill\nthe cost of the meal in US dollars\n\n\nTip\nthe tip in US dollars\n\n\nGender\ngender of the bill payer\n\n\nSmoker\nwhether the party included smokers\n\n\nWeekday\nday of the week\n\n\nTime\ntime the bill was paid\n\n\nPartySize\nsize of the party\n\n\n\n\n\nExercise 3\n\n\n\n\n\n\nQuestion\n\n\n\nEven though the column names are self-explanatory, we might have more questions about the data. For example, we might conjecture that people tip differently for breakfast and lunch, but our data only tells us if the bill was paid at ‚ÄúDay‚Äù or ‚ÄúNight.‚Äù State another reasonable conjecture about a factor that might affect tipping behavior. What additional information would be helpful to explore that conjecture?\n\n\n\n\n\n\n\n\nRender-Commit-Push\n\n\n\nThis is a good place to render, commit, and push changes to your hw-eda repo on GitHub. Write an informative commit message (e.g.¬†‚ÄúCompleted exercises 1 - 3‚Äù), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "hw/01-hw-eda.html#graphical-summaries",
    "href": "hw/01-hw-eda.html#graphical-summaries",
    "title": "Homework 1: Exploratory Data Analysis",
    "section": "Graphical Summaries",
    "text": "Graphical Summaries\nGraphical summaries are a key tool in exploratory data analysis to to help you understand your data. They also help you communicate insights about your data to others. For example, we might want to display relationships about some of our categorical variables. So we could start by graphing different party sizes in our data set.\n\nTIPS |&gt; \n  ggplot(aes(x = PartySize)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nOr we could explore the question about the percentage of tables that are smokers on different days of the week visually.\n\nTIPS |&gt; \n  ggplot(aes(x = Weekday, fill = Smoker)) +\n    geom_bar()\n\n\n\n\n\n\n\nTIPS |&gt; \n  ggplot(aes(x = Weekday, fill = Smoker)) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nWe might summarize a numerical variable with a histogram. For example, here is a histogram of all of the tips in the data set.\n\nTIPS |&gt; \n  ggplot(aes(x = Tip)) +\n    geom_histogram(bins = 100)\n\n\n\n\n\n\n\n\n\nExercise 7\n\n\n\n\n\n\nQuestion\n\n\n\nNotice that there are a few ‚Äúspikes‚Äù in the histogram above. What do you think is causing this?\n\n\nWe can also summarize this numerical data broken down by one of the categorical variables using boxplots, violin plots, or sina plots. Note that to create sina plots we need the ggforce package.\n\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_boxplot() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_violin() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\nlibrary(ggforce)\n\nWarning: package 'ggforce' was built under R version 4.4.2\n\nTIPS |&gt; \n  ggplot(aes(x=Weekday, y=Tip)) +\n  geom_sina() +\n  labs(title = \"Tips by Day of the Week\", \n       x = \"Day of the Week\",\n       y = \"Tips\")\n\n\n\n\n\n\n\n\nOr we can visualize the relationship between a lot of our numerical variables at once.\n\n# Using pairs (only numerical allowed)\npairs(~ Bill + TipPercentage + Tip\n    , data = TIPS\n    , main=\"Scatterplot Matrix for TIPS\")\n\n\n\n\n\n\n\n# Using ggpairs from GGally package (preferable even though more syntax)\nlibrary(GGally)\nTIPS |&gt; \n  select(Bill, TipPercentage, Tip, Weekday) |&gt; \n  ggpairs()\n\n\n\n\n\n\n\n\n\n\nExercise 8\n\n\n\n\n\n\nQuestion\n\n\n\nAre there any clear linear relationships in the scatterplots above? What do you think is the explanation for these relationships?\n\n\nThere are lots of other interesting graphical summaries available for interpreting and displaying data. In addition, there are lots of R packages that allow you to draw these graphics and to further customize some of the ones we discussed here. In your projects, you are welcome to use any of these that you think are appropriate.\n\n\n\n\n\n\nRender-Commit-Push\n\n\n\nThis is a good place to render, commit, and push changes to your hw-eda repo on GitHub. Write an informative commit message (e.g.¬†‚ÄúCompleted exercises 7 - 8‚Äù), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\n\nExercise 9\n\n\n\n\n\n\nQuestion\n\n\n\nState a reasonable conjecture about tipping behavior that you would like to explore in the data set. For example, you might think that people on dates tip more or that the waiter gets smaller tips when he has too many tables. Give at least one numerical and one graphical summary to explore this conjecture. Is there any evidence to support your conjecture?\nIt‚Äôs okay if your conjecture is not supported or if you are just wrong‚Äìthat‚Äôs often the case in exploratory data analysis.\n\n\n\n\n\n\n\n\nRender-Commit-Push\n\n\n\nThis is a good place to render, commit, and push changes to your hw-eda repo on GitHub. Write an informative commit message (e.g.¬†‚ÄúCompleted exercises 9‚Äù), and push every file to GitHub by clicking the checkbox next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nBefore you‚Äôre done with you work, make sure you look it over one last time to make sure the rendered document looks like you want it to! I can‚Äôt tell you how often students turn in work and their output doesn‚Äôt match their prose or the output definitely doesn‚Äôt look the way they wanted it to."
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#books-free-online",
    "href": "computing-r-resources.html#books-free-online",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#rstudio-primers",
    "href": "computing-r-resources.html#rstudio-primers",
    "title": "Resources for learning R",
    "section": "RStudio Primers",
    "text": "RStudio Primers\n\nInteractive LearnR Tutorial\nAnother set of tutorials\nR Primers for full list of tutorials. Note that we will be using a simplified version of ggplot2 in this course.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nRStudio Cheatsheets\nR Fun workshops and videos by Duke Center for Data and Visualization Sciences.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "prepare/prep-01.html",
    "href": "prepare/prep-01.html",
    "title": "Preparation for Big Picture Discussion",
    "section": "",
    "text": "Review Syllabus\nRead Chapter 1 from ISLR2"
  },
  {
    "objectID": "prepare/prep-01.html#assigned-reading",
    "href": "prepare/prep-01.html#assigned-reading",
    "title": "Preparation for Big Picture Discussion",
    "section": "",
    "text": "Review Syllabus\nRead Chapter 1 from ISLR2"
  },
  {
    "objectID": "slides/03-Regression.html#concept-check",
    "href": "slides/03-Regression.html#concept-check",
    "title": "MATH 427: Linear Regression",
    "section": "Concept Check",
    "text": "Concept Check\nWhat do we call these:\n\n\\(\\mathbf{X}\\)\n\\(Y\\)\n\nWhat is our goal?"
  },
  {
    "objectID": "slides/03-Regression.html#concept-check-1",
    "href": "slides/03-Regression.html#concept-check-1",
    "title": "MATH 427: Linear Regression",
    "section": "Concept Check",
    "text": "Concept Check\n\nFeatures: \\(\\mathbf{X}\\)\nTarget: \\(Y\\)\nGoal: Predict \\(Y\\) using \\(\\mathbf{X}\\)\n\n\n\nWhat do we call this process if \\(Y\\) is numerical? What about categorical?"
  },
  {
    "objectID": "slides/03-Regression.html#concept-check-2",
    "href": "slides/03-Regression.html#concept-check-2",
    "title": "MATH 427: Linear Regression",
    "section": "Concept Check",
    "text": "Concept Check\n\nWhat do we call this process if \\(Y\\) is numerical? What about categorical?\n\nNumerical: regression\nCategorical: classification\n\n\n\n\nWhat is the difference between a training and a test set?\nHow do we evaluate the performance of a regression model?"
  },
  {
    "objectID": "slides/03-Regression.html#a-familiar-supervised-learning-model",
    "href": "slides/03-Regression.html#a-familiar-supervised-learning-model",
    "title": "MATH 427: Linear Regression",
    "section": "A Familiar Supervised Learning Model",
    "text": "A Familiar Supervised Learning Model\n\nAssume relationship between \\(\\mathbf{X}\\) and \\(Y\\) is: \\[Y=f(\\mathbf{X}) + \\epsilon\\] where \\(\\epsilon\\) is a random error term (includes measurement error, other discrepancies) independent of \\(\\mathbf{X}\\) and has mean zero.\nObjective: To approximate/estimate \\(f(\\mathbf{X})\\)\nLinear Regression: assume that \\(f(\\mathbf{X})\\) is a linear function of \\(\\mathbf{X}\\)\n\nFor \\(p=1\\): \\(f(\\mathbf{X}) = \\beta_0 + \\beta_1 X_1\\)\nFor \\(p &gt; 1\\): \\(f(\\mathbf{X}) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\\)"
  },
  {
    "objectID": "slides/03-Regression.html#linear-regression",
    "href": "slides/03-Regression.html#linear-regression",
    "title": "MATH 427: Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nSuppose the CEO of a restaurant franchise is considering opening new outlets in different cities. They would like to expand their business to cities that give them higher profits with the assumption that highly populated cities will probably yield higher profits.\nThey have data on the population (in 100,000) and profit (in $1,000) at 97 cities where they currently have outlets.\n\noutlets &lt;- readRDS(\"../data/outlets.rds\")   # load dataset\nhead(outlets) |&gt; kable() # first six observations of the dataset\n\n\n\n\npopulation\nprofit\n\n\n\n\n6.1101\n17.5920\n\n\n5.5277\n9.1302\n\n\n8.5186\n13.6620\n\n\n7.0032\n11.8540\n\n\n5.8598\n6.8233\n\n\n8.3829\n11.8860"
  },
  {
    "objectID": "slides/03-Regression.html#linear-regression-1",
    "href": "slides/03-Regression.html#linear-regression-1",
    "title": "MATH 427: Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nObjective: choose ‚Äúbest‚Äù \\(\\beta_0\\) and \\(\\beta_1\\) if we assume \\[\\text{profit} = \\beta_0 + \\beta_1\\times\\text{population}\\]\nOnce this is done, we can:\n\npredict the profit for a new city with a given population\nunderstand the relationship between population and profit better"
  },
  {
    "objectID": "slides/03-Regression.html#outlet-eda",
    "href": "slides/03-Regression.html#outlet-eda",
    "title": "MATH 427: Linear Regression",
    "section": "Outlet EDA",
    "text": "Outlet EDA\n\noutlets |&gt; ggpairs()"
  },
  {
    "objectID": "slides/03-Regression.html#linear-regression-estimating-parameters",
    "href": "slides/03-Regression.html#linear-regression-estimating-parameters",
    "title": "MATH 427: Linear Regression",
    "section": "Linear Regression: Estimating Parameters",
    "text": "Linear Regression: Estimating Parameters\n\n\nSuppose we have \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)\nObserved response: \\(y_i\\) for \\(i=1,\\ldots,n\\)\nPredicted response: \\(\\hat{y}_i\\) for \\(i=1, \\ldots, n\\)\nResidual: \\(e_i = \\hat{y}_i - y_i\\) for \\(i=1, \\ldots, n\\)\nMean Squared Error (MSE): \\(MSE =\\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n}\\) also known as the loss/cost function\nGOAL: Find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) which minimizes \\(MSE\\)"
  },
  {
    "objectID": "slides/03-Regression.html#gradient-descent-algorithm",
    "href": "slides/03-Regression.html#gradient-descent-algorithm",
    "title": "MATH 427: Linear Regression",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\nHow do we minimize the \\(MSE\\)?\n\nCan be done ‚Äúanalytically‚Äù but most ML algorithms can‚Äôt be fit that way\n\nToday: popular optimization algorithm called gradient descent.\nNOTE: Gradient Descent is not a machine learning technique. It is an optimization technique that helps to fit machine learning models."
  },
  {
    "objectID": "slides/03-Regression.html#gradient-descent-algorithm-1",
    "href": "slides/03-Regression.html#gradient-descent-algorithm-1",
    "title": "MATH 427: Linear Regression",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\nThink of the MSE as a function of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)\n\n\\(\\text{MSE} = \\frac{\\sum (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1)}{n}\\)"
  },
  {
    "objectID": "slides/03-Regression.html#gradient-descent-algorithm-2",
    "href": "slides/03-Regression.html#gradient-descent-algorithm-2",
    "title": "MATH 427: Linear Regression",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\nUpdates to the variable: \\[\n\\begin{aligned}\n\\text{new value of variable} &= \\text{old value of variable}\\\\\n&\\qquad- \\text{step size} \\times \\text{gradient of function with respect to variable}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-Regression.html#gradient-descent-algorithm-3",
    "href": "slides/03-Regression.html#gradient-descent-algorithm-3",
    "title": "MATH 427: Linear Regression",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm"
  },
  {
    "objectID": "slides/03-Regression.html#gradient-descent-algorithm-4",
    "href": "slides/03-Regression.html#gradient-descent-algorithm-4",
    "title": "MATH 427: Linear Regression",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\n\n\n\n\n\n\n\n\n\nStep size too big\n\n\n\n\n\n\n\nStep size too small"
  },
  {
    "objectID": "slides/03-Regression.html#gradient-descent-for-linear-regression",
    "href": "slides/03-Regression.html#gradient-descent-for-linear-regression",
    "title": "MATH 427: Linear Regression",
    "section": "Gradient Descent for Linear Regression",
    "text": "Gradient Descent for Linear Regression\n\nObjective: We want to find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) which minimizes\n\\[MSE = \\dfrac{e^2_1+e^2_2+\\ldots+e^2_n}{n} = \\dfrac{(\\hat{y}_1 - y_1)^2 +  (\\hat{y}_2 - y_2)^2 + \\ldots + (\\hat{y}_n - y_n)^2}{n}\\]\n\\[MSE = \\dfrac{(\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_1 - y_1)^2 +  (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_2 - y_2)^2 + \\ldots + (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_n - y_n)^2}{n}\\]\n\\[MSE = \\displaystyle \\dfrac{1}{n}\\sum_{i=1}^{n} (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_i - y_i)^2 = \\displaystyle \\dfrac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\displaystyle \\dfrac{1}{n}\\sum_{i=1}^{n} (e_i)^2\\]"
  },
  {
    "objectID": "slides/03-Regression.html#gradient-descent-for-linear-regression-1",
    "href": "slides/03-Regression.html#gradient-descent-for-linear-regression-1",
    "title": "MATH 427: Linear Regression",
    "section": "Gradient Descent for Linear Regression",
    "text": "Gradient Descent for Linear Regression\n\nTo compute gradient, need partial derivatives of \\(MSE\\) with respect to \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\).\n\\[\\text{gradient of MSE with respect to} \\ \\hat{\\beta}_0 = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_i - y_i) = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} (\\hat{y}_i - y_i)\\]\n\\[\\text{gradient of MSE with respect to} \\ \\hat{\\beta}_1 = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} x_i (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\ x_i - y_i) = \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} x_i (\\hat{y}_i - y_i)\\]"
  },
  {
    "objectID": "slides/03-Regression.html#gradient-descent-for-linear-regression-2",
    "href": "slides/03-Regression.html#gradient-descent-for-linear-regression-2",
    "title": "MATH 427: Linear Regression",
    "section": "Gradient Descent for Linear Regression",
    "text": "Gradient Descent for Linear Regression\n\nGradient descent update:\n\nFor \\(\\hat{\\beta}_0\\): \\[\n\\begin{aligned}\n\\hat{\\beta}_0 \\ \\text{(new)}\n&= \\hat{\\beta}_0 \\ \\text{(old)}- \\bigg(\\text{step size} \\times \\text{derivative w.r.t} \\ \\hat{\\beta}_0\\bigg)\\\\\n&= \\hat{\\beta}_0 \\ \\text{(old)}- \\bigg(\\text{step size} \\times \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} (\\hat{y}_i - y_i)\\bigg)\n\\end{aligned}\n\\]\nFor \\(\\hat{\\beta}_1\\): \\[\n\\begin{aligned}\n\\hat{\\beta}_1  \\ \\text{(new)} &= \\hat{\\beta}_1  \\ \\text{(old)} - \\bigg(\\text{step size} \\times \\text{derivative w.r.t} \\ \\hat{\\beta}_1 \\bigg)\\\\\n&= \\hat{\\beta}_1  \\ \\text{(old)} - \\bigg(\\text{step size} \\times \\dfrac{2}{n} \\displaystyle \\sum_{i=1}^{n} x_i (\\hat{y}_i - y_i)\\bigg)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-Regression.html#linear-regression-in-r",
    "href": "slides/03-Regression.html#linear-regression-in-r",
    "title": "MATH 427: Linear Regression",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\n\n\n\noutlets_model &lt;- lm(profit ~ population, data = outlets)\noutlets_model\n\n\nCall:\nlm(formula = profit ~ population, data = outlets)\n\nCoefficients:\n(Intercept)   population  \n     -3.896        1.193  \n\n\n\nThis corresponds to the model:\n\\[\n\\begin{aligned}\n\\text{Profit}  &= -3.90 + 1.19\\times\\text{Population}\\\\\n\\hat{Y}_i &= -3.90 + 1.19X_i\n\\end{aligned}\n\\] i.e.¬†\\(\\hat{\\beta}_0 = -3.90\\) and \\(\\hat{\\beta}_1 = 1.19\\)"
  },
  {
    "objectID": "slides/03-Regression.html#linear-regression-in-r-1",
    "href": "slides/03-Regression.html#linear-regression-in-r-1",
    "title": "MATH 427: Linear Regression",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\n\nggplot(data = outlets) +\n  geom_point(mapping = aes(x = population, y = profit)) +    # create scatterplot\n  geom_smooth(mapping = aes(x = population, y = profit), \n              method = \"lm\", se = FALSE)   # add the regression line"
  },
  {
    "objectID": "slides/03-Regression.html#linear-regression-in-r-prediction",
    "href": "slides/03-Regression.html#linear-regression-in-r-prediction",
    "title": "MATH 427: Linear Regression",
    "section": "Linear Regression in R: Prediction",
    "text": "Linear Regression in R: Prediction\n\npredict(outlets_model, newdata = data.frame(population = 17))\n\n       1 \n16.38579 \n\n\nNote: New data must be a data frame with the same columns names as the training data\n\n\n\n\nüîó MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#data-generating-process",
    "href": "slides/02-StatisticalLearning.html#data-generating-process",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Data Generating Process",
    "text": "Data Generating Process\nSuppose we have\n\nFeatures: \\(\\mathbf{X}\\)\nTarget: \\(Y\\)\nGoal: Predict \\(Y\\) using \\(\\mathbf{X}\\)\n\n\n\nData generating process: underlying, unseen and unknowable process that generates \\(Y\\) given \\(\\mathbf{X}\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#population",
    "href": "slides/02-StatisticalLearning.html#population",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Population",
    "text": "Population\nMore mathematically, the ‚Äútrue‚Äù/population model can be represented by\n\\[Y=f(\\mathbf{X}) + \\epsilon\\]\nwhere \\(\\epsilon\\) is a random error term (includes measurement error, other discrepancies) independent of \\(\\mathbf{X}\\) and has mean zero.\n\nGOAL: Estimate \\(f\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#why-estimate-fmathbfx",
    "href": "slides/02-StatisticalLearning.html#why-estimate-fmathbfx",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Why Estimate \\(f(\\mathbf{X})\\)?",
    "text": "Why Estimate \\(f(\\mathbf{X})\\)?\nWe wish to know about \\(f(\\mathbf{X})\\) for two reasons:\n\nPrediction: make an educated guess for what \\(y\\) should be given a new \\(x_0\\): \\[\\hat{y}_0=\\hat{f}(x_0) \\ \\ \\ \\text{or} \\ \\ \\ \\hat{y}_0=\\hat{C}(x_0)\\]\nInference: Understand the relationship between \\(\\mathbf{X}\\) and \\(Y\\).\n\n\n\nAn ML algorithm that is developed mainly for predictive purposes is often termed as a Black Box algorithm."
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#prediction",
    "href": "slides/02-StatisticalLearning.html#prediction",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Prediction",
    "text": "Prediction\nThere are two types of prediction problems:\n\nRegression (response \\(Y\\) is quantitative): Build a model \\(\\hat{Y} = \\hat{f}(\\mathbf{X})\\)\nClassification (response \\(Y\\) is qualitative/categorical): Build a classifier \\(\\hat{Y}=\\hat{C}(\\mathbf{X})\\)\n\n\n\nNote: a ‚Äúhat‚Äù, \\(\\hat{\\phantom{f}}\\), over an object represents an estimate of that object\n\nE.g. \\(\\hat{Y}\\) is an estimate of \\(Y\\) and \\(\\hat{f}\\) is an estimate of \\(f\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#prediction-and-inference",
    "href": "slides/02-StatisticalLearning.html#prediction-and-inference",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Prediction and Inference",
    "text": "Prediction and Inference\nIncome dataset\n\nWhy ML? (from ISLR2)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#prediction-and-inference-1",
    "href": "slides/02-StatisticalLearning.html#prediction-and-inference-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Prediction and Inference",
    "text": "Prediction and Inference\nIncome dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy ML? (from ISLR2)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#question",
    "href": "slides/02-StatisticalLearning.html#question",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Question!!!",
    "text": "Question!!!\nBased on the previous two slides, which of the following statements are correct?\n\nQuestionsAnswers\n\n\n\nAs Years of Education increases, Income increases, keeping Seniority fixed.\nAs Years of Education increases, Income decreases, keeping Seniority fixed.\nAs Years of Education increases, Income increases.\nAs Seniority increases, Income increases, keeping Years of Education fixed.\nAs Seniority increases, Income decreases, keeping Years of Education fixed.\nAs Seniority increases, Income increases.\n\n\n\n\nAs Years of Education increases, Income increases, keeping Seniority fixed. TRUE\nAs Years of Education increases, Income decreases, keeping Seniority fixed. FALSE\nAs Years of Education increases, Income increases. TRUE\nAs Seniority increases, Income increases, keeping Years of Education fixed. TRUE\nAs Seniority increases, Income decreases, keeping Years of Education fixed. FALSE\nAs Seniority increases, Income increases. TRUE"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#discussion",
    "href": "slides/02-StatisticalLearning.html#discussion",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Discussion",
    "text": "Discussion\nWhat‚Äôs the difference between these two statements:\n\nAs Years of Education increases, Income increases, keeping Seniority fixed.\nAs Years of Education increases, Income increases."
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#how-do-we-estimate-fmathbfx",
    "href": "slides/02-StatisticalLearning.html#how-do-we-estimate-fmathbfx",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "How Do We Estimate \\(f(\\mathbf{X})\\)?",
    "text": "How Do We Estimate \\(f(\\mathbf{X})\\)?\nBroadly speaking, we have two approaches.\n\nParametric methods\nNon-parametric methods"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#parametric-methods",
    "href": "slides/02-StatisticalLearning.html#parametric-methods",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Parametric Methods",
    "text": "Parametric Methods\n\nAssume a functional form for \\(f(\\mathbf{X})\\)\n\nLinear Regression: \\(f(\\mathbf{X})=\\beta_0 + \\beta_1 \\mathbf{x}_1 + \\beta_2 \\mathbf{x}_2 + \\ldots + \\beta_p \\mathbf{x}_p\\)\nEstimate the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using labeled data\n\nChoosing \\(\\beta\\)‚Äôs that minimize some error metrics is called fitting the model\nThe data we use to fit the model is called our training data"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#parametric-methods-1",
    "href": "slides/02-StatisticalLearning.html#parametric-methods-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Parametric Methods",
    "text": "Parametric Methods\n\nParametric model fit (from ISLR2)\n\nWhat are some potential parametric models that could result in this picture?\nNote: Right line is the true relationship"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#parametric-methods-2",
    "href": "slides/02-StatisticalLearning.html#parametric-methods-2",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Parametric Methods",
    "text": "Parametric Methods\nIncome dataset\n\n\n\n\n\n\n\n\n\nTrue relationship\n\n\n\n\n\n\n\nParametric model\n\n\n\n\n\n\nFrom ISLR2\n\n\n\n\n\nWhat are some functions that could have resulted in the model on the right?\n\\(\\text{Income} \\approx \\beta_0 + \\beta_1\\times\\text{Years of Education} + \\beta_2\\times\\text{Seniority}\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#non-parametric-methods",
    "href": "slides/02-StatisticalLearning.html#non-parametric-methods",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Non-parametric Methods",
    "text": "Non-parametric Methods\n\nNon-parametric approach: no explicit assumptions about the functional form of \\(f(\\mathbf{X})\\)\nMuch more observations (compared to a parametric approach) required to fit non-parametric model\n\nIdea: parametric model restricts space of possible answers\n\n\nIncome dataset\n\n\n\n\n\n\n\n\n\nTrue relationship\n\n\n\n\n\n\n\nNon-parametric model fit\n\n\n\n\n\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-flexibility-of-models",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-flexibility-of-models",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Flexibility of Models",
    "text": "Supervised Learning: Flexibility of Models\n\nFlexibility: smoothness of functions\nMore theoretically: how many parameters are there to estimate?\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess'\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_smooth()`).\n\n\n\n[More flexible \\(\\implies\\) More complex \\(\\implies\\) Less Smooth \\(\\implies\\) Less Restrictive \\(\\implies\\) Less Interpretable"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-some-trade-offs",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-some-trade-offs",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Some Trade-offs",
    "text": "Supervised Learning: Some Trade-offs\n\nPrediction Accuracy versus Interpretability\nGood Fit versus Over-fit or Under-fit\n\n\nTrade-off between flexibility and interpretability (from ISLR2)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-selecting-a-model",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-selecting-a-model",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Selecting a Model",
    "text": "Supervised Learning: Selecting a Model\n\nWhy so many different ML techniques?\nThere is no free lunch in statistics: All methods have different pros and cons\n\nMust select correct model for each use-case\n\nRelevant questions in model selection:\n\nHow much observations \\(n\\) and variables \\(p\\)?\nWhat is the relative importance is prediction, interpretability, and inference?\nDo we expect relationship to be non-linear?\nRegression or classification?"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\nWhen we estimate \\(f(\\mathbf{X})\\) using \\(\\hat{f}(\\mathbf{X})\\), then,\n\n\\[\\underbrace{E\\left[Y-\\hat{Y}\\right]^2}_{Error}=E\\left[f(\\mathbf{X})+\\epsilon - \\hat{f}(\\mathbf{X})\\right]^2=\\underbrace{\\left[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})\\right]^2}_{Reducible} + \\underbrace{Var(\\epsilon)}_{Irreducible}\\]\n\n\\(E\\left[Y-\\hat{Y}\\right]^2\\): Expected (average) squared difference between predicted and actual (observed) response, Mean Squared Error (MSE)\nGoal: find an estimate of \\(f(\\mathbf{X})\\) to minimize the reducible error"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance-1",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\n\nLabeled training data \\((x_1,y_1), (x_2, y_2), \\ldots, (x_n,y_n)\\)\n\ni.e.¬†\\(n\\) training observations\n\nFit/train a model from training data\n\n\\(\\hat{y}=\\hat{f}(x)\\), regression\n\\(\\hat{y}=\\hat{C}(x)\\), classification\n\n\nObtain estimates \\(\\hat{f}(x_1), \\hat{f}(x_2), \\ldots, \\hat{f}(x_n)\\) (or, \\(\\hat{C}(x_1), \\hat{C}(x_2), \\ldots, \\hat{C}(x_n)\\)) of training data\nCompute error:\n\nRegression \\[\\text{Training MSE}=\\text{Average}_{Training} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\left(y_i-\\hat{f}(x_i)\\right)^2\\]\nClassification \\[\n\\begin{aligned}\n\\text{Training Error Rate}\n&=\\text{Average}_{Training} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]\\\\\n&= \\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} \\ I\\left(y_i \\ne \\hat{C}(x_i)\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance-2",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-assessing-model-performance-2",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Assessing Model Performance",
    "text": "Supervised Learning: Assessing Model Performance\n\nIn general, not interested in performance on training data\nWant: performance on unseen test data‚Ä¶ why?\nFresh test data: \\((x_1^{test},y_1^{test}), (x_2^{test},y_2^{test}), \\ldots, (x_m^{test},y_m^{test})\\).\nCompute test error:\n\nRegression \\[\\text{Test MSE}=\\text{Average}_{Test} \\left(y-\\hat{f}(x)\\right)^2 = \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\left(y_i^{test}-\\hat{f}(x_i^{test})\\right)^2\\]\nClassification \\[\\text{Test Error Rate}=\\text{Average}_{Test} \\ \\left[I \\left(y\\ne\\hat{C}(x)\\right) \\right]= \\frac{1}{m} \\displaystyle \\sum_{i=1}^{m} \\ I\\left(y_i^{test} \\ne \\hat{C}(x_i^{test})\\right)\\]"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-bias-variance-trade-off",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-bias-variance-trade-off",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Bias-Variance Trade-off",
    "text": "Supervised Learning: Bias-Variance Trade-off\n\nModel fit on training data \\(\\hat{f}(x)\\)\n‚ÄúTrue‚Äù relationship: \\(Y=f(x)+\\epsilon\\)\n\\((x_0^{test}, y_0^{test})\\): test observation\nBias-Variance Trade-Off (Theoretical) \\[\\underbrace{E\\left(y_0^{test}-\\hat{f}(x_0^{test})\\right)^2}_{total \\ error}=\\underbrace{Var\\left(\\hat{f}(x_0^{test})\\right)}_{source \\ 1} + \\underbrace{\\left[Bias\\left(\\hat{f}(x_0^{test})\\right)\\right]^2}_{source \\ 2}+\\underbrace{Var(\\epsilon)}_{source \\ 3}\\] where \\(Bias\\left(\\hat{f}(x_0)\\right)=E\\left(\\hat{f}(x_0)\\right)-f(x_0)\\)\n\n\n\nQuestion: Where is \\(\\hat{y}_0^{test}\\)?"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-bias-variance-trade-off-1",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-bias-variance-trade-off-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Bias-Variance Trade-off",
    "text": "Supervised Learning: Bias-Variance Trade-off\n\nReducible Error:\n\nSource 1: how \\(\\hat{f}(x)\\) varies among different randomly selected possible training data (Variance)\nSource 2: how \\(\\hat{f}(x)\\) (when predicting the test data) differs from its target \\(f(x)\\) (Bias)\n\nIrreducible Error:\n\nSource 3: how \\(y\\) differs from ‚Äútrue‚Äù \\(f(x)\\)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#supervised-learning-comparing-bias-and-variance",
    "href": "slides/02-StatisticalLearning.html#supervised-learning-comparing-bias-and-variance",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Supervised Learning: Comparing Bias and Variance",
    "text": "Supervised Learning: Comparing Bias and Variance\nInsert App Stuff Here"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#bias-variance-trade-off-example",
    "href": "slides/02-StatisticalLearning.html#bias-variance-trade-off-example",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFor now: focus on regression problems (ideas extend to classification)\nConsider: three different examples of simulated ‚Äútoy‚Äù datasets and three types of models (\\(\\hat{f}_i(.)\\))\n\nLinear Regression orange\nSmoothing Spline 1 blue\nMore flexible Smoothing Spline 2 green\n\n‚ÄúTrue‚Äù (simulated) function \\(f(.)\\) black\nTraining Error\nTest Error"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#bias-variance-trade-off-example-1",
    "href": "slides/02-StatisticalLearning.html#bias-variance-trade-off-example-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#bias-variance-trade-off-example-2",
    "href": "slides/02-StatisticalLearning.html#bias-variance-trade-off-example-2",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#bias-variance-trade-off-example-3",
    "href": "slides/02-StatisticalLearning.html#bias-variance-trade-off-example-3",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#bias-variance-trade-off-example-4",
    "href": "slides/02-StatisticalLearning.html#bias-variance-trade-off-example-4",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Bias-Variance Trade-off: Example",
    "text": "Bias-Variance Trade-off: Example\n\nFrom ISLR2"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#question-1",
    "href": "slides/02-StatisticalLearning.html#question-1",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Question!!!",
    "text": "Question!!!\nAs flexibility increases,\n\nQuestionsAnswers\n\n\n\nits variance (increases/decreases)\nits bias (increases/decreases)\nits training MSE (increases/decreases)\nits test MSE (describe)\n\n\n\n\nits variance (increases)\nits bias (decreases)\nits training MSE (decreases)\nits test MSE (decreases at first, then increases and the model starts to overfit, U-shaped)"
  },
  {
    "objectID": "slides/02-StatisticalLearning.html#recap",
    "href": "slides/02-StatisticalLearning.html#recap",
    "title": "MATH 427: Intro to Machine Learning",
    "section": "Recap",
    "text": "Recap\n\nRegression vs.¬†Classification\nParametric vs.¬†non-parametric models\nTraining v. test data\nAssessing regression models: Mean-Squared Error\nTrade-offs:\n\nFlexibility vs.¬†interpretability\nBias vs.¬†variance\n\n\n\n\n\n\nüîó MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "slides/00-welcome.html#meet-prof.-friedlander",
    "href": "slides/00-welcome.html#meet-prof.-friedlander",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Meet Prof.¬†Friedlander!",
    "text": "Meet Prof.¬†Friedlander!\n\n\nEducation and career journey\n\nGrew up outside New York City\nBS in Math & Statistics from Rice University (Houston, TX)\nBusiness Analyst at Capital One (Plano, TX)\nMS and PhD in Statistics & Operations Research from UNC-Chapel Hill\nPostdoc in Population Genetics at University of Chicago\nAssistant Professor of Math at St.¬†Norbert College (Green Bay, WI)\n\nWork focuses on statistics education, queuing theory, and population genetics\nBig sports fan: NY Knicks, Giants, Rangers, Yankees, UNC Tarheels\nDad of three cute dogs: Allie, Miriam, Tony"
  },
  {
    "objectID": "slides/00-welcome.html#meet-prof.-friedlander-1",
    "href": "slides/00-welcome.html#meet-prof.-friedlander-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Meet Prof.¬†Friedlander!",
    "text": "Meet Prof.¬†Friedlander!"
  },
  {
    "objectID": "slides/00-welcome.html#tell-me-about-yourself-github",
    "href": "slides/00-welcome.html#tell-me-about-yourself-github",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Tell me about yourself + GitHub",
    "text": "Tell me about yourself + GitHub\nCreate a GitHub account. You may want to use this to show-off work to future employers so I recommend using something professional (like your name) as your user name.\nSend me an email with answers to the following questions:\n\nWhat is the GitHub username you just created?\nWhat would you like me to call you?\nWhy are you taking this class?\nWhat are your career goals?\nIs there anything else you would like me to know about you? E.g. athlete, preferred pronouns, accommodations, etc‚Ä¶\nPlease recommend at least one and up to infinity songs for the class playlist."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq",
    "href": "slides/00-welcome.html#course-faq",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What background is assumed for the course?\nA - Familiarity with concepts from statistical inference, linear regression, and logistic regression. A solid grounding in R, including the tidyverse and ggplot."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-1",
    "href": "slides/00-welcome.html#course-faq-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - Will we be doing computing?\nA - Yes. We will use the computing language R for analysis, Quarto for writing up results, and GitHub for version control and collaboration"
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-2",
    "href": "slides/00-welcome.html#course-faq-2",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - Will we learn the mathematical theory?\nA - Yes and No.¬†The course is primarily focused on application; however, we will discuss some of the mathematics occasionally."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-3",
    "href": "slides/00-welcome.html#course-faq-3",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What distinguishes this from a 300-level course?\nA - I expect a high level of independence from you. You should not be relying on me to teach you every small detail from this course. For example, if you tell you about an R function, I expect that you will be able to figure out how to use it yourself."
  },
  {
    "objectID": "slides/00-welcome.html#course-faq-4",
    "href": "slides/00-welcome.html#course-faq-4",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - Is there anything else I should know?\nA - Machine learning is a RAPIDLY evolving field. If you want to be successful in this field going forward, you will need to be able to learn things for yourself and SELF-ASSESS whether you know them. There are portions of this course that I have intentionally designed to not give you enough information to solve on your own."
  },
  {
    "objectID": "slides/00-welcome.html#course-learning-objectives",
    "href": "slides/00-welcome.html#course-learning-objectives",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\ntackle predictive modeling problems arising from real data.\nuse R to fit and evaluate machine learning models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate results results through writing and oral presentations."
  },
  {
    "objectID": "slides/00-welcome.html#course-toolkit",
    "href": "slides/00-welcome.html#course-toolkit",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nCourse website\n\nCentral hub for the course!\nTour of the website\n\nCanvas\n\nGradebook\nAnnouncements\n\nGitHub\n\nDistribute assignments\nPlatform for version control and collaboration"
  },
  {
    "objectID": "slides/00-welcome.html#computing-toolkit",
    "href": "slides/00-welcome.html#computing-toolkit",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nWrite reproducible reports in Quarto\nAccess RStudio through your personal computer (preferred) or the RStudio Server (email me ASAP if you are doing this)\n\n\n\n\n\n\n\n\n\n\n\n\nAccess assignments\nFacilitates version control and collaboration\nAll work in MAT 427 course classroom"
  },
  {
    "objectID": "slides/00-welcome.html#prepare-participate-practice-perform",
    "href": "slides/00-welcome.html#prepare-participate-practice-perform",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Prepare, Participate, Practice, Perform",
    "text": "Prepare, Participate, Practice, Perform\n\n\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with team-based homework graded for completion\nPerform: Put together what you‚Äôve learned to analyze real-world data\n\nTwo Job Applications/Portfolios (individual)\nTwo Job Interviews (individual)\nOne Hack-a-thon/Presentation (individual-ish)\nOne Project & Presentation (team)"
  },
  {
    "objectID": "slides/00-welcome.html#grading",
    "href": "slides/00-welcome.html#grading",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n10%\n\n\nJob Application 1\n15%\n\n\nJob Application 2\n15%\n\n\nJob Interview 1\n15%\n\n\nJob Interview 2\n15%\n\n\nHack-a-thon & Presentation\n15%\n\n\nFinal Project\n15%\n\n\n\nSee the syllabus for details on how the final letter grade will be calculated."
  },
  {
    "objectID": "slides/00-welcome.html#support",
    "href": "slides/00-welcome.html#support",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Support",
    "text": "Support\n\nAttend office hours\n\nProf.¬†Friedlander office hours\n\nDedicated homework help session\nUse email for questions regarding personal matters and/or grades\nSee the Course Support page for more details"
  },
  {
    "objectID": "slides/00-welcome.html#late-homework",
    "href": "slides/00-welcome.html#late-homework",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Late Homework",
    "text": "Late Homework\n\nOne week late (no grade penalty)\nAfter I start grading (no feedback)\nWhy should I care about feedback?\n\nIt‚Äôs how you learn‚Ä¶ duh\nYou will be repurposing your homeworks for your job applications"
  },
  {
    "objectID": "slides/00-welcome.html#school-sponsored-events",
    "href": "slides/00-welcome.html#school-sponsored-events",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "School-Sponsored Events",
    "text": "School-Sponsored Events\n\nExcused absences for event? Email me at least a week in advance\nSick or injured? Email me as soon as it is safe to do so.\n\nDon‚Äôt get me sick‚Ä¶\n\nFailure to adhere to this policy gets you a 35% point reduction"
  },
  {
    "objectID": "slides/00-welcome.html#academic-integrity",
    "href": "slides/00-welcome.html#academic-integrity",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nThe College of Idaho maintains that academic honesty and integrity are essential values in the educational process. Operating under an Honor Code philosophy, the College expects conduct rooted in honesty, integrity, and understanding, allowing members of a diverse student body to live together and interact and learn from one another in ways that protect both personal freedom and community standards. Violations of academic honesty are addressed primarily by the instructor and may be referred to the Student Judicial Board.\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the College of Idaho Honor Code."
  },
  {
    "objectID": "slides/00-welcome.html#collaboration-sharing-code",
    "href": "slides/00-welcome.html#collaboration-sharing-code",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Collaboration & sharing code",
    "text": "Collaboration & sharing code\n\nI have policies!"
  },
  {
    "objectID": "slides/00-welcome.html#use-of-artificial-intelligence-ai",
    "href": "slides/00-welcome.html#use-of-artificial-intelligence-ai",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n\nYou should treat AI tools, such as ChatGPT, the same as other online resources.\nThere are two guiding principles that govern how you can use AI in this course:1\n\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate‚Äîrather than hinder‚Äîlearning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.‚Ü©Ô∏é"
  },
  {
    "objectID": "slides/00-welcome.html#use-of-artificial-intelligence-ai-1",
    "href": "slides/00-welcome.html#use-of-artificial-intelligence-ai-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n\nUnderstand everything you write down\nTell me where you got it from\nDon‚Äôt lie about it\n\n\n\n\n\n\n\nImportant\n\n\nIn general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content. Any code or content from your homework is eligible for inclusion during your job interview."
  },
  {
    "objectID": "slides/00-welcome.html#in-class-agreements",
    "href": "slides/00-welcome.html#in-class-agreements",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "In class agreements",
    "text": "In class agreements\nIf we discuss/agree to something in class or office hours which requires action from me (e.g.¬†‚Äúyou may turn in your homework late due to a sporting event‚Äù), you MUST send me a follow-up message. If you don‚Äôt, I will almost certainly forget, and our agreement will be considered null and void."
  },
  {
    "objectID": "slides/00-welcome.html#five-tips-for-success",
    "href": "slides/00-welcome.html#five-tips-for-success",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work (readings and videos) before class.\nAsk questions, come to office hours and help session.\nDo the homework; get started on homework early when possible.\nDon‚Äôt procrastinate and don‚Äôt let a week pass by with lingering questions.\nStay up-to-date on announcements on Canvas and sent via email."
  },
  {
    "objectID": "slides/00-welcome.html#emails-for-help",
    "href": "slides/00-welcome.html#emails-for-help",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Emails for help",
    "text": "Emails for help\nIf you email me about an error please include a screenshot of the error and the code causing the error."
  },
  {
    "objectID": "slides/00-welcome.html#what-is-machine-learning",
    "href": "slides/00-welcome.html#what-is-machine-learning",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nMachine Learning is the study of tools/techniques for extracting information and making predictions from complex datasets\nThe name machine learning was coined in 1959 by Arthur Samuel\n\n‚ÄúField of study that gives computers the ability to learn without being explicitly programmed‚Äù"
  },
  {
    "objectID": "slides/00-welcome.html#what-is-machine-learning-1",
    "href": "slides/00-welcome.html#what-is-machine-learning-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\nTom M. Mitchell (1998):\n\nA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."
  },
  {
    "objectID": "slides/00-welcome.html#what-is-machine-learning-2",
    "href": "slides/00-welcome.html#what-is-machine-learning-2",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\n\nMNIST handwritten digits (from ISLR, James et al.)"
  },
  {
    "objectID": "slides/00-welcome.html#question",
    "href": "slides/00-welcome.html#question",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Question!!!",
    "text": "Question!!!\nSuppose your email program watches which emails you do or do not mark as spam, and based on that learns how to better filter spam. According to Tom Mitchell‚Äôs definition, which of the following is the task T, experience E, and performance measure P in this setting?\n\nP The number (or fraction) of emails correctly classified as spam/ham (not spam)\nT Classifying emails as spam or ham\nE Watching you label emails as spam or ham\n\n\n\n\n‚àí+\n01:00"
  },
  {
    "objectID": "slides/00-welcome.html#statistical-learning-vs-machine-learning-vs-data-science",
    "href": "slides/00-welcome.html#statistical-learning-vs-machine-learning-vs-data-science",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Statistical Learning vs Machine Learning vs Data Science",
    "text": "Statistical Learning vs Machine Learning vs Data Science\n\nMachine learning arose as a sub-field of Artificial Intelligence which is a sub-fields of Computer Science\nStatistical learning arose as a sub-field of Statistics\nThere is much overlap, a great deal of ‚Äúcross-fertilization‚Äù\n‚ÄúData Science‚Äù - Reflects the fact that both statistical and machine learning are about data\n‚ÄúMachine Learning‚Äù or ‚ÄúData Science‚Äù are ‚Äúfancier‚Äù terms"
  },
  {
    "objectID": "slides/00-welcome.html#statistics-vs-machine-learning",
    "href": "slides/00-welcome.html#statistics-vs-machine-learning",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Statistics vs Machine Learning",
    "text": "Statistics vs Machine Learning\n\nStatistics: more concerned with answering why and how things work, making inferences\nMachine/Statistical learning: more concerned with making predictions"
  },
  {
    "objectID": "slides/00-welcome.html#terminologynotation",
    "href": "slides/00-welcome.html#terminologynotation",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Terminology/Notation",
    "text": "Terminology/Notation\nAmes Housing dataset - Contains data on 881 houses in Ames, IA. We are interested in predicting sale price.\nThe first ten observations are shown below.\n\n\n\n\n\nSale_Price\nGr_Liv_Area\nGarage_Type\nGarage_Cars\nGarage_Area\nStreet\nUtilities\nPool_Area\nNeighborhood\n\n\n\n\n244000\n2110\nAttchd\n2\n522\nPave\nAllPub\n0\nNorth_Ames\n\n\n213500\n1338\nAttchd\n2\n582\nPave\nAllPub\n0\nStone_Brook\n\n\n185000\n1187\nAttchd\n2\n420\nPave\nAllPub\n0\nGilbert\n\n\n394432\n1856\nAttchd\n3\n834\nPave\nAllPub\n0\nStone_Brook\n\n\n190000\n1844\nAttchd\n2\n546\nPave\nAllPub\n0\nNorthwest_Ames\n\n\n149000\nNA\nAttchd\n2\n480\nPave\nAllPub\n0\nNorth_Ames\n\n\n149900\nNA\nAttchd\n2\n500\nPave\nAllPub\n0\nNorth_Ames\n\n\n127500\n1069\nAttchd\n2\n440\nPave\nAllPub\n0\nNorthpark_Villa\n\n\n395192\n1940\nAttchd\n3\n606\nPave\nAllPub\n0\nNorthridge_Heights\n\n\n290941\n1544\nAttchd\n3\n868\nPave\nAllPub\n0\nNorthridge_Heights"
  },
  {
    "objectID": "slides/00-welcome.html#terminologiesnotations",
    "href": "slides/00-welcome.html#terminologiesnotations",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Terminologies/Notations",
    "text": "Terminologies/Notations\nDefault dataset - Contains credit card default data on 10,000 individuals. We are interested in predicting whether somebody will default or not.\nTen observations are shown below.\n\n\n\n\n\ndefault\nstudent\nbalance\nincome\n\n\n\n\nNo\nNo\n939.0985\n45519.02\n\n\nNo\nYes\n397.5425\n22710.87\n\n\nYes\nNo\n1511.6110\n53506.94\n\n\nNo\nNo\n301.3194\n51539.95\n\n\nNo\nNo\n878.4461\n29561.78\n\n\nYes\nNo\n1673.4863\n49310.33\n\n\nNo\nNo\n310.1302\n37697.22\n\n\nNo\nNo\n1272.0539\n44895.59\n\n\nNo\nNo\n887.2014\n41641.45\n\n\nNo\nNo\n230.8689\n32798.78"
  },
  {
    "objectID": "slides/00-welcome.html#terminologiesnotations-1",
    "href": "slides/00-welcome.html#terminologiesnotations-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Terminologies/Notations",
    "text": "Terminologies/Notations\n\nResponse/Target/Outcome - variable we are interested in predicting, denoted as \\(Y\\)\nFeatures/Inputs/Predictors - variables used to predict the response, denoted as \\(X\\)\nFeature Matrix - all features taken together, denoted as \\(\\mathbf{X}\\)\nNumber of data points/observations denoted as \\(n\\)\nNumber of features/inputs/predictors denotes as \\(p\\)\nMissing entries in R are denoted as NA"
  },
  {
    "objectID": "slides/00-welcome.html#question-1",
    "href": "slides/00-welcome.html#question-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Question!!!",
    "text": "Question!!!\nFor the Ames Housing and Default datasets:\n\nQuestionsAnswers\n\n\n\nWhat are the corresponding values of \\(n\\) and \\(p\\)?\nWhat will be the dimension of the corresponding response vector \\(Y\\)?\nWhat is the value of the 3rd feature for the 2nd observation?\n\n\n\n\nWhat are the corresponding values of \\(n\\) and \\(p\\)?\n\nAmes: \\(n = 881\\) and \\(p = 9\\)\nDefault: \\(n = 10000\\) and \\(p = 4\\)\n\nWhat will be the dimension of the corresponding response vector \\(Y\\)?\n\nAmes: \\(881\\times 1\\)\nDefault: \\(10000\\times 1\\)\n\nWhat is the value of the 3rd feature for the 2nd observation?\n\nAmes: Attchd\nDefault: 397.5425"
  },
  {
    "objectID": "slides/00-welcome.html#question-2",
    "href": "slides/00-welcome.html#question-2",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Question!!!",
    "text": "Question!!!\nSuppose you have information about 867 cancer patients on their age, tumor size, clump thickness of the tumor, uniformity of cell size, and whether the tumor is malignant or benign. Based on these data, you are interested in building a model to predict the type of tumor (malignant or benign) for future cancer patients.\n\nWhat are the values of \\(n\\) and \\(p\\) in this dataset? \\(n = 867, p = 5\\)\nWhat are the inputs/features?"
  },
  {
    "objectID": "slides/00-welcome.html#supervised-vs-unsupervised-learning",
    "href": "slides/00-welcome.html#supervised-vs-unsupervised-learning",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Supervised vs Unsupervised Learning",
    "text": "Supervised vs Unsupervised Learning\n\nMachine Learning Tasks (from Bunker and Fayez, 2017)"
  },
  {
    "objectID": "slides/00-welcome.html#supervised-learning",
    "href": "slides/00-welcome.html#supervised-learning",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nWe have access to labeled data\nObjective: learn overall pattern of relationship between the inputs (\\(\\mathbf{X}\\)) and response (\\(Y\\)) in order to\n\nInvestigate the relationship between inputs and response\nPredict for potential unseen test cases\nAssess the quality of predictions"
  },
  {
    "objectID": "slides/00-welcome.html#types-of-supervised-learning",
    "href": "slides/00-welcome.html#types-of-supervised-learning",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Types of Supervised Learning",
    "text": "Types of Supervised Learning\nSupervised Learning problems can be categorized into:\n\nRegression problems (response is quantitative, continuous)\nClassification problems (response is qualitative, categorical)"
  },
  {
    "objectID": "slides/00-welcome.html#unsupervised-learning",
    "href": "slides/00-welcome.html#unsupervised-learning",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nNo response/outcome variable, just \\(\\mathbf{X}\\)\nUnderstand structure within data\n\nfind similar groups of observations based on features (clustering)\nfind a smaller subset of features with the most variation (dimensionality reduction)\n\nNo gold-standard\nEasier to collect unlabeled data\nUseful pre-processing step for supervised learning"
  },
  {
    "objectID": "slides/00-welcome.html#unsupervised-learning-1",
    "href": "slides/00-welcome.html#unsupervised-learning-1",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nUS Arrests dataset - Data on arrests for 50 US states.\nThe first ten observations are shown below.\n\n\n\n\n\n\nMurder\nAssault\nUrbanPop\nRape\n\n\n\n\nAlabama\n13.2\n236\n58\n21.2\n\n\nAlaska\n10.0\n263\n48\n44.5\n\n\nArizona\n8.1\n294\n80\n31.0\n\n\nArkansas\n8.8\n190\n50\n19.5\n\n\nCalifornia\n9.0\n276\n91\n40.6\n\n\nColorado\n7.9\n204\n78\n38.7\n\n\nConnecticut\n3.3\n110\n77\n11.1\n\n\nDelaware\n5.9\n238\n72\n15.8\n\n\nFlorida\n15.4\n335\n80\n31.9\n\n\nGeorgia\n17.4\n211\n60\n25.8"
  },
  {
    "objectID": "slides/00-welcome.html#question-3",
    "href": "slides/00-welcome.html#question-3",
    "title": "MATH 427: Statistical Machine Learning",
    "section": "Question!!!",
    "text": "Question!!!\n\nFor each of the following, identify whether the problem belongs to the supervised or unsupervised learning paradigm\n\nExamine the statistics of two football teams, and predict which team will win tomorrow‚Äôs match (given historical data of teams‚Äô wins/losses to learn from) supervised\nGiven genetic (DNA) data from a person, predict the probability of the person developing diabetes over the next 10 years supervised\nTake a collection of 1000 essays written on the US economy, and find a way to automatically group these essays into a small number of groups of essays that are somehow ‚Äúsimilar‚Äù or ‚Äúrelated‚Äù unsupervised\nExamine data on the income and years of education of adults in a neighborhood and build a model to predict the income from years of education supervised\n\n\n\n\n\n\nüîó MAT 427 - Spring 2025 - Schedule"
  },
  {
    "objectID": "prepare/prep-03.html",
    "href": "prepare/prep-03.html",
    "title": "Preparation for Linear Regression",
    "section": "",
    "text": "Read Chapter 3 from ISLR2"
  },
  {
    "objectID": "prepare/prep-03.html#assigned-reading",
    "href": "prepare/prep-03.html#assigned-reading",
    "title": "Preparation for Linear Regression",
    "section": "",
    "text": "Read Chapter 3 from ISLR2"
  },
  {
    "objectID": "prepare/prep-02.html",
    "href": "prepare/prep-02.html",
    "title": "Preparation for Introduction to Statistical Learning",
    "section": "",
    "text": "Read Chapter 2 from ISLR2"
  },
  {
    "objectID": "prepare/prep-02.html#assigned-reading",
    "href": "prepare/prep-02.html#assigned-reading",
    "title": "Preparation for Introduction to Statistical Learning",
    "section": "",
    "text": "Read Chapter 2 from ISLR2"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlecture\ndow\ndate\nweek\ntopic\nprepare\nslides\nhw\nproject\nnotes\n\n\n\n\n0\nM\nFeb 3\n1\nWelcome\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nW\nFeb 5\n1\nBig Picture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nF\nFeb 7\n1\nWhat is Statistical Learning",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#lectures",
    "href": "support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#office-hours-homework-lab",
    "href": "support.html#office-hours-homework-lab",
    "title": "Course support",
    "section": "Office hours + Homework Lab",
    "text": "Office hours + Homework Lab\nDr.¬†Friedlander is here to help you be successful in the course. You are encouraged to attend office hours and the homework lab during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters or course content, you may email Dr.¬†Friedlander at efriedlander@collegeofidaho.edu. If you email me about an error please include a screenshot of the error and the code causing the error. Barring extenuating circumstances, I will respond to MAT 427 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Course support"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "Not that we will go through this as a call and on your first homework. Feel free to revisit it here if you need to.",
    "crumbs": [
      "Computing",
      "Computing access"
    ]
  },
  {
    "objectID": "computing-access.html#r-rstudio",
    "href": "computing-access.html#r-rstudio",
    "title": "Computing access",
    "section": "R & RStudio",
    "text": "R & RStudio\nIt is highly recommended that you install R and RStudio on your own personal computer. Follow the directions here to install R and RStudio on your computer\nIf, for some reason, you are unable to use R or RStudio on your personal computer, you may use the College of Idaho‚Äôs RStudio Server. However, I do not recommend you do this as I want you to practice installing packages in this course and you do not have the permissions to do that on the server. If you must use the RStudio Server, please notify Dr.¬†Friedlander immediately.",
    "crumbs": [
      "Computing",
      "Computing access"
    ]
  },
  {
    "objectID": "computing-access.html#git-github",
    "href": "computing-access.html#git-github",
    "title": "Computing access",
    "section": "Git & Github",
    "text": "Git & Github\nYou need to create a GitHub account. You may want to use this to show-off work to future employers so I recommend using something professional (like your name) as your user name. Once you have done this, email it to Dr.¬†Friedlander so he can add you to the GitHub Classroom.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. Below are an outline of the authentication steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system.\n\n\n\nStep 0: Type install.packages(\"credentials\") into your console.\nStep 1: Type credentials::ssh_setup_github() into your console.\nStep 2: R will ask ‚ÄúNo SSH key found. Generate one now?‚Äù Click 1 for yes.\nStep 3: You will generate a key. It will begin with ‚Äússh-rsa‚Ä¶.‚Äù R will then ask ‚ÄúWould you like to open a browser now?‚Äù Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used, e.g., mat427)\n\nYou can find more detailed instructions here if you‚Äôre interested.\n\n\nConfigure git\nThere is one more thing we need to do before getting started on the assignment. Specifically, we need to configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package. You will need to install the usethis package in the same way you installed the credentials packages above.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"EricFriedlander\",\n  user.email = \"efriedlander@collegeofidaho.edu\")\n\nYou are now ready interact between GitHub and RStudio!",
    "crumbs": [
      "Computing",
      "Computing access"
    ]
  }
]