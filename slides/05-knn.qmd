---
title: 'MAT-427: Data Splitting + KNN'
author: Eric Friedlander
footer: "[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
---


## Computational Setup

```{r setup}
library(tidyverse)
library(tidymodels)
library(knitr)
library(readODS)
library(modeldata) # contains ames dataset

tidymodels_prefer()

mlr_model <- linear_reg() |> 
  set_engine("lm")
```

## Comparing Models: Data Splitting {.smaller}

- Split `ames` data set into two parts
  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model
  + Test set: randomly selected proportion $1-p$ of data used for estimating prediction error
- If comparing A LOT of models, split into *three* parts to prevent **information leakage**
  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model
  + Validation set: randomly selected proportion $q$ (typically 20-30%) of data used to choosing tuning parameters
  + Test set: randomly selected proportion $1-p-q$ of data used for estimating prediction error
- Idea: use data your model hasn't seen to get more accurate estimate of error and prevent overfitting

## Comparing Models: Data Splitting with `tidymodels` {.smaller}

```{r}
set.seed(427) # Why?

ames_split <- initial_split(ames, prop = 0.70, strata = Sale_Price) # initialize 70/30
ames_split

ames_train <- training(ames_split) # get training data
ames_test <- testing(ames_split) # get test data
```

- `strata` not necessary but good practice
  + `strata` will use *stratified sampling* on the variable you specify (very little downside) 

## Linear Regression: Comparing Models {.smaller}

- Let's create three models with `Sale_Price` as the response:
  + **fit1**: a linear regression model with `Bedroom_AbvGr`  as the only predictor
  + **fit2**: a linear regression model with `Gr_Liv_Area` as the only predictor
  + **fit3** (similar to model in previous slides): a multiple regression model with `Gr_Liv_Area` and `Bedroom_AbvGr` as predictors
  + **fit4**: super flexible model which fits a 10th degree polynomial to `Gr_Liv_Area` and a 2nd degree polynomial to `Bedroom_AbvGr`

```{r}
fit1 <- mlr_model |> fit(Sale_Price ~ Bedroom_AbvGr, data = ames_train) # Use only training set
fit2 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area, data = ames_train)
fit3 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area + Bedroom_AbvGr, data = ames_train)
fit4 <- mlr_model |> fit(Sale_Price ~ poly(Gr_Liv_Area, degree = 10) + poly(Bedroom_AbvGr, degree = 2), data = ames_train)
```

## Computing MSE {.smaller}

```{r}
# Fit 1
fit1_train_mse <- mean((ames_train$Sale_Price - predict(fit1, new_data = ames_train)$.pred)^2)
fit1_test_mse <- mean((ames_test$Sale_Price - predict(fit1, new_data = ames_test)$.pred)^2)

# Fit 2
fit2_train_mse <- mean((ames_train$Sale_Price - predict(fit2, new_data = ames_train)$.pred)^2)
fit2_test_mse <- mean((ames_test$Sale_Price - predict(fit2, new_data = ames_test)$.pred)^2)

# Fit 
fit3_train_mse <- mean((ames_train$Sale_Price - predict(fit3, , new_data = ames_train)$.pred)^2)
fit3_test_mse <- mean((ames_test$Sale_Price - predict(fit3, new_data = ames_test)$.pred)^2)

# Fit 
fit4_train_mse <- mean((ames_train$Sale_Price - predict(fit4, , new_data = ames_train)$.pred)^2)
fit4_test_mse <- mean((ames_test$Sale_Price - predict(fit4, new_data = ames_test)$.pred)^2)
```

## [Question]{style="color:blue"}

Without looking at the numbers

1. Do we know which of the following is the smallest: `fit1_train_mse`, `fit2_train_mse`, `fit3_train_mse`, `fit4_train_mse`? [Yes, `fit4_train_mse`]{.fragment .fade-in}
2. Do we know which of the following is the smallest: `fit1_test_mse`, `fit2_test_mse`, `fit3_test_mse`, `fit4_test_mse`? [No]{.fragment .fade-in}

## Choosing a Model {.smaller}

::::{.columns}
:::{.column}
```{r}
# Training Errors
c(fit1_train_mse, fit2_train_mse, 
  fit3_train_mse, fit4_train_mse)
which.min(c(fit1_train_mse, fit2_train_mse, 
            fit3_train_mse, fit4_train_mse))

# test Errors
c(fit1_test_mse, fit2_test_mse, 
  fit3_test_mse, fit4_test_mse)
which.min(c(fit1_test_mse, fit2_test_mse, 
            fit3_test_mse, fit4_test_mse))
```
:::
:::{.column}
- `fit4` has the lowest training MSE (to be expected)
- `fit3` has the lowest test MSE
  + We would choose `fit3`
- Anything else interesting we see?
:::
::::

# K-Nearest Neighbors

## Regression: Conditional Averaging {.smaller}

**Restaurant Outlets Profit dataset**

```{r}
#| echo: FALSE
outlets <- readRDS("../data/outlets.rds")   # load dataset

ggplot(data = outlets, aes(x = population, y = profit)) +
  geom_point() +   # create scatterplot
  geom_smooth(method = "lm", se = FALSE)   # add the SLR line
```


What is a good value of $\hat{f}(x)$ (expected profit), say at $x=6$?

A possible choice is the **average of the observed responses** at $x=6$. But we may not observe responses for certain $x$ values.


## K-Nearest Neighbors (KNN) Regression  {.smaller}

- Non-parametric approach
- Formally: Given a value for $K$ and a test data point $x_0$,
$$\hat{f}(x_0)=\dfrac{1}{K} \sum_{x_i \in \mathcal{N}_0} y_i=\text{Average} \ \left(y_i \ \text{for all} \ i:\ x_i \in \mathcal{N}_0\right) $$
where $\mathcal{N}_0$ is the set of the $K$ training observations closest to $x_0$.
- Informally, average together the $K$ "closest" observations in your training set
- "Closeness": usually use the **Euclidean metric** to measure distance
- Euclidean distance between $\mathbf{X}_i=(x_{i1}, x_{i2}, \ldots, x_{ip})$ and $\mathbf{x}_j=(x_{j1}, x_{j2}, \ldots, x_{jp})$:
$$||\mathbf{x}_i-\mathbf{x}_j||_2 = \sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \ldots + (x_{ip}-x_{jp    })^2}$$

## [KNN Regression (single predictor): Fit]{.r-fit-text} {.smaller}

::::{.columns}
:::{.column}
**$K=1$**
```{r}
knnfit1 <- nearest_neighbor(neighbors = 1) |> 
  set_engine("kknn") |> 
  set_mode("regression") |> 
  fit(profit ~ population, data = outlets)   # 1-nn regression
predict(knnfit1, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction
```

```{r}
#| echo: FALSE
outlets |>
  mutate(dist = abs(population - 6),
         is_knn = if_else(dist <= min(dist), TRUE, FALSE)) |>
  ggplot(aes(x = population, y = profit, color = is_knn)) +
  geom_point(size=3) +
  geom_vline(xintercept = 6) +
  theme(text = element_text(size = 20))
```


:::
:::{.column}
**$K=5$**
```{r}
knnfit5 <- nearest_neighbor(neighbors = 5) |> 
  set_engine("kknn") |> 
  set_mode("regression") |> 
  fit(profit ~ population, data = outlets)   # 1-nn regression
predict(knnfit5, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction
```

```{r}
#| echo: FALSE
outlets |>
  mutate(dist = abs(population - 6),
         is_knn = if_else(dist <= nth(dist, n=-5), TRUE, FALSE)) |>
  ggplot(aes(x = population, y = profit, color = is_knn)) +
  geom_point(size = 3) +
  geom_vline(xintercept = 6) +
  theme(text = element_text(size = 20))
```
:::
::::

## Regression Methods: Comparison

```{r}
#| echo: FALSE

slrfit <- linear_reg() |> 
  set_engine("lm") |> 
  fit(profit ~ population, data = outlets)   # fit the SLR model
pop_seq <- tibble(population = seq(min(outlets$population, na.rm = TRUE), max(outlets$population, na.rm = TRUE), 0.01))

# obtain predictions for all training data points
knn_1 <- predict(knnfit1, new_data = pop_seq)
knn_5 <- predict(knnfit5, new_data = pop_seq)
p <-  predict(slrfit, new_data = pop_seq)

# column bind original data with predicted values
predictions <- pop_seq |> 
  bind_cols(linear = p$.pred, knn_1 = knn_1$.pred, knn_5 = knn_5$.pred) |>
  pivot_longer(cols = !population, names_to = "Method", values_to = "profit")


# plot the three models
ggplot(data = outlets, aes(x = population, y = profit)) +
  geom_point() +
  geom_line(data = predictions, aes(x = population, y = profit, color = Method, linetype = Method), linewidth = 1)
```


## <span style="color:blue">Question!!!</span>

As $K$ in KNN regression increases:

- the flexibility of the fit ([increases]{.fragment .highlight-red} /decreases)
- the bias of the fit (increases/[decreases]{.fragment .highlight-red} )
- the variance of the fit ([increases]{.fragment .highlight-red}/decreases)


## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}

- Let's look at the `house_prices` data
```{r}
#| echo: TRUE

ames |>
  select(Sale_Price, Gr_Liv_Area, Bedroom_AbvGr) |>
  head() |> 
  kable()
```
:::{.fragment}
:::{.incremental}
- Should 1 square foot count the same as 1 bedroom?
- Need to **center and scale** (freq. just say scale)
  + subtract mean from each predictor
  + divide by standard deviation of each predictor
  + compares apples-to-apples
:::
:::

## Scaling in R

```{r}
# scale predictors
ames_scaled <- tibble(size_scaled = scale(ames$Gr_Liv_Area),
                                  num_bedrooms_scaled = scale(ames$Bedroom_AbvGr),
                                  price = ames$Sale_Price)

head(ames_scaled) |> kable()  # first six observations
```

## Question...

:::{.incremental}
-   What about the training and test sets?
-   Need to scale BOTH sets based on the mean and standard deviation of the training set...
-   Discussion: Why?
-   Discussion: Why don't I need to center and scale `Sale_Price`?
:::

## Scaling Revisited

```{r}
ames_train_scaled <- tibble(size_scaled = scale(ames_train$Gr_Liv_Area),
                                  num_bedrooms_scaled = scale(ames_train$Bedroom_AbvGr),
                                  price = ames_train$Sale_Price)

ames_test_scaled <- tibble(size_scaled = (ames_test$Gr_Liv_Area - mean(ames_train$Gr_Liv_Area)/sd(ames_train$Gr_Liv_Area)),
                                  num_bedrooms_scaled = (ames_test$Bedroom_AbvGr - mean(ames_train$Bedroom_AbvGr))/sd(ames_train$Bedroom_AbvGr),
                                  price = ames_test$Sale_Price)
```

-   Next time: using `recipe`'s in `tidymodels` to simplify this process

## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}


```{r}
knnfit10 <- nearest_neighbor(neighbors = 10) |>   # 10-nn regression
  set_engine("kknn") |> 
  set_mode("regression") |> 
  fit(price ~ size_scaled + num_bedrooms_scaled, data = ames_train_scaled)
```

- Test Point: `Gr_Liv_area` = 2000 square feet, and `Bedroom_AbvGr` = 3, then

```{r}
# obtain 10-nn prediction

predict(knnfit10, new_data = tibble(size_scaled = (2000 - mean(ames_train$Gr_Liv_Area))/sd(ames_train$Gr_Liv_Area),
                                     num_bedrooms_scaled = (3 - mean(ames_train$Bedroom_AbvGr))/sd(ames_train$Bedroom_AbvGr)))
```


## [Linear Regression vs K-Nearest Neighbors]{.r-fit-text} {.smaller}

- Linear regression is a parametric approach (with restrictive assumptions), KNN is non-parametric.
- Linear regression works for regression problems ($Y$ numerical), KNN can be used for both regression and classification - i.e. $Y$ qualitative
- Linear regression is interpretable, KNN is not.
- Linear regression can accommodate qualitative predictors and can be extended to include interaction terms as well while KNN does not allow for qualitative predictors
- Performance: KNN can be pretty good for small $p$, that is, $p \le 4$ and large $n$. Performance of KNN deteriorates as $p$ increases - *curse of dimensionality*

## Classification Problems {.smaller}

- Response $Y$ is qualitative (categorical).
- Objective: build a classifier $\hat{Y}=\hat{C}(\mathbf{X})$
  + assigns class label to a future unlabeled (unseen) observations
  + understand the relationship between the predictors and response
- Two ways to make predictions
  + Class probabilities
  + Class labels

## Default Dataset {.smaller}


A simulated data set containing information on ten thousand customers. The aim here is to predict which customers will default on their credit card debt.

```{r, message=FALSE}
library(ISLR2)   # load library
head(Default) |> kable()  # print first six observations
```

## Summarizing our response variable

```{r,message=FALSE}
library(janitor)
Default |> tabyl(default) |> kable()  # class frequencies
```

We will consider `default` as the response variable.

## Data Types in R {.smaller}

```{r}
Default |> glimpse()
```

-   `fct` = `factor` which is the data type you want to use for categorical data
-   `as_factor` will typically transform things (including numbers) into factors for you
-   `chr` can also be used but `factor`s are better because they store all possible levels for your categorical data
-   `factor`s are helpful for plotting because you can reorder the levels to help you plot things

## K-Nearest Neighbors Classifier

Given a value for $K$ and a test data point $x_0$,
$$P(Y=j | X=x_0)=\dfrac{1}{K} \sum_{x_i \in \mathcal{N}_0} I(y_i = j)$$

where $\mathcal{N}_0$ is known as the **neighborhood** of $x_0$.


For classification problems, the predictions are obtained in terms of **majority vote** (unlike in regression where predictions are obtained by averaging).


## K-Nearest Neighbors Classifier: Build Model

**Default dataset**

response ($Y$): `default` and predictor ($X$): `balance`

```{r}
knn_default_fit <- nearest_neighbor(neighbors = 10) |> 
  set_engine("kknn") |> 
  set_mode("classification") |> 
  fit(default ~ balance, data = Default)   # fit 10-nn model
```


## K-Nearest Neighbors Classifier: Predictions

-   `predict` with a categorical response: [documentation](https://parsnip.tidymodels.org/reference/predict.model_fit.html)
-   Two different ways of making predictions

## Predicting a class

```{r}
knn_class_preds <- predict(knn_default_fit, new_data = Default, type = "class")   # obtain default class label predictions

knn_class_preds |> head() |> kable()
```

##  Predicting a probability

```{r}
knn_prob_preds <- predict(knn_default_fit, new_data = Default, type = "prob")   # obtain predictions as probabilities
knn_prob_preds |> filter(.pred_No*.pred_Yes >0) |> head() |> kable()
```

## Questions

-   How do you think the probabilities are calculated?
-   How do you think we get from the probabilities to the class predictions?
-   Can you think of a situation in which you might choose a different way of going to probabilities to class labels?

## Thresholds

-   Can convert probabilities to classifications

```{r}
threshold <- 0.5   # set threshold

knn_class_preds_2 <- factor(if_else(knn_prob_preds$.pred_Yes > threshold, "Yes", "No"))   # obtain predictions as class labels
knn_class_preds_2 |> tabyl()
```

. . .

-   What if the downside of default is REALLY BAD 

## Thresholds

-   Can convert probabilities to classifications

```{r}
threshold <- 0.2   # set threshold

knn_class_preds_3 <- factor(if_else(knn_prob_preds$.pred_Yes > threshold, "Yes", "No"))   # obtain predictions as class labels
knn_class_preds_3 |> tabyl()
```


<!-- ## K-Nearest Neighbors Classifier: Performance  {.smaller} -->

<!-- **Default dataset** -->

<!-- ```{r} -->
<!-- # create confusion matrix -->

<!-- # use the following code only when all predictions are from the same class -->
<!-- # levels(knn_class_preds_1) = c("No", "Yes") -->

<!-- confusionMatrix(data = knn_class_preds_1, reference = Default$default, positive = "Yes") -->
<!-- ``` -->


