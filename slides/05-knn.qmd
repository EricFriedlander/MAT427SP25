---
title: 'MAT-427: Data Splitting + KNN'
author: Eric Friedlander
footer: "[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
---


## Computational Setup

```{r setup}
library(tidyverse)
library(tidymodels)
library(knitr)
library(readODS)
library(modeldata) # contains ames dataset

tidymodels_prefer()

mlr_model <- linear_reg() |> 
  set_engine("lm")
```

## Comparing Models: Data Splitting {.smaller}

- Split `ames` data set into two parts
  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model
  + Test set: randomly selected proportion $1-p$ of data used for estimating prediction error
- If comparing A LOT of models, split into *three* parts to prevent **information leakage**
  + Training set: randomly selected proportion $p$ (typically 50-90%) of data used for fitting model
  + Validation set: randomly selected proportion $q$ (typically 20-30%) of data used to choosing tuning parameters
  + Test set: randomly selected proportion $1-p-q$ of data used for estimating prediction error
- Idea: use data your model hasn't seen to get more accurate estimate of error and prevent overfitting

## Comparing Models: Data Splitting with `tidymodels` {.smaller}

```{r}
set.seed(427) # Why?

ames_split <- initial_split(ames, prop = 0.70, strata = Sale_Price) # initialize 70/30
ames_split

ames_train <- training(ames_split) # get training data
ames_test <- testing(ames_split) # get test data
```

- `strata` not necessary but good practice
  + `strata` will use *stratified sampling* on the variable you specify (very little downside) 

## Linear Regression: Comparing Models {.smaller}

- Let's create three models with `Sale_Price` as the response:
  + **fit1**: a linear regression model with `Bedroom_AbvGr`  as the only predictor
  + **fit2**: a linear regression model with `Gr_Liv_Area` as the only predictor
  + **fit3** (similar to model in previous slides): a multiple regression model with `Gr_Liv_Area` and `Bedroom_AbvGr` as predictors
  + **fit4**: super flexible model which fits a 10th degree polynomial to `Gr_Liv_Area` and a 2nd degree polynomial to `Bedroom_AbvGr`

```{r}
fit1 <- mlr_model |> fit(Sale_Price ~ Bedroom_AbvGr, data = ames_train) # Use only training set
fit2 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area, data = ames_train)
fit3 <- mlr_model |> fit(Sale_Price ~ Gr_Liv_Area + Bedroom_AbvGr, data = ames_train)
fit4 <- mlr_model |> fit(Sale_Price ~ poly(Gr_Liv_Area, degree = 10) + poly(Bedroom_AbvGr, degree = 2), data = ames_train)
```

## Computing MSE {.smaller}

```{r}
# Fit 1
fit1_train_mse <- mean((ames_train$Sale_Price - predict(fit1, new_data = ames_train)$.pred)^2)
fit1_test_mse <- mean((ames_test$Sale_Price - predict(fit1, new_data = ames_test)$.pred)^2)

# Fit 2
fit2_train_mse <- mean((ames_train$Sale_Price - predict(fit2, new_data = ames_train)$.pred)^2)
fit2_test_mse <- mean((ames_test$Sale_Price - predict(fit2, new_data = ames_test)$.pred)^2)

# Fit 
fit3_train_mse <- mean((ames_train$Sale_Price - predict(fit3, , new_data = ames_train)$.pred)^2)
fit3_test_mse <- mean((ames_test$Sale_Price - predict(fit3, new_data = ames_test)$.pred)^2)

# Fit 
fit4_train_mse <- mean((ames_train$Sale_Price - predict(fit4, , new_data = ames_train)$.pred)^2)
fit4_test_mse <- mean((ames_test$Sale_Price - predict(fit4, new_data = ames_test)$.pred)^2)
```

## [Question]{style="color:blue"}

Without looking at the numbers

1. Do we know which of the following is the smallest: `fit1_train_mse`, `fit2_train_mse`, `fit3_train_mse`, `fit4_train_mse`? [Yes, `fit4_train_mse`]{.fragment .fade-in}
2. Do we know which of the following is the smallest: `fit1_test_mse`, `fit2_test_mse`, `fit3_test_mse`, `fit4_test_mse`? [No]{.fragment .fade-in}

## Choosing a Model {.smaller}

::::{.columns}
:::{.column}
```{r}
# Training Errors
c(fit1_train_mse, fit2_train_mse, 
  fit3_train_mse, fit4_train_mse)
which.min(c(fit1_train_mse, fit2_train_mse, 
            fit3_train_mse, fit4_train_mse))

# test Errors
c(fit1_test_mse, fit2_test_mse, 
  fit3_test_mse, fit4_test_mse)
which.min(c(fit1_test_mse, fit2_test_mse, 
            fit3_test_mse, fit4_test_mse))
```
:::
:::{.column}
- `fit4` has the lowest training MSE (to be expected)
- `fit3` has the lowest test MSE
  + We would choose `fit3`
- Anything else interesting we see?
:::
::::

# K-Nearest Neighbors

## Regression: Conditional Averaging {.smaller}

**Restaurant Outlets Profit dataset**

```{r}
#| echo: FALSE
outlets <- readRDS("../data/outlets.rds")   # load dataset

ggplot(data = outlets, aes(x = population, y = profit)) +
  geom_point() +   # create scatterplot
  geom_smooth(method = "lm", se = FALSE)   # add the SLR line
```


What is a good value of $\hat{f}(x)$ (expected profit), say at $x=6$?

A possible choice is the **average of the observed responses** at $x=6$. But we may not observe responses for certain $x$ values.


## K-Nearest Neighbors (KNN) Regression  {.smaller}

- Non-parametric approach
- Formally: Given a value for $K$ and a test data point $x_0$,
$$\hat{f}(x_0)=\dfrac{1}{K} \sum_{x_i \in \mathcal{N}_0} y_i=\text{Average} \ \left(y_i \ \text{for all} \ i:\ x_i \in \mathcal{N}_0\right) $$
where $\mathcal{N}_0$ is the set of the $K$ training observations closest to $x_0$.
- Informally, average together the $K$ "closest" observations in your training set
- "Closeness": usually use the **Euclidean metric** to measure distance
- Euclidean distance between $\mathbf{X}_i=(x_{i1}, x_{i2}, \ldots, x_{ip})$ and $\mathbf{x}_j=(x_{j1}, x_{j2}, \ldots, x_{jp})$:
$$||\mathbf{x}_i-\mathbf{x}_j||_2 = \sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \ldots + (x_{ip}-x_{jp    })^2}$$

## [KNN Regression (single predictor): Fit]{.r-fit-text} {.smaller}

::::{.columns}
:::{.column}
**$K=1$**
```{r}
knnfit1 <- nearest_neighbor(mode = "regression", neighbors = 1) |> 
  set_engine("kknn") |> 
  fit(profit ~ population, data = outlets)   # 1-nn regression
predict(knnfit1, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction
```

```{r}
#| echo: FALSE
outlets |>
  mutate(dist = abs(population - 6),
         is_knn = if_else(dist <= min(dist), TRUE, FALSE)) |>
  ggplot(aes(x = population, y = profit, color = is_knn)) +
  geom_point(size=3) +
  geom_vline(xintercept = 6) +
  theme(text = element_text(size = 20))
```


:::
:::{.column}
**$K=5$**
```{r}
knnfit5 <- nearest_neighbor(mode = "regression", neighbors = 5) |> 
  set_engine("kknn") |> 
  fit(profit ~ population, data = outlets)   # 1-nn regression
predict(knnfit5, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction
```

```{r}
#| echo: FALSE
outlets |>
  mutate(dist = abs(population - 6),
         is_knn = if_else(dist <= nth(dist, n=-5), TRUE, FALSE)) |>
  ggplot(aes(x = population, y = profit, color = is_knn)) +
  geom_point(size = 3) +
  geom_vline(xintercept = 6) +
  theme(text = element_text(size = 20))
```
:::
::::

## Regression Methods: Comparison

```{r}
#| echo: FALSE

slrfit <- linear_reg() |> 
  set_engine("lm") |> 
  fit(profit ~ population, data = outlets)   # fit the SLR model
pop_seq <- tibble(population = seq(min(outlets$population, na.rm = TRUE), max(outlets$population, na.rm = TRUE), 0.01))

# obtain predictions for all training data points
knn_1 <- predict(knnfit1, new_data = pop_seq)
knn_5 <- predict(knnfit5, new_data = pop_seq)
p <-  predict(slrfit, new_data = pop_seq)

# column bind original data with predicted values
predictions <- pop_seq |> 
  bind_cols(linear = p$.pred, knn_1 = knn_1$.pred, knn_5 = knn_5$.pred) |>
  pivot_longer(cols = !population, names_to = "Method", values_to = "profit")


# plot the three models
ggplot(data = outlets, aes(x = population, y = profit)) +
  geom_point() +
  geom_line(data = predictions, aes(x = population, y = profit, color = Method, linetype = Method), linewidth = 1)
```


## <span style="color:blue">Question!!!</span>

As $K$ in KNN regression increases:

- the flexibility of the fit $\underline{\hspace{5cm}}$ ([increases]{.fragment .highlight-red} /decreases)
- the bias of the fit $\underline{\hspace{5cm}}$ (increases/[decreases]{.fragment .highlight-red} )
- the variance of the fit $\underline{\hspace{5cm}}$ ([increases]{.fragment .highlight-red}/decreases)


## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}

- Let's look at the `house_prices` data
```{r}
#| echo: FALSE

ames |>
  select(Sale_Price, Gr_Liv_Area, Bedroom_AbvGr) |>
  head()
```
:::{.fragment}
:::{.incremental}
- Should 1 square foot count the same as 1 bedroom?
- Need to **center and scale** (freq. just say scale)
  + subtract mean from each predictor
  + divide by standard deviation of each predictor
  + compares apples-to-apples
:::
:::

## Scaling in R

```{r}
# scale predictors
ames_scaled <- tibble(size_scaled = scale(ames$Gr_Liv_Area),
                                  num_bedrooms_scaled = scale(ames$Bedroom_AbvGr),
                                  price = ames$Sale_Price)

head(ames_scaled)   # first six observations
```


## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}


```{r}
library(caret)
knnfit10 <- knnreg(price ~ size_scaled + num_bedrooms_scaled, data = ames_scaled, k = 10)   # 10-nn regression
```

- Must also scale test data points **using mean and sd from training set!!!!**
- Test Point: `size` = 2000 square feet, and `num_bedrooms` = 3, then

```{r}
# obtain 10-nn prediction

predict(knnfit10, newdata = tibble(size_scaled = (2000 - mean(ames$Gr_Liv_Area))/sd(ames$Gr_Liv_Area),
                                     num_bedrooms_scaled = (3 - mean(ames$Bedroom_AbvGr))/sd(ames$Bedroom_AbvGr)))
```


## [Linear Regression vs K-Nearest Neighbors]{.r-fit-text} {.smaller}

- Linear regression is a parametric approach (with restrictive assumptions), KNN is non-parametric.
- Linear regression works for regression problems ($Y$ numerical), KNN can be used for both regression and classification - i.e. $Y$ qualitative (next lesson)
- Linear regression is interpretable, KNN is not.
- Linear regression can accommodate qualitative predictors and can be extended to include interaction terms as well while KNN does not allow for qualitative predictors
- Performance: KNN can be pretty good for small $p$, that is, $p \le 4$ and large $n$. Performance of KNN deteriorates as $p$ increases - *curse of dimensionality*

## Classification Problems {.smaller}

- Response $Y$ is qualitative (categorical).
- Objective: build a classifier $\hat{Y}=\hat{C}(\mathbf{X})$
  + assigns class label to a future unlabeled (unseen) observations
  + understand the relationship between the predictors and response
- Two ways to make predictions
  + Class probabilities
  + Class labels

## Classification Problems: Example

**Default dataset**

```{r, message=FALSE}
library(ISLR2)   # load library
data("Default")   # load dataset
```

```{r}
head(Default)   # print first six observations
```

```{r,message=FALSE}
table(Default$default)   # class frequencies
```


**We will consider `default` as the response variable.**


## Classification Problems: Example

For some algorithms, we might need to convert the categorical response to numeric (0/1) values.

**Default dataset**

```{r,message=FALSE}
Default$default_id <- ifelse(Default$default == "Yes", 1, 0)   # create 0/1 variable

head(Default, 10)   # print first ten observations
```




## K-Nearest Neighbors Classifier

Given a value for $K$ and a test data point $x_0$,
$$P(Y=j | X=x_0)=\dfrac{1}{K} \sum_{x_i \in \mathcal{N}_0} I(y_i = j)$$

where $\mathcal{N}_0$ is known as the **neighborhood** of $x_0$.


For classification problems, the predictions are obtained in terms of **majority vote** (unlike in regression where predictions are obtained by averaging).


## K-Nearest Neighbors Classifier: Build Model

**Default dataset**

response ($Y$): `default` and predictor ($X$): `balance`

```{r}
knnfit <- knn3(default ~ balance, data = Default, k = 10)   # fit 10-nn model
```


## K-Nearest Neighbors Classifier: Predictions

**Default dataset**

* One can directly obtain the class label predictions as below.

```{r}
knn_class_preds_1 <- predict(knnfit, newdata = Default, type = "class")   # obtain default class label predictions
```


* Otherwise, one can first obtain predictions in terms of probabilities and then convert them into class label predictions based on a threshold.

```{r}
knn_prob_preds <- predict(knnfit, newdata = Default, type = "prob")   # obtain predictions as probabilities
```


```{r}
threshold <- 0.5   # set threshold

knn_class_preds_2 <- factor(ifelse(knn_prob_preds[,2] > threshold, "Yes", "No"))   # obtain predictions as class labels
```


## K-Nearest Neighbors Classifier: Performance  {.smaller}

**Default dataset**

```{r}
# create confusion matrix

# use the following code only when all predictions are from the same class
# levels(knn_class_preds_1) = c("No", "Yes")

confusionMatrix(data = knn_class_preds_1, reference = Default$default, positive = "Yes")
```


