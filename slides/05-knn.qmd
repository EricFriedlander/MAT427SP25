---
title: 'MAT-427: Multiple Linear Regression + Data Splitting'
author: Eric Friedlander
footer: "[ðŸ”— MAT 427 - Spring 2025 -  Schedule](https://mat427sp25.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
---


## Computational Setup

```{r setup}
library(tidyverse)
library(tidymodels)
library(gridExtra)
library(modeldata)
library(knitr)

tidymodels_prefer()
```

# K-Nearest Neighbors

## Regression: Conditional Averaging {.smaller}

**Restaurant Outlets Profit dataset**

```{r}
#| echo: FALSE
outlets <- readRDS("../data/outlets.rds")   # load dataset

ggplot(data = outlets, aes(x = population, y = profit)) +
  geom_point() +   # create scatterplot
  geom_smooth(method = "lm", se = FALSE)   # add the SLR line
```


What is a good value of $\hat{f}(x)$ (expected profit), say at $x=6$?

A possible choice is the **average of the observed responses** at $x=6$. But we may not observe responses for certain $x$ values.


## K-Nearest Neighbors (KNN) Regression  {.smaller}

- Non-parametric approach
- Formally: Given a value for $K$ and a test data point $x_0$,
$$\hat{f}(x_0)=\dfrac{1}{K} \sum_{x_i \in \mathcal{N}_0} y_i=\text{Average} \ \left(y_i \ \text{for all} \ i:\ x_i \in \mathcal{N}_0\right) $$
where $\mathcal{N}_0$ is the set of the $K$ training observations closest to $x_0$.
- Informally, average together the $K$ "closest" observations in your training set
- "Closeness": usually use the **Euclidean metric** to measure distance
- Euclidean distance between $\mathbf{X}_i=(x_{i1}, x_{i2}, \ldots, x_{ip})$ and $\mathbf{x}_j=(x_{j1}, x_{j2}, \ldots, x_{jp})$:
$$||\mathbf{x}_i-\mathbf{x}_j||_2 = \sqrt{(x_{i1}-x_{j1})^2 + (x_{i2}-x_{j2})^2 + \ldots + (x_{ip}-x_{jp    })^2}$$

## [KNN Regression (single predictor): Fit]{.r-fit-text} {.smaller}

::::{.columns}
:::{.column}
**$K=1$**
```{r}
knnfit1 <- nearest_neighbor(mode = "regression", neighbors = 1) |> 
  set_engine("kknn") |> 
  fit(profit ~ population, data = outlets)   # 1-nn regression
predict(knnfit1, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction
```

```{r}
#| echo: FALSE
outlets |>
  mutate(dist = abs(population - 6),
         is_knn = if_else(dist <= min(dist), TRUE, FALSE)) |>
  ggplot(aes(x = population, y = profit, color = is_knn)) +
  geom_point(size=3) +
  geom_vline(xintercept = 6) +
  theme(text = element_text(size = 20))
```


:::
:::{.column}
**$K=5$**
```{r}
knnfit5 <- nearest_neighbor(mode = "regression", neighbors = 5) |> 
  set_engine("kknn") |> 
  fit(profit ~ population, data = outlets)   # 1-nn regression
predict(knnfit5, new_data = tibble(population = 6)) |> kable()  # 1-nn prediction
```

```{r}
#| echo: FALSE
outlets |>
  mutate(dist = abs(population - 6),
         is_knn = if_else(dist <= nth(dist, n=-5), TRUE, FALSE)) |>
  ggplot(aes(x = population, y = profit, color = is_knn)) +
  geom_point(size = 3) +
  geom_vline(xintercept = 6) +
  theme(text = element_text(size = 20))
```
:::
::::

## Regression Methods: Comparison

```{r}
#| echo: FALSE

slrfit <- linear_reg() |> 
  set_engine("lm") |> 
  fit(profit ~ population, data = outlets)   # fit the SLR model
pop_seq <- tibble(population = seq(min(outlets$population, na.rm = TRUE), max(outlets$population, na.rm = TRUE), 0.01))

# obtain predictions for all training data points
knn_1 <- predict(knnfit1, new_data = pop_seq)
knn_5 <- predict(knnfit5, new_data = pop_seq)
p <-  predict(slrfit, new_data = pop_seq)

# column bind original data with predicted values
predictions <- pop_seq |> 
  bind_cols(linear = p$.pred, knn_1 = knn_1$.pred, knn_5 = knn_5$.pred) |>
  pivot_longer(cols = !population, names_to = "Method", values_to = "profit")


# plot the three models
ggplot(data = outlets, aes(x = population, y = profit)) +
  geom_point() +
  geom_line(data = predictions, aes(x = population, y = profit, color = Method, linetype = Method), linewidth = 1)
```


## <span style="color:blue">Question!!!</span>

As $K$ in KNN regression increases:

- the flexibility of the fit $\underline{\hspace{5cm}}$ ([increases]{.fragment .highlight-red} /decreases)
- the bias of the fit $\underline{\hspace{5cm}}$ (increases/[decreases]{.fragment .highlight-red} )
- the variance of the fit $\underline{\hspace{5cm}}$ ([increases]{.fragment .highlight-red}/decreases)


## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}

- Let's look at the `house_prices` data
```{r}
#| echo: FALSE

ames |>
  select(Sale_Price, Gr_Liv_Area, Bedroom_AbvGr) |>
  head()
```
:::{.fragment}
:::{.incremental}
- Should 1 square foot count the same as 1 bedroom?
- Need to **center and scale** (freq. just say scale)
  + subtract mean from each predictor
  + divide by standard deviation of each predictor
  + compares apples-to-apples
:::
:::

## Scaling in R

```{r}
# scale predictors
ames_scaled <- tibble(size_scaled = scale(ames$Gr_Liv_Area),
                                  num_bedrooms_scaled = scale(ames$Bedroom_AbvGr),
                                  price = ames$Sale_Price)

head(ames_scaled)   # first six observations
```


## [K-Nearest Neighbors Regression (multiple predictors)]{.r-fit-text} {.smaller}


```{r}
library(caret)
knnfit10 <- knnreg(price ~ size_scaled + num_bedrooms_scaled, data = ames_scaled, k = 10)   # 10-nn regression
```

- Must also scale test data points **using mean and sd from training set!!!!**
- Test Point: `size` = 2000 square feet, and `num_bedrooms` = 3, then

```{r}
# obtain 10-nn prediction

predict(knnfit10, newdata = tibble(size_scaled = (2000 - mean(ames$Gr_Liv_Area))/sd(ames$Gr_Liv_Area),
                                     num_bedrooms_scaled = (3 - mean(ames$Bedroom_AbvGr))/sd(ames$Bedroom_AbvGr)))
```


## [Linear Regression vs K-Nearest Neighbors]{.r-fit-text} {.smaller}

- Linear regression is a parametric approach (with restrictive assumptions), KNN is non-parametric.
- Linear regression works for regression problems ($Y$ numerical), KNN can be used for both regression and classification - i.e. $Y$ qualitative (next lesson)
- Linear regression is interpretable, KNN is not.
- Linear regression can accommodate qualitative predictors and can be extended to include interaction terms as well while KNN does not allow for qualitative predictors
- Performance: KNN can be pretty good for small $p$, that is, $p \le 4$ and large $n$. Performance of KNN deteriorates as $p$ increases - *curse of dimensionality*

## Classification Problems {.smaller}

- Response $Y$ is qualitative (categorical).
- Objective: build a classifier $\hat{Y}=\hat{C}(\mathbf{X})$
  + assigns class label to a future unlabeled (unseen) observations
  + understand the relationship between the predictors and response
- Two ways to make predictions
  + Class probabilities
  + Class labels

## Classification Problems: Example

**Default dataset**

```{r, message=FALSE}
library(ISLR2)   # load library
data("Default")   # load dataset
```

```{r}
head(Default)   # print first six observations
```

```{r,message=FALSE}
table(Default$default)   # class frequencies
```


**We will consider `default` as the response variable.**


## Classification Problems: Example

For some algorithms, we might need to convert the categorical response to numeric (0/1) values.

**Default dataset**

```{r,message=FALSE}
Default$default_id <- ifelse(Default$default == "Yes", 1, 0)   # create 0/1 variable

head(Default, 10)   # print first ten observations
```




## K-Nearest Neighbors Classifier

Given a value for $K$ and a test data point $x_0$,
$$P(Y=j | X=x_0)=\dfrac{1}{K} \sum_{x_i \in \mathcal{N}_0} I(y_i = j)$$

where $\mathcal{N}_0$ is known as the **neighborhood** of $x_0$.


For classification problems, the predictions are obtained in terms of **majority vote** (unlike in regression where predictions are obtained by averaging).


## K-Nearest Neighbors Classifier: Build Model

**Default dataset**

response ($Y$): `default` and predictor ($X$): `balance`

```{r}
knnfit <- knn3(default ~ balance, data = Default, k = 10)   # fit 10-nn model
```


## K-Nearest Neighbors Classifier: Predictions

**Default dataset**

* One can directly obtain the class label predictions as below.

```{r}
knn_class_preds_1 <- predict(knnfit, newdata = Default, type = "class")   # obtain default class label predictions
```


* Otherwise, one can first obtain predictions in terms of probabilities and then convert them into class label predictions based on a threshold.

```{r}
knn_prob_preds <- predict(knnfit, newdata = Default, type = "prob")   # obtain predictions as probabilities
```


```{r}
threshold <- 0.5   # set threshold

knn_class_preds_2 <- factor(ifelse(knn_prob_preds[,2] > threshold, "Yes", "No"))   # obtain predictions as class labels
```


## K-Nearest Neighbors Classifier: Performance  {.smaller}

**Default dataset**

```{r}
# create confusion matrix

# use the following code only when all predictions are from the same class
# levels(knn_class_preds_1) = c("No", "Yes")

confusionMatrix(data = knn_class_preds_1, reference = Default$default, positive = "Yes")
```


